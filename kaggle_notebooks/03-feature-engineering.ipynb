{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Setup Instructions\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before running this notebook, ensure you have completed the following setup:\n",
    "\n",
    "- [ ] **Java 11 installed** and `JAVA_HOME` configured\n",
    "  - macOS: `brew install openjdk@11`\n",
    "  - Set: `export JAVA_HOME=$(/usr/libexec/java_home -v 11)`\n",
    "- [ ] **Conda environment `fraud-shield` created and activated**\n",
    "  - Create: `conda env create -f environment.yml`\n",
    "  - Activate: `conda activate fraud-shield`\n",
    "- [ ] **Data directories created**\n",
    "  - `data/checkpoints/` - for EDA checkpoints\n",
    "  - `data/processed/` - for processed data\n",
    "  - `models/` - for saved models\n",
    "\n",
    "## Environment Activation\n",
    "\n",
    "```bash\n",
    "conda activate fraud-shield\n",
    "```\n",
    "\n",
    "**Note:** This is a local execution version configured for the `fraud-shield` conda environment on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection for Fraud Detection\n",
    "\n",
    "**Notebook:** 03-local-feature-engineering.ipynb  \n",
    "**Objective:** Feature importance analysis, aggregated rankings, selection, and visualization using preprocessed data\n",
    "\n",
    "**Pipeline:**\n",
    "1. **Load:** Preprocessed train/val from `02-local-preprocessing.ipynb` (train_preprocessed.parquet, val_preprocessed.parquet).\n",
    "2. **Feature importance:** RF, XGBoost, LightGBM, Mutual Information; each method yields importance scores and validation AUC.\n",
    "3. **Aggregate rankings:** Combine scores into average rank and average (normalized) score; sort by rank; save `data/features/feature_rankings.csv`.\n",
    "4. **Feature selection:** Top N features by aggregate rank (and optional min score); save `data/features/selected_features.pkl` and `feature_importance.pkl`.\n",
    "5. **Visualization:** Per-method bar charts and aggregated top-k bar chart; save to `data/features/` (e.g. feature_importance_plots.png, aggregated_feature_importance.png).\n",
    "\n",
    "**Outputs:** feature_rankings.csv, selected_features.pkl, feature_importance.pkl, plots. Downstream: notebook 04 uses these for iterative model training.\n",
    "\n",
    "**Note:** Local execution - conda environment `fraud-shield`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "\n",
    "# Path resolution for local execution\n",
    "# Kaggle environment paths\n",
    "PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "INPUT_DIR = Path(\"/kaggle/input\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "FEATURES_DIR = DATA_DIR / \"features\"\n",
    "\n",
    "# Create output dirs\n",
    "for d in [CHECKPOINT_DIR, PROCESSED_DATA_DIR, MODELS_DIR, RESULTS_DIR, FEATURES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_in_kaggle_input(filename):\n",
    "    \"\"\"Find file in any /kaggle/input subdir.\"\"\"\n",
    "    for p in INPUT_DIR.iterdir():\n",
    "        if p.is_dir():\n",
    "            f = p / filename\n",
    "            if f.exists():\n",
    "                return str(f)\n",
    "    return None\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "FEATURES_DIR = DATA_DIR / \"features\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "FEATURES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "PREPROCESSED_TRAIN_PATH = PROCESSED_DATA_DIR / 'train_preprocessed.parquet'\n",
    "PREPROCESSED_VAL_PATH = PROCESSED_DATA_DIR / 'val_preprocessed.parquet'\n",
    "FEATURE_NAMES_PATH = MODELS_DIR / 'feature_names.pkl'\n",
    "\n",
    "# Output paths\n",
    "FEATURE_IMPORTANCE_PATH = FEATURES_DIR / 'feature_importance.pkl'\n",
    "SELECTED_FEATURES_PATH = FEATURES_DIR / 'selected_features.pkl'\n",
    "FEATURE_RANKINGS_PATH = FEATURES_DIR / 'feature_rankings.csv'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")\n",
    "print(f\"Features directory: {FEATURES_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Preprocessed Data\n",
    "\n",
    "Load the preprocessed training and validation data from the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "def load_preprocessed_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load preprocessed training and validation data.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_df, val_df) as pandas DataFrames\n",
    "    \"\"\"\n",
    "    if not PREPROCESSED_TRAIN_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Preprocessed data not found: {PREPROCESSED_TRAIN_PATH}\\n\"\n",
    "            \"Please run 02-local-preprocessing.ipynb first.\"\n",
    "        )\n",
    "    \n",
    "    print(\"Loading preprocessed data...\")\n",
    "    train_df = pd.read_parquet(PREPROCESSED_TRAIN_PATH)\n",
    "    val_df = pd.read_parquet(PREPROCESSED_VAL_PATH)\n",
    "    \n",
    "    print(f\"Training data loaded: {len(train_df):,} samples, {train_df.shape[1]} features\")\n",
    "    print(f\"Validation data loaded: {len(val_df):,} samples, {val_df.shape[1]} features\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop(columns=['is_fraud'])\n",
    "    y_train = train_df['is_fraud']\n",
    "    X_val = val_df.drop(columns=['is_fraud'])\n",
    "    y_val = val_df['is_fraud']\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape:\")\n",
    "    print(f\"  Train: {X_train.shape}\")\n",
    "    print(f\"  Validation: {X_val.shape}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(f\"  Train - Fraud: {y_train.sum():,} ({y_train.mean():.4%})\")\n",
    "    print(f\"  Validation - Fraud: {y_val.sum():,} ({y_val.mean():.4%})\")\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "# Load data\n",
    "X_train, y_train, X_val, y_val = load_preprocessed_data()\n",
    "feature_names = X_train.columns.tolist()\n",
    "print(f\"\\nTotal features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Importance Analysis\n",
    "\n",
    "Analyze feature importance using multiple methods:\n",
    "1. Random Forest feature importance\n",
    "2. XGBoost feature importance\n",
    "3. LightGBM feature importance\n",
    "4. Statistical feature selection (mutual information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "def calculate_feature_importance(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    "    feature_names: List[str]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculate feature importance using multiple methods.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with importance scores from different methods\n",
    "    \"\"\"\n",
    "    importance_scores = {}\n",
    "    \n",
    "    print(\"Calculating feature importance using multiple methods...\\n\")\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    print(\"1. Training Random Forest...\")\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=1000,\n",
    "        min_samples_leaf=500,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_scores['random_forest'] = rf_importance\n",
    "    print(f\"   Random Forest AUC: {roc_auc_score(y_val, rf.predict_proba(X_val)[:, 1]):.4f}\")\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    print(\"2. Training XGBoost...\")\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    xgb_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': xgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_scores['xgboost'] = xgb_importance\n",
    "    print(f\"   XGBoost AUC: {roc_auc_score(y_val, xgb_model.predict_proba(X_val)[:, 1]):.4f}\")\n",
    "    \n",
    "    # 3. LightGBM\n",
    "    print(\"3. Training LightGBM...\")\n",
    "    lgb_model = lgb.LGBMClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    lgb_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': lgb_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_scores['lightgbm'] = lgb_importance\n",
    "    print(f\"   LightGBM AUC: {roc_auc_score(y_val, lgb_model.predict_proba(X_val)[:, 1]):.4f}\")\n",
    "    \n",
    "    # 4. Mutual Information\n",
    "    print(\"4. Calculating Mutual Information...\")\n",
    "    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "    mi_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': mi_scores\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    importance_scores['mutual_info'] = mi_importance\n",
    "    print(\"   Mutual Information calculated\")\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "\n",
    "# Calculate importance scores\n",
    "importance_scores = calculate_feature_importance(X_train, y_train, X_val, y_val, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Aggregate Feature Rankings\n",
    "\n",
    "Combine importance scores from all methods to create a unified feature ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AGGREGATE FEATURE RANKINGS\n",
    "# ============================================================\n",
    "\n",
    "def aggregate_rankings(importance_scores: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate feature rankings from multiple methods.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with aggregated rankings and scores\n",
    "    \"\"\"\n",
    "    # Create a combined DataFrame\n",
    "    combined = pd.DataFrame({'feature': feature_names})\n",
    "    \n",
    "    # Add normalized importance scores from each method\n",
    "    for method, df in importance_scores.items():\n",
    "        # Create a mapping from feature to rank (1 = most important)\n",
    "        df_sorted = df.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "        rank_map = {feat: idx + 1 for idx, feat in enumerate(df_sorted['feature'])}\n",
    "        score_map = {feat: score for feat, score in zip(df['feature'], df['importance'])}\n",
    "        \n",
    "        combined[f'{method}_score'] = combined['feature'].map(score_map)\n",
    "        combined[f'{method}_rank'] = combined['feature'].map(rank_map)\n",
    "    \n",
    "    # Calculate average rank (lower is better)\n",
    "    rank_cols = [col for col in combined.columns if col.endswith('_rank')]\n",
    "    combined['avg_rank'] = combined[rank_cols].mean(axis=1)\n",
    "    \n",
    "    # Calculate average normalized score (higher is better)\n",
    "    score_cols = [col for col in combined.columns if col.endswith('_score')]\n",
    "    combined['avg_score'] = combined[score_cols].mean(axis=1)\n",
    "    \n",
    "    # Sort by average rank\n",
    "    combined = combined.sort_values('avg_rank').reset_index(drop=True)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "# Create aggregated rankings\n",
    "feature_rankings = aggregate_rankings(importance_scores)\n",
    "\n",
    "print(\"Top 15 Features by Average Rank:\")\n",
    "print(feature_rankings[['feature', 'avg_rank', 'avg_score']].head(15).to_string(index=False))\n",
    "\n",
    "# Save rankings\n",
    "feature_rankings.to_csv(FEATURE_RANKINGS_PATH, index=False)\n",
    "print(f\"\\nFeature rankings saved: {FEATURE_RANKINGS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Feature Selection\n",
    "\n",
    "Select top features based on importance rankings and validate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================\n",
    "\n",
    "def select_top_features(\n",
    "    feature_rankings: pd.DataFrame,\n",
    "    n_features: int = 20,\n",
    "    min_score: float = 0.01\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Select top features based on rankings and minimum score threshold.\n",
    "    \n",
    "    Args:\n",
    "        feature_rankings: DataFrame with feature rankings\n",
    "        n_features: Maximum number of features to select\n",
    "        min_score: Minimum average score threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of selected feature names\n",
    "    \"\"\"\n",
    "    # Filter by minimum score\n",
    "    filtered = feature_rankings[feature_rankings['avg_score'] >= min_score]\n",
    "    \n",
    "    # Select top N features\n",
    "    selected = filtered.head(n_features)['feature'].tolist()\n",
    "    \n",
    "    print(f\"Feature Selection Summary:\")\n",
    "    print(f\"  Total features: {len(feature_rankings)}\")\n",
    "    print(f\"  Features above threshold ({min_score}): {len(filtered)}\")\n",
    "    print(f\"  Selected top {n_features} features\")\n",
    "    print(f\"\\nSelected features:\")\n",
    "    for i, feat in enumerate(selected, 1):\n",
    "        rank = feature_rankings[feature_rankings['feature'] == feat]['avg_rank'].values[0]\n",
    "        score = feature_rankings[feature_rankings['feature'] == feat]['avg_score'].values[0]\n",
    "        print(f\"  {i:2d}. {feat:30s} (rank: {rank:.1f}, score: {score:.4f})\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "# Select top features\n",
    "selected_features = select_top_features(feature_rankings, n_features=20, min_score=0.01)\n",
    "\n",
    "# Save selected features\n",
    "with open(SELECTED_FEATURES_PATH, 'wb') as f:\n",
    "    pickle.dump(selected_features, f)\n",
    "print(f\"\\nSelected features saved: {SELECTED_FEATURES_PATH}\")\n",
    "\n",
    "# Save importance scores\n",
    "with open(FEATURE_IMPORTANCE_PATH, 'wb') as f:\n",
    "    pickle.dump(importance_scores, f)\n",
    "print(f\"Feature importance scores saved: {FEATURE_IMPORTANCE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Feature Visualization\n",
    "\n",
    "Visualize feature importance rankings from different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE FEATURE IMPORTANCE\n",
    "# ============================================================\n",
    "\n",
    "def plot_feature_importance(\n",
    "    importance_scores: Dict[str, pd.DataFrame],\n",
    "    feature_rankings: pd.DataFrame,\n",
    "    top_n: int = 15\n",
    "):\n",
    "    \"\"\"Plot feature importance from different methods.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Feature Importance Rankings', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    methods = ['random_forest', 'xgboost', 'lightgbm', 'mutual_info']\n",
    "    titles = ['Random Forest', 'XGBoost', 'LightGBM', 'Mutual Information']\n",
    "    \n",
    "    for idx, (method, title) in enumerate(zip(methods, titles)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        df = importance_scores[method].head(top_n)\n",
    "        \n",
    "        ax.barh(range(len(df)), df['importance'].values, color='steelblue')\n",
    "        ax.set_yticks(range(len(df)))\n",
    "        ax.set_yticklabels(df['feature'].values, fontsize=9)\n",
    "        ax.set_xlabel('Importance Score', fontsize=10)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FEATURES_DIR / 'feature_importance_plots.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Feature importance plots saved: {FEATURES_DIR / 'feature_importance_plots.png'}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot aggregated rankings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_rankings.head(top_n)\n",
    "    plt.barh(range(len(top_features)), top_features['avg_score'].values, color='coral')\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "    plt.xlabel('Average Importance Score', fontsize=12)\n",
    "    plt.title(f'Top {top_n} Features (Aggregated Rankings)', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FEATURES_DIR / 'aggregated_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"Aggregated feature importance plot saved: {FEATURES_DIR / 'aggregated_feature_importance.png'}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create visualizations\n",
    "plot_feature_importance(importance_scores, feature_rankings, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Section Summary\n",
    "\n",
    "**Pipeline (Sections 1–5):** Load preprocessed train/val; feature importance (RF, XGBoost, LightGBM, Mutual Information); aggregate rankings; select top N features; save and visualize.\n",
    "\n",
    "**Outputs:**\n",
    "- `data/features/feature_rankings.csv` – aggregated rankings\n",
    "- `data/features/feature_importance.pkl` – per-method importance scores\n",
    "- `data/features/selected_features.pkl` – selected feature list\n",
    "- `data/features/feature_importance_plots.png` – per-method bar charts\n",
    "- `data/features/aggregated_feature_importance.png` – top-k aggregated bar chart\n",
    "\n",
    "**Next:** Use in 04-local-supervised-models.ipynb (e.g. load `feature_rankings.csv` for iterative feature order or `selected_features.pkl` for the feature list)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-shield)",
   "language": "python",
   "name": "fraud-shield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}