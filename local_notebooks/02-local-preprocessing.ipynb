{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Setup Instructions\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before running this notebook, ensure you have completed the following setup:\n",
    "\n",
    "- [ ] **Java 11 installed** and `JAVA_HOME` configured\n",
    "  - macOS: `brew install openjdk@11`\n",
    "  - Set: `export JAVA_HOME=$(/usr/libexec/java_home -v 11)`\n",
    "- [ ] **Conda environment `fraud-shield` created and activated**\n",
    "  - Create: `conda env create -f environment.yml`\n",
    "  - Activate: `conda activate fraud-shield`\n",
    "- [ ] **PySpark verified working**\n",
    "  - Test: `python -c \"from pyspark.sql import SparkSession; print('PySpark OK')\"`\n",
    "- [ ] **Data directories created**\n",
    "  - `data/checkpoints/` - for EDA checkpoints\n",
    "  - `data/processed/` - for preprocessed data\n",
    "  - `models/` - for saved preprocessors\n",
    "- [ ] **EDA checkpoint available**\n",
    "  - Run `01-local-fraud-detection-eda.ipynb` first\n",
    "  - Checkpoint should exist: `data/checkpoints/section11_enriched_features.parquet`\n",
    "\n",
    "## Environment Activation\n",
    "\n",
    "```bash\n",
    "conda activate fraud-shield\n",
    "```\n",
    "\n",
    "## Checkpoint Requirements\n",
    "\n",
    "This notebook requires the Section 11 checkpoint from the EDA notebook:\n",
    "- `data/checkpoints/section11_enriched_features.parquet`\n",
    "\n",
    "**Note:** This is a local execution version configured for the `fraud-shield` conda environment on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline for Fraud Detection - Local Execution Version\n",
    "\n",
    "**Notebook:** 02-local-preprocessing.ipynb  \n",
    "**Objective:** Feature selection, encoding, scaling, and class imbalance handling\n",
    "\n",
    "**Note:** Local execution version - configured for conda environment `fraud-shield`\n",
    "\n",
    "## Preprocessing Strategy\n",
    "\n",
    "**SMOTE Parameters:**\n",
    "- k_neighbors: 5 (default, suitable for our dataset size)\n",
    "- sampling_strategy: 0.1 (10% minority class ratio - balances performance and computational cost)\n",
    "- random_state: 42 (reproducibility)\n",
    "\n",
    "**Feature Selection Approach:**\n",
    "- Start with Critical Priority features (transaction_count_bin, card_age_bin, hour)\n",
    "- Add High Priority features (category, day_of_week, month)\n",
    "- Include interaction features (evening_high_amount, new_card_evening, high_amount_online)\n",
    "- Use feature importance from tree models to validate and refine\n",
    "\n",
    "**Validation Split Method:**\n",
    "- Time-aware split (respect temporal order)\n",
    "- Train: Jan 2012 - Dec 2012 (12 months)\n",
    "- Validation: Jan 2013 - Mar 2013 (3 months)\n",
    "- Test: Apr 2013 - Jun 2013 (3 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# PySpark (for loading checkpoint data)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/abzanganeh/Desktop/projects/fraud-shield-ai\n",
      "Data directory: /Users/abzanganeh/Desktop/projects/fraud-shield-ai/data\n",
      "Checkpoint directory: /Users/abzanganeh/Desktop/projects/fraud-shield-ai/data/checkpoints\n",
      "Models directory: /Users/abzanganeh/Desktop/projects/fraud-shield-ai/models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "\n",
    "# Path resolution for local execution\n",
    "# Calculate PROJECT_ROOT based on notebook location\n",
    "# Since notebook is in local_notebooks/, project root is parent directory\n",
    "NOTEBOOK_DIR = Path.cwd()  # Current working directory\n",
    "# If we're in local_notebooks/, go up one level to get project root\n",
    "if NOTEBOOK_DIR.name == \"local_notebooks\":\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    # Fallback: assume we're already at project root\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "# Change working directory to project root for consistency\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Checkpoint path from EDA notebook (Section 11 final checkpoint)\n",
    "CHECKPOINT_SECTION11 = CHECKPOINT_DIR / 'section11_enriched_features.parquet'\n",
    "\n",
    "# Output paths\n",
    "PREPROCESSED_TRAIN_PATH = PROCESSED_DATA_DIR / 'train_preprocessed.parquet'\n",
    "PREPROCESSED_VAL_PATH = PROCESSED_DATA_DIR / 'val_preprocessed.parquet'\n",
    "PREPROCESSED_TEST_PATH = PROCESSED_DATA_DIR / 'test_preprocessed.parquet'\n",
    "PREPROCESSER_PATH = MODELS_DIR / 'preprocessor.pkl'\n",
    "FEATURE_NAMES_PATH = MODELS_DIR / 'feature_names.pkl'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/25 22:57:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/25 22:57:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INITIALIZE SPARK SESSION (for loading checkpoint data)\n",
    "# ============================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionPreprocessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from EDA Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from: /Users/abzanganeh/Desktop/projects/fraud-shield-ai/data/checkpoints/section11_enriched_features.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Spark DataFrame to Pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/25 22:57:55 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "26/01/25 22:58:11 ERROR Executor: Exception in task 2.0 in stage 1.0 (TID 3)/ 4]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2546/0x00000008011c6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2532/0x00000008011bc040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2529/0x000000080117f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1616/0x0000000800bfdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "26/01/25 22:58:11 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 1.0 (TID 3),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2546/0x00000008011c6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2532/0x00000008011bc040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2529/0x000000080117f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1616/0x0000000800bfdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "26/01/25 22:58:11 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 3) (alirezas-imac.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:61)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:348)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$2$adapted(SparkPlan.scala:368)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2546/0x00000008011c6840.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)\n",
      "\tat org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n",
      "\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:391)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2532/0x00000008011bc040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2529/0x000000080117f040.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1616/0x0000000800bfdc40.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "26/01/25 22:58:11 ERROR TaskSetManager: Task 2 in stage 1.0 failed 1 times; aborting job\n",
      "ERROR:root:Exception while sending command.                         (0 + 3) / 4]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abzanganeh/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/abzanganeh/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abzanganeh/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abzanganeh/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/abzanganeh/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31m<class 'str'>\u001b[39m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1257\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m-> \u001b[39m\u001b[32m1257\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     converted = \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[39m, in \u001b[36mconvert_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[39m, in \u001b[36mCapturedException.__init__\u001b[39m\u001b[34m(self, desc, stackTrace, cause, origin)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mself\u001b[39m.stackTrace = (\n\u001b[32m     61\u001b[39m     stackTrace\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext._jvm.org.apache.spark.util.Utils.exceptionString(origin))\n\u001b[32m     64\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28mself\u001b[39m.cause = \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin.getCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:157\u001b[39m, in \u001b[36mconvert_exception\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    156\u001b[39m c: Py4JJavaError = e.getCause()\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m stacktrace: \u001b[38;5;28mstr\u001b[39m = \u001b[43mjvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43morg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutil\u001b[49m.Utils.exceptionString(e)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    159\u001b[39m     is_instance_of(gw, c, \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.api.python.PythonException\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     )\n\u001b[32m    166\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/java_gateway.py:1664\u001b[39m, in \u001b[36mJavaPackage.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m does not exist in the JVM\u001b[39m\u001b[33m\"\u001b[39m.format(new_fqn))\n",
      "\u001b[31mPy4JError\u001b[39m: org.apache.spark.util does not exist in the JVM",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m df = \u001b[43mload_eda_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mload_eda_checkpoint\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Convert to Pandas (for modeling)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mConverting Spark DataFrame to Pandas...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m df = \u001b[43mspark_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✓ Loaded successfully\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/sql/pandas/conversion.py:202\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    199\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    204\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    205\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    206\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1256\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m   1237\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[32m   1238\u001b[39m \n\u001b[32m   1239\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m \u001b[33;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[32m   1255\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSCCallSiteSync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/pyspark/traceback_utils.py:81\u001b[39m, in \u001b[36mSCCallSiteSync.__exit__\u001b[39m\u001b[34m(self, type, value, tb)\u001b[39m\n\u001b[32m     79\u001b[39m SCCallSiteSync._spark_stack_depth -= \u001b[32m1\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync._spark_stack_depth == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetCallSite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fraud-shield/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# LOAD DATA FROM EDA CHECKPOINT\n",
    "# ============================================================\n",
    "\n",
    "def load_eda_checkpoint() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the final enriched dataset from EDA notebook checkpoint.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with all engineered features\n",
    "    \"\"\"\n",
    "    if not CHECKPOINT_SECTION11.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"EDA checkpoint not found: {CHECKPOINT_SECTION11}\\n\"\n",
    "            \"Please run the EDA notebook (01-local-fraud-detection-eda.ipynb) first.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Loading checkpoint from: {CHECKPOINT_SECTION11}\")\n",
    "    \n",
    "    # Load as Spark DataFrame\n",
    "    spark_df = spark.read.parquet(str(CHECKPOINT_SECTION11))\n",
    "    \n",
    "    # Convert to Pandas (for modeling)\n",
    "    print(\"Converting Spark DataFrame to Pandas...\")\n",
    "    df = spark_df.toPandas()\n",
    "    \n",
    "    print(f\"\\n✓ Loaded successfully\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(df['is_fraud'].value_counts())\n",
    "    print(f\"\\nFraud rate: {df['is_fraud'].mean():.4%}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_eda_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTION - CRITICAL PRIORITY FEATURES\n",
    "# ============================================================\n",
    "\n",
    "# Define feature categories based on EDA findings\n",
    "CRITICAL_FEATURES = [\n",
    "    'transaction_count_bin',  # 191.57x ratio\n",
    "    'card_age_bin',            # 13.77x ratio\n",
    "    'hour',                    # 36.85x ratio\n",
    "    'time_bin',              # Derived from hour\n",
    "    'is_peak_fraud_hour',     # Pattern flag\n",
    "    'is_new_card',            # Pattern flag\n",
    "    'is_low_volume_card'      # Pattern flag\n",
    "]\n",
    "\n",
    "HIGH_PRIORITY_FEATURES = [\n",
    "    'category',               # 11.34x ratio\n",
    "    'day_of_week',            # Temporal pattern\n",
    "    'month',                  # 2.26x ratio\n",
    "    'is_peak_fraud_day',      # Pattern flag\n",
    "    'is_peak_fraud_season',   # Pattern flag\n",
    "    'is_high_risk_category',  # Pattern flag\n",
    "    'card_age_days',          # Continuous version\n",
    "    'transaction_count'       # Continuous version\n",
    "]\n",
    "\n",
    "INTERACTION_FEATURES = [\n",
    "    'evening_high_amount',\n",
    "    'evening_online_shopping',\n",
    "    'large_city_evening',\n",
    "    'new_card_evening',\n",
    "    'high_amount_online'\n",
    "]\n",
    "\n",
    "ENRICHED_FEATURES = [\n",
    "    'temporal_risk_score',\n",
    "    'geographic_risk_score',\n",
    "    'card_risk_score',\n",
    "    'risk_tier'\n",
    "]\n",
    "\n",
    "# Combine all features (start with Critical + High Priority)\n",
    "SELECTED_FEATURES = CRITICAL_FEATURES + HIGH_PRIORITY_FEATURES + INTERACTION_FEATURES + ENRICHED_FEATURES\n",
    "\n",
    "# Filter to only features that exist in the dataset\n",
    "available_features = [f for f in SELECTED_FEATURES if f in df.columns]\n",
    "missing_features = [f for f in SELECTED_FEATURES if f not in df.columns]\n",
    "\n",
    "print(\"Feature Selection Summary:\")\n",
    "print(f\"  Critical features: {len([f for f in CRITICAL_FEATURES if f in df.columns])}/{len(CRITICAL_FEATURES)}\")\n",
    "print(f\"  High priority features: {len([f for f in HIGH_PRIORITY_FEATURES if f in df.columns])}/{len(HIGH_PRIORITY_FEATURES)}\")\n",
    "print(f\"  Interaction features: {len([f for f in INTERACTION_FEATURES if f in df.columns])}/{len(INTERACTION_FEATURES)}\")\n",
    "print(f\"  Enriched features: {len([f for f in ENRICHED_FEATURES if f in df.columns])}/{len(ENRICHED_FEATURES)}\")\n",
    "print(f\"\\n  Total selected: {len(available_features)} features\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n  ⚠ Missing features (will be skipped): {missing_features}\")\n",
    "\n",
    "# Store feature list for later use\n",
    "feature_names = available_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Splitting (Time-Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIME-AWARE DATA SPLITTING\n",
    "# ============================================================\n",
    "\n",
    "def split_data_time_aware(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str = 'trans_date_trans_time',\n",
    "    train_end: str = '2012-12-31',\n",
    "    val_end: str = '2013-03-31'\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data respecting temporal order.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        date_col: Name of date column\n",
    "        train_end: End date for training set (YYYY-MM-DD)\n",
    "        val_end: End date for validation set (YYYY-MM-DD)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    # Convert date column to datetime if needed\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    train_end_dt = pd.to_datetime(train_end)\n",
    "    val_end_dt = pd.to_datetime(val_end)\n",
    "    \n",
    "    # Split by date\n",
    "    train_df = df[df[date_col] <= train_end_dt].copy()\n",
    "    val_df = df[(df[date_col] > train_end_dt) & (df[date_col] <= val_end_dt)].copy()\n",
    "    test_df = df[df[date_col] > val_end_dt].copy()\n",
    "    \n",
    "    print(\"Time-aware split summary:\")\n",
    "    print(f\"  Train: {train_df.shape[0]:,} samples ({train_df[date_col].min()} to {train_df[date_col].max()})\")\n",
    "    print(f\"  Validation: {val_df.shape[0]:,} samples ({val_df[date_col].min()} to {val_df[date_col].max()})\")\n",
    "    print(f\"  Test: {test_df.shape[0]:,} samples ({test_df[date_col].min()} to {test_df[date_col].max()})\")\n",
    "    \n",
    "    print(f\"\\nFraud rates:\")\n",
    "    print(f\"  Train: {train_df['is_fraud'].mean():.4%}\")\n",
    "    print(f\"  Validation: {val_df['is_fraud'].mean():.4%}\")\n",
    "    print(f\"  Test: {test_df['is_fraud'].mean():.4%}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Check if we have a date column (try common names)\n",
    "date_col = None\n",
    "for col_name in ['trans_date_trans_time', 'merchant_local_time', 'customer_local_time']:\n",
    "    if col_name in df.columns:\n",
    "        date_col = col_name\n",
    "        break\n",
    "\n",
    "if date_col:\n",
    "    train_df, val_df, test_df = split_data_time_aware(df, date_col=date_col)\n",
    "else:\n",
    "    print(\"⚠ No date column found - using random split instead\")\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['is_fraud'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['is_fraud'])\n",
    "    print(f\"  Train: {train_df.shape[0]:,} samples\")\n",
    "    print(f\"  Validation: {val_df.shape[0]:,} samples\")\n",
    "    print(f\"  Test: {test_df.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Encoding & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE ENCODING CLASS\n",
    "# ============================================================\n",
    "\n",
    "class FeaturePreprocessor:\n",
    "    \"\"\"\n",
    "    Handles encoding, scaling, and preprocessing of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.label_encoders: Dict[str, LabelEncoder] = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names: List[str] = []\n",
    "        self.categorical_features: List[str] = []\n",
    "        self.numerical_features: List[str] = []\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _identify_feature_types(self, df: pd.DataFrame, features: List[str]) -> None:\n",
    "        \"\"\"Identify categorical vs numerical features.\"\"\"\n",
    "        self.categorical_features = []\n",
    "        self.numerical_features = []\n",
    "        \n",
    "        for feat in features:\n",
    "            if feat not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Check if categorical (object type or low cardinality integer)\n",
    "            if df[feat].dtype == 'object' or df[feat].dtype.name == 'category':\n",
    "                self.categorical_features.append(feat)\n",
    "            elif df[feat].dtype in ['int64', 'int32', 'float64', 'float32']:\n",
    "                # Check cardinality for integer features\n",
    "                if df[feat].dtype in ['int64', 'int32']:\n",
    "                    unique_count = df[feat].nunique()\n",
    "                    if unique_count < 20:  # Low cardinality - treat as categorical\n",
    "                        self.categorical_features.append(feat)\n",
    "                    else:\n",
    "                        self.numerical_features.append(feat)\n",
    "                else:\n",
    "                    self.numerical_features.append(feat)\n",
    "            else:\n",
    "                # Default to numerical\n",
    "                self.numerical_features.append(feat)\n",
    "    \n",
    "    def fit(self, df: pd.DataFrame, features: List[str], target: str = 'is_fraud') -> 'FeaturePreprocessor':\n",
    "        \"\"\"\n",
    "        Fit preprocessor on training data.\n",
    "        \n",
    "        Args:\n",
    "            df: Training dataframe\n",
    "            features: List of feature names to process\n",
    "            target: Name of target column\n",
    "        \"\"\"\n",
    "        self.feature_names = [f for f in features if f in df.columns]\n",
    "        self._identify_feature_types(df, self.feature_names)\n",
    "        \n",
    "        # Fit label encoders for categorical features\n",
    "        for feat in self.categorical_features:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df[feat].astype(str).fillna('missing'))\n",
    "            self.label_encoders[feat] = le\n",
    "        \n",
    "        # Prepare numerical features for scaling\n",
    "        X_numerical = df[self.numerical_features].fillna(0).values if len(self.numerical_features) > 0 else np.array([]).reshape(len(df), 0)\n",
    "        \n",
    "        # Fit scaler\n",
    "        if len(self.numerical_features) > 0:\n",
    "            self.scaler.fit(X_numerical)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform data using fitted encoders and scaler.\n",
    "        \n",
    "        Args:\n",
    "            df: Dataframe to transform\n",
    "        \n",
    "        Returns:\n",
    "            numpy array of transformed features\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        \n",
    "        encoded_features = []\n",
    "        \n",
    "        # Encode categorical features\n",
    "        for feat in self.categorical_features:\n",
    "            if feat in df.columns:\n",
    "                le = self.label_encoders[feat]\n",
    "                # Handle unseen categories\n",
    "                encoded = df[feat].astype(str).fillna('missing')\n",
    "                # Map to known classes or 'missing'\n",
    "                encoded = encoded.apply(lambda x: x if x in le.classes_ else 'missing')\n",
    "                # Re-encode if 'missing' was added\n",
    "                if 'missing' not in le.classes_:\n",
    "                    le.classes_ = np.append(le.classes_, 'missing')\n",
    "                encoded_features.append(le.transform(encoded).reshape(-1, 1))\n",
    "        \n",
    "        # Scale numerical features\n",
    "        if len(self.numerical_features) > 0:\n",
    "            X_numerical = df[self.numerical_features].fillna(0).values\n",
    "            X_numerical_scaled = self.scaler.transform(X_numerical)\n",
    "            encoded_features.append(X_numerical_scaled)\n",
    "        \n",
    "        # Combine all features\n",
    "        if encoded_features:\n",
    "            X_combined = np.hstack(encoded_features)\n",
    "        else:\n",
    "            X_combined = np.array([]).reshape(len(df), 0)\n",
    "        \n",
    "        return X_combined\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get list of feature names after encoding.\"\"\"\n",
    "        feature_names_out = []\n",
    "        \n",
    "        # Categorical features (one per category after encoding)\n",
    "        for feat in self.categorical_features:\n",
    "            feature_names_out.append(feat)\n",
    "        \n",
    "        # Numerical features\n",
    "        feature_names_out.extend(self.numerical_features)\n",
    "        \n",
    "        return feature_names_out\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = FeaturePreprocessor()\n",
    "preprocessor.fit(train_df, feature_names)\n",
    "\n",
    "print(\"Preprocessor fitted successfully\")\n",
    "print(f\"  Categorical features: {len(preprocessor.categorical_features)}\")\n",
    "print(f\"  Numerical features: {len(preprocessor.numerical_features)}\")\n",
    "print(f\"  Total output features: {len(preprocessor.get_feature_names())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Preprocessing & Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY PREPROCESSING TO ALL SPLITS\n",
    "# ============================================================\n",
    "\n",
    "# Transform all splits\n",
    "X_train = preprocessor.transform(train_df)\n",
    "y_train = train_df['is_fraud'].values\n",
    "\n",
    "X_val = preprocessor.transform(val_df)\n",
    "y_val = val_df['is_fraud'].values\n",
    "\n",
    "X_test = preprocessor.transform(test_df)\n",
    "y_test = test_df['is_fraud'].values\n",
    "\n",
    "print(\"Preprocessing applied:\")\n",
    "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"  Validation: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"  Test: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "print(f\"\\nOriginal fraud rates:\")\n",
    "print(f\"  Train: {y_train.mean():.4%}\")\n",
    "print(f\"  Validation: {y_val.mean():.4%}\")\n",
    "print(f\"  Test: {y_test.mean():.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# APPLY SMOTE FOR CLASS IMBALANCE HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# SMOTE parameters (as decided in preprocessing strategy)\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.1,  # 10% minority class ratio\n",
    "    k_neighbors=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Applying SMOTE to training set...\")\n",
    "print(f\"  Before SMOTE: {X_train.shape[0]:,} samples (fraud: {y_train.sum():,}, {y_train.mean():.4%})\")\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"  After SMOTE: {X_train_resampled.shape[0]:,} samples (fraud: {y_train_resampled.sum():,}, {y_train_resampled.mean():.4%})\")\n",
    "print(f\"  Oversampling ratio: {X_train_resampled.shape[0] / X_train.shape[0]:.2f}x\")\n",
    "\n",
    "# Note: SMOTE is only applied to training set\n",
    "# Validation and test sets remain imbalanced (real-world scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data & Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE PREPROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "# Save as DataFrames for easier loading\n",
    "train_preprocessed = pd.DataFrame(X_train_resampled, columns=preprocessor.get_feature_names())\n",
    "train_preprocessed['is_fraud'] = y_train_resampled\n",
    "\n",
    "val_preprocessed = pd.DataFrame(X_val, columns=preprocessor.get_feature_names())\n",
    "val_preprocessed['is_fraud'] = y_val\n",
    "\n",
    "test_preprocessed = pd.DataFrame(X_test, columns=preprocessor.get_feature_names())\n",
    "test_preprocessed['is_fraud'] = y_test\n",
    "\n",
    "# Save to parquet\n",
    "train_preprocessed.to_parquet(PREPROCESSED_TRAIN_PATH, index=False)\n",
    "val_preprocessed.to_parquet(PREPROCESSED_VAL_PATH, index=False)\n",
    "test_preprocessed.to_parquet(PREPROCESSED_TEST_PATH, index=False)\n",
    "\n",
    "print(\"Preprocessed data saved:\")\n",
    "print(f\"  Train: {PREPROCESSED_TRAIN_PATH}\")\n",
    "print(f\"  Validation: {PREPROCESSED_VAL_PATH}\")\n",
    "print(f\"  Test: {PREPROCESSED_TEST_PATH}\")\n",
    "\n",
    "# Save preprocessor\n",
    "joblib.dump(preprocessor, PREPROCESSER_PATH)\n",
    "print(f\"\\nPreprocessor saved: {PREPROCESSER_PATH}\")\n",
    "\n",
    "# Save feature names\n",
    "with open(FEATURE_NAMES_PATH, 'wb') as f:\n",
    "    pickle.dump(preprocessor.get_feature_names(), f)\n",
    "print(f\"Feature names saved: {FEATURE_NAMES_PATH}\")\n",
    "\n",
    "print(\"\\n✓ Preprocessing pipeline complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
