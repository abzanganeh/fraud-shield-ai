{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline - Experiment Documentation\n",
    "\n",
    "## Class Imbalance Handling Strategy\n",
    "\n",
    "This notebook handles the severe class imbalance (0.58% fraud rate) through **cost-sensitive learning** rather than synthetic oversampling.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiments Conducted\n",
    "\n",
    "### Experiment 1: SMOTE Oversampling (REJECTED)\n",
    "\n",
    "**Status:** Tested and rejected - code preserved below (commented out)\n",
    "\n",
    "**Configuration Tested:**\n",
    "```python\n",
    "# SMOTE(sampling_strategy=0.1, k_neighbors=5, random_state=42)\n",
    "# Result: 9.09% fraud rate in training data\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- Validation F1: 0.53\n",
    "- Test F1: **0.008** (catastrophic failure)\n",
    "- Reason: Distribution shift between synthetic training data and real test data\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 2: SMOTE Variants Testing (ALL REJECTED)\n",
    "\n",
    "We tested multiple SMOTE variants and sampling strategies:\n",
    "\n",
    "| Variant | Sampling | Test F1 | Verdict |\n",
    "|---------|----------|---------|---------|\n",
    "| No SMOTE (Baseline) | 0.58% | **0.290** | **BEST** |\n",
    "| SMOTE | 1% | 0.141 | Rejected |\n",
    "| SMOTE | 2% | 0.248 | Rejected |\n",
    "| SMOTE | 5% | 0.196 | Rejected |\n",
    "| SMOTE | 10% | 0.183 | Rejected |\n",
    "| BorderlineSMOTE | 5% | 0.095 | Rejected |\n",
    "| ADASYN | 5% | 0.088 | Rejected |\n",
    "| SMOTEENN | 5% | 0.265 | Rejected |\n",
    "| SMOTETomek | 5% | 0.182 | Rejected |\n",
    "\n",
    "**Conclusion:** All oversampling methods degraded test performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 3: Natural Distribution + Class Weights (ADOPTED)\n",
    "\n",
    "**Current Implementation:**\n",
    "\n",
    "- Training data uses **natural distribution** (0.58% fraud)\n",
    "- Class imbalance handled via:\n",
    "  - **XGBoost:** `scale_pos_weight = (1 - fraud_rate) / fraud_rate`\n",
    "  - **Deep Learning:** Focal Loss or Weighted BCE with `pos_weight`\n",
    "- Threshold optimized on validation set using PR curve\n",
    "\n",
    "---\n",
    "\n",
    "## Code Organization\n",
    "\n",
    "The SMOTE experiment code is preserved below (Section 9) but **commented out**. The active code uses natural distribution with class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Local Setup Instructions\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before running this notebook, ensure you have completed the following setup:\n",
    "\n",
    "- [ ] **Java 11 installed** and `JAVA_HOME` configured\n",
    "  - macOS: `brew install openjdk@11`\n",
    "  - Set: `export JAVA_HOME=$(/usr/libexec/java_home -v 11)`\n",
    "- [ ] **Conda environment `fraud-shield` created and activated**\n",
    "  - Create: `conda env create -f environment.yml`\n",
    "  - Activate: `conda activate fraud-shield`\n",
    "- [ ] **PySpark verified working**\n",
    "  - Test: `python -c \"from pyspark.sql import SparkSession; print('PySpark OK')\"`\n",
    "- [ ] **Data directories created**\n",
    "  - `data/checkpoints/` - for EDA checkpoints\n",
    "  - `data/processed/` - for preprocessed data\n",
    "  - `models/` - for saved preprocessors\n",
    "- [ ] **EDA checkpoint available**\n",
    "  - Run `01-local-fraud-detection-eda.ipynb` first\n",
    "  - Checkpoint should exist: `data/checkpoints/section8_geographic_features.parquet`\n",
    "\n",
    "## Environment Activation\n",
    "\n",
    "```bash\n",
    "conda activate fraud-shield\n",
    "```\n",
    "\n",
    "## Checkpoint Requirements\n",
    "\n",
    "This notebook requires the Section 8 checkpoint from the EDA notebook:\n",
    "- `data/checkpoints/section8_geographic_features.parquet`\n",
    "\n",
    "**Checkpoint & Parquet Update Behavior:** Processed parquet files (train/val/test) and saved pipelines are updated when the notebook runs the relevant sections and the save path executes. They do not auto-update on a schedule. If you change the EDA checkpoint or preprocessing code, re-run the affected sections to refresh outputs.\n",
    "\n",
    "**Note:** This is a local execution version configured for the `fraud-shield` conda environment on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Preprocessing Pipeline for Fraud Detection\n",
    "\n",
    "**Notebook:** 02-local-preprocessing.ipynb\n",
    "**Objective:** Production-grade preprocessing using Spark MLlib for scalability\n",
    "\n",
    "**Architecture:**\n",
    "- Training data: Loaded from EDA Section 8 checkpoint (geographic features, 35 columns)\n",
    "- Test data: Loaded separately from `fraudTest.csv` (never used for fitting)\n",
    "- Preprocessing: Spark MLlib Pipeline (StringIndexer, VectorAssembler, StandardScaler, Imputer)\n",
    "- Class Imbalance: Handled via cost-sensitive learning (NOT SMOTE - see experiment documentation)\n",
    "- Output: Both Spark MLlib and sklearn pipelines saved for flexibility\n",
    "\n",
    "**Pipeline Order:**\n",
    "1. Load section8 checkpoint (row-level geographic + temporal features only)\n",
    "2. Compute row-level features before split (day_of_week, peak flags, risk category)\n",
    "3. Time-aware train/val split (80% / 10%)\n",
    "4. Compute card-level + velocity features after split using point-in-time windows\n",
    "5. Derive interaction and risk score features\n",
    "6. Feature selection (~30 features including amt, city_pop, velocity)\n",
    "7. Fit StandardScaler on training data only\n",
    "8. Transform train / val / test consistently\n",
    "\n",
    "**Data Leakage Safeguards:**\n",
    "1. Card-level features (transaction_count, card_age_days) computed via cumulative backward-only windows per split\n",
    "2. Velocity features (txn_count_last_1h, txn_count_last_24h) use backward time-range windows\n",
    "3. Validation features computed on train+val union so val rows see training history (past = correct) without future leakage\n",
    "4. Test data loaded separately, never used for fitting\n",
    "5. Preprocessor fitted only on training data\n",
    "6. Time-aware split prevents future data leakage\n",
    "\n",
    "**Class Imbalance Strategy:**\n",
    "- **SMOTE:** Tested and rejected (see experiment log above)\n",
    "- **Adopted:** Natural distribution + class weights (scale_pos_weight, Focal Loss)\n",
    "- **Time-aware Split:** train = first 80% of period, val = next 10%; test = separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:19.962022Z",
     "iopub.status.busy": "2026-02-07T06:38:19.961969Z",
     "iopub.status.idle": "2026-02-07T06:38:20.341995Z",
     "shell.execute_reply": "2026-02-07T06:38:20.341775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PySpark & Spark MLlib\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, when, isnan, isnull,\n",
    "    min as spark_min, max as spark_max,\n",
    "    dayofweek, hour, month, datediff, lit,\n",
    "    sum as spark_sum, count, floor, broadcast, first, trim,\n",
    "    avg as spark_avg, coalesce, monotonically_increasing_id,\n",
    "    sin, cos, sqrt, radians, atan2,\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Sklearn (for pipeline conversion and inference)\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder as SklearnLabelEncoder\n",
    "from sklearn.impute import SimpleImputer as SklearnSimpleImputer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration\n",
    "\n",
    "Configure project paths, directories, and Spark session. All paths are resolved relative to the project root for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:20.355682Z",
     "iopub.status.busy": "2026-02-07T06:38:20.355543Z",
     "iopub.status.idle": "2026-02-07T06:38:20.358762Z",
     "shell.execute_reply": "2026-02-07T06:38:20.358585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/alireza/Desktop/projects/fraud-shield-ai\n",
      "Data directory: /home/alireza/Desktop/projects/fraud-shield-ai/data\n",
      "Checkpoint directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/checkpoints\n",
      "Input directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/input\n",
      "Models directory: /home/alireza/Desktop/projects/fraud-shield-ai/models\n",
      "Processed data directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "if NOTEBOOK_DIR.name == \"local_notebooks\":\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "INPUT_DIR = DATA_DIR / \"input\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "CHECKPOINT_SECTION8 = CHECKPOINT_DIR / 'section8_geographic_features.parquet'\n",
    "TEST_DATA_PATH = INPUT_DIR / 'fraudTest.csv'\n",
    "\n",
    "# Output paths\n",
    "PREPROCESSED_TRAIN_PATH = PROCESSED_DATA_DIR / 'train_preprocessed.parquet'\n",
    "PREPROCESSED_VAL_PATH = PROCESSED_DATA_DIR / 'val_preprocessed.parquet'\n",
    "PREPROCESSED_TEST_PATH = PROCESSED_DATA_DIR / 'test_preprocessed.parquet'\n",
    "SPARK_PREPROCESSER_PATH = MODELS_DIR / 'spark_preprocessor.pkl'\n",
    "SKLEARN_PREPROCESSER_PATH = MODELS_DIR / 'sklearn_preprocessor.pkl'\n",
    "FEATURE_NAMES_PATH = MODELS_DIR / 'feature_names.pkl'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:20.359612Z",
     "iopub.status.busy": "2026-02-07T06:38:20.359562Z",
     "iopub.status.idle": "2026-02-07T06:38:20.371866Z",
     "shell.execute_reply": "2026-02-07T06:38:20.371711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alireza/anaconda3/envs/fraud-shield/bin/python\n"
     ]
    }
   ],
   "source": [
    "python_exec = sys.executable\n",
    "print(python_exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:20.372761Z",
     "iopub.status.busy": "2026-02-07T06:38:20.372714Z",
     "iopub.status.idle": "2026-02-07T06:38:21.777147Z",
     "shell.execute_reply": "2026-02-07T06:38:21.776881Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JAVA_HOME already set: /home/alireza/anaconda3/envs/fraud-shield/lib/jvm\n",
      "✓ Java path set (version check skipped)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 22:38:20 WARN Utils: Your hostname, zanganeh-ai resolves to a loopback address: 127.0.1.1; using 192.168.86.248 instead (on interface wlp129s0)\n",
      "26/02/06 22:38:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 22:38:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized with optimized memory settings\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SET JAVA_HOME FROM CONDA ENVIRONMENT\n",
    "# ============================================================\n",
    "\n",
    "# Stop any existing Spark session first\n",
    "try:\n",
    "    spark_existing = SparkSession.getActiveSession()\n",
    "    if spark_existing:\n",
    "        print(\"Stopping existing Spark session...\")\n",
    "        spark_existing.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Auto-detect JAVA_HOME from conda environment\n",
    "if 'JAVA_HOME' not in os.environ or not os.environ.get('JAVA_HOME'):\n",
    "    # Method 1: Try conda environment (check sys.executable path)\n",
    "    python_exec = sys.executable\n",
    "    if 'envs' in python_exec:\n",
    "        # Extract conda env path from Python executable\n",
    "        env_path = python_exec.split('envs/')[0] + 'envs/' + python_exec.split('envs/')[1].split('/')[0]\n",
    "        java_bin = os.path.join(env_path, 'bin', 'java')\n",
    "        if os.path.exists(java_bin):\n",
    "            os.environ['JAVA_HOME'] = env_path\n",
    "            print(f\"✓ JAVA_HOME set from conda environment: {env_path}\")\n",
    "        else:\n",
    "            # Method 2: Try CONDA_PREFIX if available\n",
    "            conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "            if conda_prefix:\n",
    "                java_bin = os.path.join(conda_prefix, 'bin', 'java')\n",
    "                if os.path.exists(java_bin):\n",
    "                    os.environ['JAVA_HOME'] = conda_prefix\n",
    "                    print(f\"✓ JAVA_HOME set from CONDA_PREFIX: {conda_prefix}\")\n",
    "                else:\n",
    "                    # Method 3: Find Java via which\n",
    "                    java_path = shutil.which('java')\n",
    "                    if java_path and os.path.exists(java_path):\n",
    "                        java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "                        os.environ['JAVA_HOME'] = java_home\n",
    "                        print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "                    else:\n",
    "                        raise RuntimeError(\n",
    "                            \"Java not found. Please install Java 11:\\n\"\n",
    "                            \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                            \"Then restart the Jupyter kernel.\"\n",
    "                        )\n",
    "            else:\n",
    "                # Method 3: Find Java via which\n",
    "                java_path = shutil.which('java')\n",
    "                if java_path and os.path.exists(java_path):\n",
    "                    java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "                    os.environ['JAVA_HOME'] = java_home\n",
    "                    print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        \"Java not found. Please install Java 11:\\n\"\n",
    "                        \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                        \"Then restart the Jupyter kernel.\"\n",
    "                    )\n",
    "    else:\n",
    "        # Not in conda, try system Java\n",
    "        java_path = shutil.which('java')\n",
    "        if java_path and os.path.exists(java_path):\n",
    "            java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Java not found. Please install Java 11:\\n\"\n",
    "                \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                \"Then restart the Jupyter kernel.\"\n",
    "            )\n",
    "else:\n",
    "    print(f\"✓ JAVA_HOME already set: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Verify Java is accessible\n",
    "java_home = os.environ.get('JAVA_HOME')\n",
    "if java_home:\n",
    "    java_exe = os.path.join(java_home, 'bin', 'java')\n",
    "    if not os.path.exists(java_exe):\n",
    "        print(f\"⚠ Warning: Java executable not found at {java_exe}\")\n",
    "    else:\n",
    "        # Test Java version\n",
    "        try:\n",
    "            result = subprocess.run([java_exe, '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT, timeout=5)\n",
    "            print(f\"✓ Java verified: {result.stdout.split(chr(10))[0] if result.stdout else 'Java is working'}\")\n",
    "        except:\n",
    "            print(\"✓ Java path set (version check skipped)\")\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE SPARK SESSION\n",
    "# ============================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionPreprocessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized with optimized memory settings\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:21.778096Z",
     "iopub.status.busy": "2026-02-07T06:38:21.778029Z",
     "iopub.status.idle": "2026-02-07T06:38:24.403553Z",
     "shell.execute_reply": "2026-02-07T06:38:24.403269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imported timezone pipeline from scripts/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ZIP reference table created: 3,197 unique grid cells\n",
      "✓ TimezonePipeline instance created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIMEZONE PIPELINE SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Try to import from scripts (local/production)\n",
    "try:\n",
    "    import sys\n",
    "    if Path.cwd().name == \"local_notebooks\":\n",
    "        sys.path.insert(0, str(Path.cwd().parent))\n",
    "    from scripts.timezone_pipeline import TimezonePipeline\n",
    "    print(\"✓ Imported timezone pipeline from scripts/\")\n",
    "except ImportError:\n",
    "    # Fallback: define inline (for Kaggle/Colab)\n",
    "    print(\"⚠️ Could not import from scripts/, defining inline...\")\n",
    "    # [Copy class definitions here if needed for Kaggle/Colab]\n",
    "    print(\"✓ Timezone pipeline defined inline\")\n",
    "\n",
    "# Build ZIP reference table (same as EDA notebook)\n",
    "GRID_SIZE = 0.5  # ~50km grid resolution\n",
    "full_zip_path = os.path.join(INPUT_DIR, \"uszips.csv\")\n",
    "\n",
    "if os.path.exists(full_zip_path):\n",
    "    zip_ref_df = (\n",
    "        spark.read.csv(full_zip_path, header=True, inferSchema=True)\n",
    "        .withColumnRenamed(\"zip\", \"zip_ref\")\n",
    "        .withColumn(\"lat_grid\", floor(col(\"lat\") / GRID_SIZE))\n",
    "        .withColumn(\"lng_grid\", floor(col(\"lng\") / GRID_SIZE))\n",
    "        .select(\"lat_grid\", \"lng_grid\", \"timezone\")\n",
    "        .filter(\n",
    "            (trim(col(\"timezone\")) != \"\") &\n",
    "            (col(\"timezone\") != \"FALSE\") &\n",
    "            col(\"timezone\").rlike(\"^[A-Za-z_/]+$\")\n",
    "        )\n",
    "        .distinct()\n",
    "        .groupBy(\"lat_grid\", \"lng_grid\")\n",
    "        .agg(first(\"timezone\").alias(\"timezone\"))\n",
    "        .cache()\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ZIP reference table created: {zip_ref_df.count():,} unique grid cells\")\n",
    "    \n",
    "    # Create TimezonePipeline instance\n",
    "    timezone_pipeline = TimezonePipeline(\n",
    "        zip_ref_df=zip_ref_df,\n",
    "        grid_size=GRID_SIZE\n",
    "    )\n",
    "    print(\"✓ TimezonePipeline instance created\")\n",
    "else:\n",
    "    print(f\"⚠️ Warning: uszips.csv not found at {full_zip_path}\")\n",
    "    print(\"  Timezone conversion will be skipped. Ensure EDA notebook has been run first.\")\n",
    "    timezone_pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading\n",
    "\n",
    "Load training data from the Section 8 EDA checkpoint (geographic features, 35 columns) and test data from CSV. Training and test data are kept separate to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:24.404668Z",
     "iopub.status.busy": "2026-02-07T06:38:24.404588Z",
     "iopub.status.idle": "2026-02-07T06:38:28.162730Z",
     "shell.execute_reply": "2026-02-07T06:38:28.162402Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from checkpoint: /home/alireza/Desktop/projects/fraud-shield-ai/data/checkpoints/section8_geographic_features.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 22:38:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:===============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:======================================================>  (23 + 1) / 24]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows: 1,296,675\n",
      "  Columns: 35\n",
      "  Fraud rate: 0.5789%\n",
      "Loading test data from: /home/alireza/Desktop/projects/fraud-shield-ai/data/input/fraudTest.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows: 555,719, Columns: 23\n",
      "  Fraud rate: 0.3860%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "def load_training_data() -> DataFrame:\n",
    "    \"\"\"Load training data from EDA Section 8 checkpoint.\"\"\"\n",
    "    if not CHECKPOINT_SECTION8.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"EDA checkpoint not found: {CHECKPOINT_SECTION8}\\n\"\n",
    "            \"Please run the EDA notebook (01-local-fraud-detection-eda.ipynb) first.\"\n",
    "        )\n",
    "\n",
    "    spark.catalog.clearCache()\n",
    "    try:\n",
    "        spark.catalog.refreshByPath(str(CHECKPOINT_SECTION8))\n",
    "    except (AnalysisException, AttributeError):\n",
    "        pass\n",
    "\n",
    "    print(f\"Loading training data from checkpoint: {CHECKPOINT_SECTION8}\")\n",
    "    spark_df = spark.read.parquet(str(CHECKPOINT_SECTION8)).cache()\n",
    "    total_rows = spark_df.count()\n",
    "\n",
    "    print(f\"  Rows: {total_rows:,}\")\n",
    "    print(f\"  Columns: {len(spark_df.columns)}\")\n",
    "\n",
    "    fraud_yes = spark_df.filter(col(\"is_fraud\") == 1).count()\n",
    "    fraud_rate = fraud_yes / total_rows if total_rows else 0.0\n",
    "    print(f\"  Fraud rate: {fraud_rate:.4%}\")\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "\n",
    "def load_test_data() -> DataFrame:\n",
    "    \"\"\"Load test data from CSV. Never used for fitting.\"\"\"\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Test data not found: {TEST_DATA_PATH}\\n\"\n",
    "            \"Please ensure fraudTest.csv is in the data/input/ directory.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "    spark_df = spark.read.csv(str(TEST_DATA_PATH), header=True, inferSchema=True)\n",
    "    print(f\"  Rows: {spark_df.count():,}, Columns: {len(spark_df.columns)}\")\n",
    "\n",
    "    if \"is_fraud\" in spark_df.columns:\n",
    "        fraud_rate = spark_df.filter(col(\"is_fraud\") == 1).count() / spark_df.count()\n",
    "        print(f\"  Fraud rate: {fraud_rate:.4%}\")\n",
    "\n",
    "    return spark_df\n",
    "\n",
    "\n",
    "train_raw = load_training_data()\n",
    "test_df_raw = load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Row-Level Features (Before Split)\n",
    "\n",
    "Compute features that depend only on the current row. These are safe to compute before the train/val split because they introduce no cross-row information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:28.163794Z",
     "iopub.status.busy": "2026-02-07T06:38:28.163726Z",
     "iopub.status.idle": "2026-02-07T06:38:28.230853Z",
     "shell.execute_reply": "2026-02-07T06:38:28.230540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After row-level features: 40 columns\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ROW-LEVEL FEATURES (safe before split)\n",
    "# ============================================================\n",
    "\n",
    "def add_row_level_features(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add features derived from a single row only (no aggregation across rows).\"\"\"\n",
    "\n",
    "    # Ensure timestamp column exists\n",
    "    if \"merchant_local_time\" not in df.columns and \"trans_date_trans_time\" in df.columns:\n",
    "        df = df.withColumn(\"merchant_local_time\", to_timestamp(col(\"trans_date_trans_time\")))\n",
    "\n",
    "    if \"trans_date_trans_time\" in df.columns:\n",
    "        df = df.withColumn(\"trans_date_trans_time\", to_timestamp(col(\"trans_date_trans_time\")))\n",
    "\n",
    "    # Temporal\n",
    "    if \"hour\" not in df.columns:\n",
    "        df = df.withColumn(\"hour\", hour(to_timestamp(col(\"merchant_local_time\"))))\n",
    "    if \"day_of_week\" not in df.columns:\n",
    "        df = df.withColumn(\"day_of_week\", dayofweek(to_timestamp(col(\"merchant_local_time\"))))\n",
    "    if \"month\" not in df.columns:\n",
    "        df = df.withColumn(\"month\", month(to_timestamp(col(\"merchant_local_time\"))))\n",
    "    if \"time_bin\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"time_bin\",\n",
    "            when(col(\"hour\").between(6, 11), \"Morning\")\n",
    "            .when(col(\"hour\").between(12, 17), \"Afternoon\")\n",
    "            .when(col(\"hour\").between(18, 23), \"Evening\")\n",
    "            .otherwise(\"Night\"),\n",
    "        )\n",
    "\n",
    "    # Peak flags\n",
    "    df = df.withColumn(\"is_peak_fraud_hour\", when(col(\"hour\").between(18, 23), 1).otherwise(0))\n",
    "    df = df.withColumn(\"is_peak_fraud_day\", when(col(\"day_of_week\").isin([4, 5, 6]), 1).otherwise(0))\n",
    "    df = df.withColumn(\"is_peak_fraud_season\", when(col(\"month\").isin([1, 2]), 1).otherwise(0))\n",
    "\n",
    "    # High-risk category flag\n",
    "    high_risk_cats = [\"grocery_pos\", \"gas_transport\", \"shopping_pos\", \"misc_pos\", \"grocery_net\"]\n",
    "    df = df.withColumn(\"is_high_risk_category\", when(col(\"category\").isin(high_risk_cats), 1).otherwise(0))\n",
    "\n",
    "    # Amount bin / city_size (needed later for interaction features)\n",
    "    if \"amount_bin\" not in df.columns and \"amt\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"amount_bin\",\n",
    "            when(col(\"amt\") > 1000, \">$1000\")\n",
    "            .when(col(\"amt\") > 500, \"$500-$1000\")\n",
    "            .when(col(\"amt\") > 300, \"$300-$500\")\n",
    "            .when(col(\"amt\") > 100, \"$100-$300\")\n",
    "            .when(col(\"amt\") > 50, \"$50-$100\")\n",
    "            .otherwise(\"<$50\"),\n",
    "        )\n",
    "    if \"city_size\" not in df.columns and \"city_pop\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"city_size\",\n",
    "            when(col(\"city_pop\") > 1000000, \"Metropolitan\")\n",
    "            .when(col(\"city_pop\") > 500000, \"Large City\")\n",
    "            .when(col(\"city_pop\") > 100000, \"Medium City\")\n",
    "            .when(col(\"city_pop\") > 10000, \"Small City\")\n",
    "            .otherwise(\"Small Town\"),\n",
    "        )\n",
    "\n",
    "    # Haversine distance between customer and merchant\n",
    "    if \"customer_merchant_distance_km\" not in df.columns:\n",
    "        if all(c in df.columns for c in [\"lat\", \"long\", \"merch_lat\", \"merch_long\"]):\n",
    "            lat1 = radians(col(\"lat\"))\n",
    "            lon1 = radians(col(\"long\"))\n",
    "            lat2 = radians(col(\"merch_lat\"))\n",
    "            lon2 = radians(col(\"merch_long\"))\n",
    "            dlat = lat2 - lat1\n",
    "            dlon = lon2 - lon1\n",
    "            a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "            c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "            df = df.withColumn(\"customer_merchant_distance_km\", lit(6371.0) * c)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_with_row_feats = add_row_level_features(train_raw)\n",
    "print(f\"After row-level features: {len(train_with_row_feats.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Train/Validation Split\n",
    "\n",
    "Perform time-aware splitting to respect temporal order. Train = first 80% of the date range, Validation = next 10%. Test is loaded separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:28.232015Z",
     "iopub.status.busy": "2026-02-07T06:38:28.231913Z",
     "iopub.status.idle": "2026-02-07T06:38:28.712510Z",
     "shell.execute_reply": "2026-02-07T06:38:28.712291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing time-aware split...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data date range: 2018-12-31 to 2020-06-21\n",
      "  Train: up to 2020-03-04\n",
      "  Val:   up to 2020-04-27\n",
      "  Train: 1,034,987 rows\n",
      "  Val:   122,480 rows\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIME-AWARE DATA SPLITTING (Spark SQL)\n",
    "# ============================================================\n",
    "\n",
    "def split_data_time_aware(\n",
    "    spark_df: DataFrame,\n",
    "    date_col: str = 'merchant_local_time',\n",
    "    train_frac: float = 0.8,\n",
    "    val_frac: float = 0.9,\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Split data into train and validation using temporal ordering.\n",
    "    Dates are derived from the actual data range (no hardcoded dates).\n",
    "    \"\"\"\n",
    "    date_col_found = None\n",
    "    if date_col in spark_df.columns:\n",
    "        date_col_found = date_col\n",
    "    else:\n",
    "        for col_name in ['merchant_local_time', 'customer_local_time', 'trans_date_trans_time']:\n",
    "            if col_name in spark_df.columns:\n",
    "                date_col_found = col_name\n",
    "                print(f\"  Using date column: {col_name}\")\n",
    "                break\n",
    "\n",
    "    if date_col_found is None:\n",
    "        date_like = [c for c in spark_df.columns if 'date' in c.lower() or 'time' in c.lower()]\n",
    "        raise ValueError(f\"Date column '{date_col}' not found. Available: {date_like}\")\n",
    "\n",
    "    date_col = date_col_found\n",
    "    spark_df = spark_df.withColumn(date_col, to_timestamp(col(date_col)))\n",
    "\n",
    "    actual_min_date = spark_df.select(spark_min(col(date_col))).collect()[0][0]\n",
    "    actual_max_date = spark_df.select(spark_max(col(date_col))).collect()[0][0]\n",
    "    if actual_min_date is None or actual_max_date is None:\n",
    "        raise ValueError(f\"Date column '{date_col}' contains no valid dates\")\n",
    "\n",
    "    actual_min_dt = pd.to_datetime(actual_min_date)\n",
    "    actual_max_dt = pd.to_datetime(actual_max_date)\n",
    "\n",
    "    if not 0 < train_frac < val_frac <= 1.0:\n",
    "        raise ValueError(\"Require 0 < train_frac < val_frac <= 1.0\")\n",
    "\n",
    "    data_span_days = (actual_max_dt - actual_min_dt).days\n",
    "    train_end_dt = actual_min_dt + pd.Timedelta(days=int(data_span_days * train_frac))\n",
    "    val_end_dt = actual_min_dt + pd.Timedelta(days=int(data_span_days * val_frac))\n",
    "\n",
    "    print(f\"  Data date range: {actual_min_dt.date()} to {actual_max_dt.date()}\")\n",
    "    print(f\"  Train: up to {train_end_dt.date()}\")\n",
    "    print(f\"  Val:   up to {val_end_dt.date()}\")\n",
    "\n",
    "    train_df = spark_df.filter(col(date_col) <= lit(train_end_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    val_df = spark_df.filter(\n",
    "        (col(date_col) > lit(train_end_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        & (col(date_col) <= lit(val_end_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "    )\n",
    "\n",
    "    train_count = train_df.count()\n",
    "    val_count = val_df.count()\n",
    "    print(f\"  Train: {train_count:,} rows\")\n",
    "    print(f\"  Val:   {val_count:,} rows\")\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "print(\"Performing time-aware split...\")\n",
    "train_split, val_split = split_data_time_aware(train_with_row_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Card-Level & Velocity Features (After Split)\n",
    "\n",
    "These features aggregate across rows for the same card (`cc_num`). To prevent data leakage they are computed with **point-in-time backward-only windows**: each row only sees transactions at or before its own timestamp.\n",
    "\n",
    "- **Train:** features computed on training data alone.\n",
    "- **Validation:** train + val are unioned, features are computed with point-in-time windows, then only val rows are kept. This lets val rows see card history from the training period (past data) without future leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:28.713706Z",
     "iopub.status.busy": "2026-02-07T06:38:28.713639Z",
     "iopub.status.idle": "2026-02-07T06:38:29.071301Z",
     "shell.execute_reply": "2026-02-07T06:38:29.071113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing card-level features for train...\n",
      "Computing card-level features for val (with train history)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train rows: 1,034,987, Val rows: 122,480\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CARD-LEVEL & VELOCITY FEATURES (point-in-time, after split)\n",
    "# ============================================================\n",
    "\n",
    "def add_card_velocity_features(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add card-level and velocity features using backward-only windows.\n",
    "    Every aggregation is cumulative up to (and including) the current row's timestamp,\n",
    "    so no future information leaks into any row.\n",
    "    \"\"\"\n",
    "    time_ordered = Window.partitionBy(\"cc_num\").orderBy(\"unix_time\")\n",
    "    cumulative_window = time_ordered.rowsBetween(Window.unboundedPreceding, 0)\n",
    "    expanding_excl = time_ordered.rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "    # Card age: days since the card's first known transaction (backward)\n",
    "    df = df.withColumn(\n",
    "        \"first_txn_time\",\n",
    "        spark_min(col(\"trans_date_trans_time\")).over(\n",
    "            Window.partitionBy(\"cc_num\").orderBy(\"unix_time\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "        ),\n",
    "    )\n",
    "    df = df.withColumn(\"card_age_days\", datediff(col(\"trans_date_trans_time\"), col(\"first_txn_time\")))\n",
    "    df = df.drop(\"first_txn_time\")\n",
    "\n",
    "    # Cumulative transaction count (not total across all time)\n",
    "    df = df.withColumn(\"transaction_count\", count(\"*\").over(cumulative_window))\n",
    "\n",
    "    # Velocity: transactions in last 1h / 24h (backward time-range windows)\n",
    "    window_1h = Window.partitionBy(\"cc_num\").orderBy(\"unix_time\").rangeBetween(-3600, 0)\n",
    "    window_24h = Window.partitionBy(\"cc_num\").orderBy(\"unix_time\").rangeBetween(-86400, 0)\n",
    "    df = df.withColumn(\"txn_count_last_1h\", count(\"*\").over(window_1h))\n",
    "    df = df.withColumn(\"txn_count_last_24h\", count(\"*\").over(window_24h))\n",
    "\n",
    "    # Amount relative to card's historical average (exclude current row)\n",
    "    df = df.withColumn(\n",
    "        \"amt_relative_to_avg\",\n",
    "        col(\"amt\") / coalesce(spark_avg(\"amt\").over(expanding_excl), lit(1.0)),\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Tag rows so we can filter val back out after union\n",
    "_TAG = \"__split_tag__\"\n",
    "\n",
    "train_tagged = train_split.withColumn(_TAG, lit(\"train\"))\n",
    "val_tagged = val_split.withColumn(_TAG, lit(\"val\"))\n",
    "\n",
    "# Train features: compute on train alone\n",
    "print(\"Computing card-level features for train...\")\n",
    "train_df = add_card_velocity_features(train_tagged).filter(col(_TAG) == \"train\").drop(_TAG)\n",
    "\n",
    "# Val features: union train+val, compute, keep val rows only\n",
    "print(\"Computing card-level features for val (with train history)...\")\n",
    "union_df = train_tagged.unionByName(val_tagged)\n",
    "val_df = add_card_velocity_features(union_df).filter(col(_TAG) == \"val\").drop(_TAG)\n",
    "\n",
    "print(f\"  Train rows: {train_df.count():,}, Val rows: {val_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Interaction & Risk Features\n",
    "\n",
    "Derive interaction features (e.g. evening + high amount) and composite risk scores from the card-level and row-level features computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:29.072181Z",
     "iopub.status.busy": "2026-02-07T06:38:29.072124Z",
     "iopub.status.idle": "2026-02-07T06:38:29.303468Z",
     "shell.execute_reply": "2026-02-07T06:38:29.303298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interaction/risk features  train cols: 58, val cols: 58\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# INTERACTION & RISK FEATURES\n",
    "# ============================================================\n",
    "\n",
    "def add_interaction_risk_features(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Add interaction features and composite risk scores.\"\"\"\n",
    "    # Bins derived from card-level features\n",
    "    df = df.withColumn(\n",
    "        \"card_age_bin\",\n",
    "        when(col(\"card_age_days\") < 7, \"<7 days\")\n",
    "        .when(col(\"card_age_days\") < 30, \"7-30 days\")\n",
    "        .when(col(\"card_age_days\") < 90, \"30-90 days\")\n",
    "        .when(col(\"card_age_days\") < 180, \"90-180 days\")\n",
    "        .otherwise(\"180+ days\"),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"transaction_count_bin\",\n",
    "        when(col(\"transaction_count\") <= 5, \"1-5\")\n",
    "        .when(col(\"transaction_count\").between(6, 20), \"6-20\")\n",
    "        .when(col(\"transaction_count\").between(21, 100), \"21-100\")\n",
    "        .otherwise(\"100+\"),\n",
    "    )\n",
    "    df = df.withColumn(\"is_new_card\", when(col(\"card_age_days\") <= 30, 1).otherwise(0))\n",
    "    df = df.withColumn(\"is_low_volume_card\", when(col(\"transaction_count\") <= 5, 1).otherwise(0))\n",
    "\n",
    "    # Interaction features\n",
    "    high_amount_bins = [\"$300-$500\", \"$500-$1000\", \">$1000\"]\n",
    "    df = df.withColumn(\n",
    "        \"evening_high_amount\",\n",
    "        when((col(\"time_bin\") == \"Evening\") & col(\"amount_bin\").isin(high_amount_bins), 1).otherwise(0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"evening_online_shopping\",\n",
    "        when((col(\"time_bin\") == \"Evening\") & col(\"category\").isin([\"shopping_net\", \"misc_net\"]), 1).otherwise(0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"large_city_evening\",\n",
    "        when((col(\"city_size\") == \"Large City\") & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"new_card_evening\",\n",
    "        when((col(\"is_new_card\") == 1) & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"high_amount_online\",\n",
    "        when(col(\"amount_bin\").isin(high_amount_bins) & col(\"category\").isin([\"shopping_net\", \"misc_net\"]), 1).otherwise(0),\n",
    "    )\n",
    "\n",
    "    # Risk scores\n",
    "    df = df.withColumn(\n",
    "        \"temporal_risk_score\",\n",
    "        col(\"is_peak_fraud_hour\").cast(\"double\") * 0.4\n",
    "        + col(\"is_peak_fraud_day\").cast(\"double\") * 0.3\n",
    "        + col(\"is_peak_fraud_season\").cast(\"double\") * 0.3,\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"geographic_risk_score\",\n",
    "        when(col(\"city_pop\") < 10000, 0.3)\n",
    "        .when(col(\"city_pop\") < 50000, 0.2)\n",
    "        .when(col(\"city_pop\") < 100000, 0.1)\n",
    "        .otherwise(0.0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"card_risk_score\",\n",
    "        col(\"is_new_card\").cast(\"double\") * 0.5\n",
    "        + col(\"is_low_volume_card\").cast(\"double\") * 0.3\n",
    "        + when(col(\"card_age_days\") < 7, 0.2).otherwise(0.0),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"total_risk\",\n",
    "        col(\"temporal_risk_score\") + col(\"geographic_risk_score\") + col(\"card_risk_score\"),\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"risk_tier\",\n",
    "        when(col(\"total_risk\") >= 0.8, \"High\")\n",
    "        .when(col(\"total_risk\") >= 0.4, \"Medium\")\n",
    "        .otherwise(\"Low\"),\n",
    "    )\n",
    "    df = df.drop(\"total_risk\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = add_interaction_risk_features(train_df)\n",
    "val_df = add_interaction_risk_features(val_df)\n",
    "print(f\"After interaction/risk features  train cols: {len(train_df.columns)}, val cols: {len(val_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Selection\n",
    "\n",
    "Select the final feature set for modeling. Includes critical, high-priority, interaction, enriched (risk), and newly-added raw + velocity features (~30 total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:29.304177Z",
     "iopub.status.busy": "2026-02-07T06:38:29.304116Z",
     "iopub.status.idle": "2026-02-07T06:38:29.306816Z",
     "shell.execute_reply": "2026-02-07T06:38:29.306681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection Summary:\n",
      "  Critical:          7/7\n",
      "  High priority:     8/8\n",
      "  Interaction:       5/5\n",
      "  Enriched (risk):   4/4\n",
      "  Raw + velocity:    6/6\n",
      "  Total selected:    30 features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================\n",
    "\n",
    "CRITICAL_FEATURES = [\n",
    "    'transaction_count_bin',\n",
    "    'card_age_bin',\n",
    "    'hour',\n",
    "    'time_bin',\n",
    "    'is_peak_fraud_hour',\n",
    "    'is_new_card',\n",
    "    'is_low_volume_card',\n",
    "]\n",
    "\n",
    "HIGH_PRIORITY_FEATURES = [\n",
    "    'category',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'is_peak_fraud_day',\n",
    "    'is_peak_fraud_season',\n",
    "    'is_high_risk_category',\n",
    "    'card_age_days',\n",
    "    'transaction_count',\n",
    "]\n",
    "\n",
    "INTERACTION_FEATURES = [\n",
    "    'evening_high_amount',\n",
    "    'evening_online_shopping',\n",
    "    'large_city_evening',\n",
    "    'new_card_evening',\n",
    "    'high_amount_online',\n",
    "]\n",
    "\n",
    "ENRICHED_FEATURES = [\n",
    "    'temporal_risk_score',\n",
    "    'geographic_risk_score',\n",
    "    'card_risk_score',\n",
    "    'risk_tier',\n",
    "]\n",
    "\n",
    "RAW_AND_VELOCITY_FEATURES = [\n",
    "    'amt',\n",
    "    'city_pop',\n",
    "    'customer_merchant_distance_km',\n",
    "    'txn_count_last_1h',\n",
    "    'txn_count_last_24h',\n",
    "    'amt_relative_to_avg',\n",
    "]\n",
    "\n",
    "SELECTED_FEATURES = (\n",
    "    CRITICAL_FEATURES\n",
    "    + HIGH_PRIORITY_FEATURES\n",
    "    + INTERACTION_FEATURES\n",
    "    + ENRICHED_FEATURES\n",
    "    + RAW_AND_VELOCITY_FEATURES\n",
    ")\n",
    "\n",
    "available_features = [f for f in SELECTED_FEATURES if f in train_df.columns]\n",
    "missing_features = [f for f in SELECTED_FEATURES if f not in train_df.columns]\n",
    "\n",
    "print(\"Feature Selection Summary:\")\n",
    "print(f\"  Critical:          {len([f for f in CRITICAL_FEATURES if f in train_df.columns])}/{len(CRITICAL_FEATURES)}\")\n",
    "print(f\"  High priority:     {len([f for f in HIGH_PRIORITY_FEATURES if f in train_df.columns])}/{len(HIGH_PRIORITY_FEATURES)}\")\n",
    "print(f\"  Interaction:       {len([f for f in INTERACTION_FEATURES if f in train_df.columns])}/{len(INTERACTION_FEATURES)}\")\n",
    "print(f\"  Enriched (risk):   {len([f for f in ENRICHED_FEATURES if f in train_df.columns])}/{len(ENRICHED_FEATURES)}\")\n",
    "print(f\"  Raw + velocity:    {len([f for f in RAW_AND_VELOCITY_FEATURES if f in train_df.columns])}/{len(RAW_AND_VELOCITY_FEATURES)}\")\n",
    "print(f\"  Total selected:    {len(available_features)} features\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"  Missing (skipped): {missing_features}\")\n",
    "\n",
    "feature_names = available_features.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Spark MLlib Preprocessing Pipeline\n",
    "\n",
    "Create and fit a Spark MLlib Pipeline (StringIndexer + Imputer + VectorAssembler + StandardScaler). The pipeline is fitted **only on training data** and then applied to all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:29.307350Z",
     "iopub.status.busy": "2026-02-07T06:38:29.307302Z",
     "iopub.status.idle": "2026-02-07T06:38:34.351828Z",
     "shell.execute_reply": "2026-02-07T06:38:34.351486Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 79:>                                                       (0 + 24) / 26]\r",
      "\r",
      "[Stage 79:==>                                                     (1 + 24) / 26]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 79:===================================================>    (24 + 2) / 26]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark MLlib Preprocessor fitted on training data\n",
      "  Categorical features: 5\n",
      "  Numerical features:   25\n",
      "  Total features:       30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SPARK MLlib PREPROCESSOR CLASS\n",
    "# ============================================================\n",
    "\n",
    "class SparkMLlibPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-grade preprocessing using Spark MLlib Pipeline.\n",
    "    Handles categorical encoding, feature assembly, scaling, and imputation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_names: List[str]):\n",
    "        self.feature_names = feature_names\n",
    "        self.categorical_features: List[str] = []\n",
    "        self.numerical_features: List[str] = []\n",
    "        self.pipeline: Optional[Pipeline] = None\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _identify_feature_types(self, spark_df: DataFrame) -> None:\n",
    "        self.categorical_features = []\n",
    "        self.numerical_features = []\n",
    "\n",
    "        missing = [f for f in self.feature_names if f not in spark_df.columns]\n",
    "        if missing:\n",
    "            print(f\"  WARNING: Features not in DataFrame (skipped): {missing}\")\n",
    "\n",
    "        schema = spark_df.schema\n",
    "        for feat in self.feature_names:\n",
    "            if feat not in spark_df.columns:\n",
    "                continue\n",
    "            field_type = str(schema[feat].dataType)\n",
    "            if 'StringType' in field_type or 'String' in field_type:\n",
    "                self.categorical_features.append(feat)\n",
    "            else:\n",
    "                self.numerical_features.append(feat)\n",
    "\n",
    "    def fit(self, spark_df: DataFrame) -> 'SparkMLlibPreprocessor':\n",
    "        self._identify_feature_types(spark_df)\n",
    "\n",
    "        stages = []\n",
    "        indexed_categorical = []\n",
    "        imputed_numerical = []\n",
    "\n",
    "        for feat in self.categorical_features:\n",
    "            indexer = StringIndexer(inputCol=feat, outputCol=f\"{feat}_indexed\", handleInvalid=\"keep\")\n",
    "            stages.append(indexer)\n",
    "            indexed_categorical.append(f\"{feat}_indexed\")\n",
    "\n",
    "        if len(self.numerical_features) > 0:\n",
    "            imputer = Imputer(\n",
    "                inputCols=self.numerical_features,\n",
    "                outputCols=[f\"{f}_imputed\" for f in self.numerical_features],\n",
    "                strategy=\"mean\",\n",
    "            )\n",
    "            stages.append(imputer)\n",
    "            imputed_numerical = [f\"{f}_imputed\" for f in self.numerical_features]\n",
    "\n",
    "        assembler_inputs = indexed_categorical + imputed_numerical\n",
    "        assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "        stages.append(assembler)\n",
    "\n",
    "        scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "        stages.append(scaler)\n",
    "\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.pipeline_model = self.pipeline.fit(spark_df)\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, spark_df: DataFrame) -> DataFrame:\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        return self.pipeline_model.transform(spark_df)\n",
    "\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        return self.feature_names.copy()\n",
    "\n",
    "\n",
    "preprocessor = SparkMLlibPreprocessor(feature_names)\n",
    "preprocessor.fit(train_df)\n",
    "\n",
    "print(\"Spark MLlib Preprocessor fitted on training data\")\n",
    "print(f\"  Categorical features: {len(preprocessor.categorical_features)}\")\n",
    "print(f\"  Numerical features:   {len(preprocessor.numerical_features)}\")\n",
    "print(f\"  Total features:       {len(preprocessor.get_feature_names())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Engineer Test Features & Apply Pipeline\n",
    "\n",
    "Build the same feature set for test data (self-contained, no train/val history needed) and then transform all three splits through the fitted pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:34.352923Z",
     "iopub.status.busy": "2026-02-07T06:38:34.352859Z",
     "iopub.status.idle": "2026-02-07T06:38:37.555195Z",
     "shell.execute_reply": "2026-02-07T06:38:37.554874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for test data...\n",
      "  Test columns after engineering: 53\n",
      "\n",
      "Transforming datasets with fitted preprocessor...\n",
      "  Transforming training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train samples: 1,034,987\n",
      "  Transforming validation data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Validation samples: 122,480\n",
      "  Transforming test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test samples: 555,719\n",
      "\n",
      "All datasets transformed successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENGINEER FEATURES FOR TEST DATA\n",
    "# ============================================================\n",
    "\n",
    "def engineer_test_features(test_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Replicate the full feature engineering pipeline for test data.\n",
    "    Test data is self-contained: card-level and velocity features are computed\n",
    "    within the test set using point-in-time backward windows.\n",
    "    \"\"\"\n",
    "    df = test_df\n",
    "\n",
    "    # --- Row-level features ---\n",
    "    df = add_row_level_features(df)\n",
    "\n",
    "    # --- Card-level + velocity (point-in-time within test) ---\n",
    "    df = add_card_velocity_features(df)\n",
    "\n",
    "    # --- Interaction + risk ---\n",
    "    df = add_interaction_risk_features(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Engineering features for test data...\")\n",
    "test_df_engineered = engineer_test_features(test_df_raw)\n",
    "print(f\"  Test columns after engineering: {len(test_df_engineered.columns)}\")\n",
    "\n",
    "# ============================================================\n",
    "# APPLY PREPROCESSING PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nTransforming datasets with fitted preprocessor...\")\n",
    "\n",
    "print(\"  Transforming training data...\")\n",
    "train_transformed = preprocessor.transform(train_df)\n",
    "print(f\"    Train samples: {train_transformed.count():,}\")\n",
    "\n",
    "print(\"  Transforming validation data...\")\n",
    "val_transformed = preprocessor.transform(val_df)\n",
    "print(f\"    Validation samples: {val_transformed.count():,}\")\n",
    "\n",
    "print(\"  Transforming test data...\")\n",
    "test_transformed = preprocessor.transform(test_df_engineered)\n",
    "print(f\"    Test samples: {test_transformed.count():,}\")\n",
    "\n",
    "print(\"\\nAll datasets transformed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparison Diagnostic\n",
    "\n",
    "Compare the distributions of train, validation, and test sets to detect potential data drift that could cause poor model generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:37.556671Z",
     "iopub.status.busy": "2026-02-07T06:38:37.556613Z",
     "iopub.status.idle": "2026-02-07T06:38:46.573110Z",
     "shell.execute_reply": "2026-02-07T06:38:46.572780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DISTRIBUTION COMPARISON: Train vs Validation vs Test\n",
      "================================================================================\n",
      "\n",
      "1. FRAUD RATE COMPARISON:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 103:========================>                             (12 + 14) / 26]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train:      0.5757%\n",
      "   Validation: 0.5250%\n",
      "   Test:       0.3860%\n",
      "   NOTE: Train and test fraud rates differ by >20% - this is expected from different time periods\n",
      "\n",
      "2. SAMPLE SIZE COMPARISON:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train:      1,034,987 samples\n",
      "   Validation: 122,480 samples\n",
      "   Test:       555,719 samples\n",
      "   Total:      1,713,186 samples\n",
      "\n",
      "3. FEATURE STATISTICS (sample of 10000 per split):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 136:==>                                                    (1 + 24) / 26]\r",
      "\r",
      "[Stage 136:==============================================>        (22 + 4) / 26]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 136:==================================================>    (24 + 2) / 26]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Split              Mean        Std        Min        Max\n",
      "   ----------------------------------------------------\n",
      "   Train           -0.0037     0.9831    -2.6052    67.7234\n",
      "   Validation      -0.0093     0.9490    -2.5516    47.1240\n",
      "   Test             0.0645     1.1564    -2.5572    48.9739\n",
      "\n",
      "4. DISTRIBUTION DRIFT CHECK:\n",
      "----------------------------------------\n",
      "   Train and test feature distributions are aligned\n",
      "      Mean difference: 0.0682\n",
      "      Std difference: 0.1734\n",
      "\n",
      "================================================================================\n",
      "Distribution comparison complete.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DISTRIBUTION COMPARISON ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUTION COMPARISON: Train vs Validation vs Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Fraud Rate Comparison (compute fresh)\n",
    "print(\"\\n1. FRAUD RATE COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "train_fraud_rate = train_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "val_fraud_rate = val_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "test_fraud_rate = test_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "\n",
    "print(f\"   Train:      {train_fraud_rate:.4%}\")\n",
    "print(f\"   Validation: {val_fraud_rate:.4%}\")\n",
    "print(f\"   Test:       {test_fraud_rate:.4%}\")\n",
    "\n",
    "# Flag if train/test fraud rate differs significantly\n",
    "if abs(train_fraud_rate - test_fraud_rate) / max(train_fraud_rate, test_fraud_rate) > 0.2:\n",
    "    print(\"   NOTE: Train and test fraud rates differ by >20% - this is expected from different time periods\")\n",
    "else:\n",
    "    print(\"   Train and test fraud rates are similar\")\n",
    "\n",
    "# 2. Sample Size Comparison\n",
    "print(\"\\n2. SAMPLE SIZE COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "train_count = train_transformed.count()\n",
    "val_count = val_transformed.count()\n",
    "test_count = test_transformed.count()\n",
    "print(f\"   Train:      {train_count:,} samples\")\n",
    "print(f\"   Validation: {val_count:,} samples\")\n",
    "print(f\"   Test:       {test_count:,} samples\")\n",
    "print(f\"   Total:      {train_count + val_count + test_count:,} samples\")\n",
    "\n",
    "# 3. Feature Vector Statistics (sample-based for efficiency)\n",
    "print(\"\\n3. FEATURE STATISTICS (sample of 10000 per split):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def get_feature_stats(df, name, sample_size=10000):\n",
    "    \"\"\"Get feature statistics from a sample of the DataFrame.\"\"\"\n",
    "    sample_df = df.select(\"features\").sample(False, min(1.0, sample_size / df.count()), seed=42).toPandas()\n",
    "    if len(sample_df) == 0:\n",
    "        return None\n",
    "    features = np.array([row.toArray() for row in sample_df[\"features\"]])\n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean': np.mean(features),\n",
    "        'std': np.std(features),\n",
    "        'min': np.min(features),\n",
    "        'max': np.max(features)\n",
    "    }\n",
    "\n",
    "train_stats = get_feature_stats(train_transformed, \"Train\")\n",
    "val_stats = get_feature_stats(val_transformed, \"Validation\")\n",
    "test_stats = get_feature_stats(test_transformed, \"Test\")\n",
    "\n",
    "print(f\"   {'Split':<12} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(f\"   {'-'*52}\")\n",
    "for stats in [train_stats, val_stats, test_stats]:\n",
    "    if stats:\n",
    "        print(f\"   {stats['name']:<12} {stats['mean']:>10.4f} {stats['std']:>10.4f} {stats['min']:>10.4f} {stats['max']:>10.4f}\")\n",
    "\n",
    "# 4. Check for significant distribution drift\n",
    "print(\"\\n4. DISTRIBUTION DRIFT CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "if train_stats and test_stats:\n",
    "    mean_diff = abs(train_stats['mean'] - test_stats['mean'])\n",
    "    std_diff = abs(train_stats['std'] - test_stats['std'])\n",
    "    \n",
    "    if mean_diff > 0.5 or std_diff > 0.5:\n",
    "        print(\"   NOTE: Some distribution drift detected (mean diff: {:.4f}, std diff: {:.4f})\".format(mean_diff, std_diff))\n",
    "    else:\n",
    "        print(\"   Train and test feature distributions are aligned\")\n",
    "        print(f\"      Mean difference: {mean_diff:.4f}\")\n",
    "        print(f\"      Std difference: {std_diff:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Distribution comparison complete.\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:46.574065Z",
     "iopub.status.busy": "2026-02-07T06:38:46.574008Z",
     "iopub.status.idle": "2026-02-07T06:38:52.012436Z",
     "shell.execute_reply": "2026-02-07T06:38:52.012098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training data to pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 157:>                                                      (0 + 24) / 26]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 157:>                                                      (0 + 25) / 26]\r",
      "\r",
      "[Stage 157:==============>                                        (7 + 19) / 26]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 157:==================================================>    (24 + 2) / 26]\r",
      "\r",
      "[Stage 157:====================================================>  (25 + 1) / 26]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1,034,987 samples, 30 features\n",
      "  Fraud cases: 5,958 (0.5757%)\n",
      "  Non-fraud cases: 1,029,029 (99.4243%)\n",
      "\n",
      "Using natural distribution (no SMOTE)\n",
      "  Class weight ratio: 172.7:1\n",
      "  Imbalance handled via cost-sensitive learning in downstream models\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CLASS IMBALANCE HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# Convert training data to pandas\n",
    "print(\"Converting training data to pandas...\")\n",
    "train_pd = train_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "\n",
    "X_train = np.array([row.toArray() for row in train_pd[\"features\"]])\n",
    "y_train = train_pd[\"is_fraud\"].values\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"  Fraud cases: {y_train.sum():,} ({y_train.mean():.4%})\")\n",
    "print(f\"  Non-fraud cases: {(y_train == 0).sum():,} ({(y_train == 0).mean():.4%})\")\n",
    "\n",
    "# ============================================================\n",
    "# SMOTE EXPERIMENTS (ALL REJECTED - PRESERVED FOR DOCUMENTATION)\n",
    "# ============================================================\n",
    "# See Cell 0 for full experiment results.\n",
    "# All SMOTE variants degraded test performance due to temporal distribution shift.\n",
    "# ============================================================\n",
    "\n",
    "# Adopted approach: natural distribution + class weights\n",
    "X_train_resampled = X_train\n",
    "y_train_resampled = y_train\n",
    "\n",
    "print(f\"\\nUsing natural distribution (no SMOTE)\")\n",
    "print(f\"  Class weight ratio: {(1 - y_train_resampled.mean()) / y_train_resampled.mean():.1f}:1\")\n",
    "print(\"  Imbalance handled via cost-sensitive learning in downstream models\")\n",
    "\n",
    "train_resampled_pd = pd.DataFrame(X_train_resampled)\n",
    "train_resampled_pd['is_fraud'] = y_train_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Pipeline Conversion & Saving\n",
    "\n",
    "Convert the fitted Spark MLlib pipeline into a sklearn-compatible pipeline for lightweight inference, then persist preprocessed datasets and pipeline artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Summary\n",
    "\n",
    "**Data Loading:**\n",
    "- Training data: Loaded from EDA Section 8 checkpoint (35 columns, geographic features)\n",
    "- Test data: Loaded separately from CSV (never used for fitting)\n",
    "\n",
    "**Row-Level Features (before split):**\n",
    "- day_of_week, is_peak_fraud_hour, is_peak_fraud_day, is_peak_fraud_season, is_high_risk_category\n",
    "- amount_bin, city_size, time_bin\n",
    "\n",
    "**Train/Validation Split:**\n",
    "- Time-aware split: train = first 80%, val = next 10%\n",
    "\n",
    "**Card-Level & Velocity Features (after split, point-in-time):**\n",
    "- card_age_days, transaction_count (cumulative backward windows)\n",
    "- txn_count_last_1h, txn_count_last_24h (backward time-range windows)\n",
    "- amt_relative_to_avg (expanding mean excluding current row)\n",
    "- Validation computed on train+val union to see training history\n",
    "\n",
    "**Interaction & Risk Features:**\n",
    "- evening_high_amount, evening_online_shopping, large_city_evening, new_card_evening, high_amount_online\n",
    "- temporal_risk_score, geographic_risk_score, card_risk_score, risk_tier\n",
    "\n",
    "**Feature Selection:**\n",
    "- ~30 features across 5 groups (Critical, High Priority, Interaction, Enriched, Raw + Velocity)\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "- Spark MLlib Pipeline with StringIndexer, Imputer, VectorAssembler, StandardScaler\n",
    "- Fitted only on training data, applied to all splits\n",
    "\n",
    "**Artifacts Saved:**\n",
    "- Preprocessed data: train, validation, test (parquet)\n",
    "- Spark MLlib pipeline, sklearn pipeline, feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-07T06:38:52.013460Z",
     "iopub.status.busy": "2026-02-07T06:38:52.013399Z",
     "iopub.status.idle": "2026-02-07T06:38:57.702874Z",
     "shell.execute_reply": "2026-02-07T06:38:57.702530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Spark MLlib pipeline to sklearn pipeline...\n",
      "  Pipeline conversion complete\n",
      "\n",
      "Saving preprocessed data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/train_preprocessed.parquet (1,034,987 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/val_preprocessed.parquet (122,480 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 164:==================================================>    (24 + 2) / 26]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/test_preprocessed.parquet (555,719 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark MLlib pipeline saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/spark_preprocessor\n",
      "Sklearn pipeline saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/sklearn_preprocessor.pkl\n",
      "Feature names saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/feature_names.pkl\n",
      "\n",
      "============================================================\n",
      "Preprocessing pipeline complete!\n",
      "============================================================\n",
      "  Selected features: 30\n",
      "  Train: 1,034,987 samples\n",
      "  Validation: 122,480 samples\n",
      "  Test: 555,719 samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PIPELINE CONVERSION & SAVING\n",
    "# ============================================================\n",
    "\n",
    "def convert_spark_to_sklearn_pipeline(\n",
    "    spark_preprocessor: SparkMLlibPreprocessor,\n",
    ") -> SklearnPipeline:\n",
    "    \"\"\"Convert Spark MLlib pipeline to sklearn pipeline for fast inference.\"\"\"\n",
    "    transformers = []\n",
    "\n",
    "    stage_idx = 0\n",
    "    for feat in spark_preprocessor.categorical_features:\n",
    "        indexer_model = spark_preprocessor.pipeline_model.stages[stage_idx]\n",
    "        labels = indexer_model.labels\n",
    "        le = SklearnLabelEncoder()\n",
    "        le.classes_ = np.array(labels)\n",
    "        transformers.append((f\"label_encoder_{feat}\", le, [feat]))\n",
    "        stage_idx += 1\n",
    "\n",
    "    if len(spark_preprocessor.numerical_features) > 0:\n",
    "        imputer_model = spark_preprocessor.pipeline_model.stages[stage_idx]\n",
    "        imputer = SklearnSimpleImputer(strategy=\"mean\")\n",
    "        try:\n",
    "            stats_row = imputer_model.surrogateDF.collect()[0]\n",
    "            stats_values = []\n",
    "            for feat in spark_preprocessor.numerical_features:\n",
    "                if feat in stats_row.asDict():\n",
    "                    stats_values.append(float(stats_row[feat]))\n",
    "                else:\n",
    "                    stats_values.append(0.0)\n",
    "            imputer.statistics_ = np.array(stats_values)\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not extract Imputer statistics: {e}\")\n",
    "            imputer.statistics_ = np.zeros(len(spark_preprocessor.numerical_features))\n",
    "        transformers.append((\"imputer\", imputer, spark_preprocessor.numerical_features))\n",
    "        stage_idx += 1\n",
    "\n",
    "    scaler_model = spark_preprocessor.pipeline_model.stages[-1]\n",
    "    scaler = SklearnStandardScaler(with_mean=True, with_std=True)\n",
    "    scaler.mean_ = np.array(scaler_model.mean.toArray())\n",
    "    scaler.scale_ = np.array(scaler_model.std.toArray())\n",
    "\n",
    "    column_transformer = ColumnTransformer(transformers, remainder=\"passthrough\")\n",
    "    sklearn_pipeline = SklearnPipeline([\n",
    "        (\"preprocessor\", column_transformer),\n",
    "        (\"scaler\", scaler),\n",
    "    ])\n",
    "    return sklearn_pipeline\n",
    "\n",
    "\n",
    "print(\"Converting Spark MLlib pipeline to sklearn pipeline...\")\n",
    "try:\n",
    "    sklearn_preprocessor = convert_spark_to_sklearn_pipeline(preprocessor)\n",
    "    print(\"  Pipeline conversion complete\")\n",
    "except Exception as e:\n",
    "    print(f\"  Pipeline conversion failed: {e}\")\n",
    "    sklearn_preprocessor = None\n",
    "\n",
    "# ============================================================\n",
    "# SAVE PREPROCESSED DATA\n",
    "# ============================================================\n",
    "print(\"\\nSaving preprocessed data...\")\n",
    "\n",
    "train_resampled_pd.to_parquet(PREPROCESSED_TRAIN_PATH, index=False)\n",
    "print(f\"  Train: {PREPROCESSED_TRAIN_PATH} ({len(train_resampled_pd):,} samples)\")\n",
    "\n",
    "val_pd = val_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "val_features = np.array([row.toArray() for row in val_pd[\"features\"]])\n",
    "val_preprocessed = pd.DataFrame(val_features)\n",
    "val_preprocessed['is_fraud'] = val_pd['is_fraud'].values\n",
    "val_preprocessed.to_parquet(PREPROCESSED_VAL_PATH, index=False)\n",
    "print(f\"  Validation: {PREPROCESSED_VAL_PATH} ({len(val_preprocessed):,} samples)\")\n",
    "\n",
    "test_pd = test_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "test_features = np.array([row.toArray() for row in test_pd[\"features\"]])\n",
    "test_preprocessed = pd.DataFrame(test_features)\n",
    "test_preprocessed['is_fraud'] = test_pd['is_fraud'].values\n",
    "test_preprocessed.to_parquet(PREPROCESSED_TEST_PATH, index=False)\n",
    "print(f\"  Test: {PREPROCESSED_TEST_PATH} ({len(test_preprocessed):,} samples)\")\n",
    "\n",
    "# Save Spark MLlib pipeline\n",
    "spark_pipeline_dir = str(SPARK_PREPROCESSER_PATH).replace('.pkl', '')\n",
    "preprocessor.pipeline_model.write().overwrite().save(spark_pipeline_dir)\n",
    "print(f\"\\nSpark MLlib pipeline saved: {spark_pipeline_dir}\")\n",
    "\n",
    "if sklearn_preprocessor is not None:\n",
    "    joblib.dump(sklearn_preprocessor, SKLEARN_PREPROCESSER_PATH)\n",
    "    print(f\"Sklearn pipeline saved: {SKLEARN_PREPROCESSER_PATH}\")\n",
    "\n",
    "with open(FEATURE_NAMES_PATH, 'wb') as f:\n",
    "    pickle.dump(preprocessor.get_feature_names(), f)\n",
    "print(f\"Feature names saved: {FEATURE_NAMES_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Preprocessing pipeline complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Selected features: {len(preprocessor.get_feature_names())}\")\n",
    "print(f\"  Train: {len(train_resampled_pd):,} samples\")\n",
    "print(f\"  Validation: {len(val_preprocessed):,} samples\")\n",
    "print(f\"  Test: {len(test_preprocessed):,} samples\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-shield)",
   "language": "python",
   "name": "fraud-shield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
