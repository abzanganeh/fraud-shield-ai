{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline - Experiment Documentation\n",
    "\n",
    "## Class Imbalance Handling Strategy\n",
    "\n",
    "This notebook handles the severe class imbalance (0.58% fraud rate) through **cost-sensitive learning** rather than synthetic oversampling.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiments Conducted\n",
    "\n",
    "### Experiment 1: SMOTE Oversampling (REJECTED)\n",
    "\n",
    "**Status:** Tested and rejected - code preserved below (commented out)\n",
    "\n",
    "**Configuration Tested:**\n",
    "```python\n",
    "# SMOTE(sampling_strategy=0.1, k_neighbors=5, random_state=42)\n",
    "# Result: 9.09% fraud rate in training data\n",
    "```\n",
    "\n",
    "**Results:**\n",
    "- Validation F1: 0.53\n",
    "- Test F1: **0.008** (catastrophic failure)\n",
    "- Reason: Distribution shift between synthetic training data and real test data\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 2: SMOTE Variants Testing (ALL REJECTED)\n",
    "\n",
    "We tested multiple SMOTE variants and sampling strategies:\n",
    "\n",
    "| Variant | Sampling | Test F1 | Verdict |\n",
    "|---------|----------|---------|---------|\n",
    "| No SMOTE (Baseline) | 0.58% | **0.290** | **BEST** |\n",
    "| SMOTE | 1% | 0.141 | Rejected |\n",
    "| SMOTE | 2% | 0.248 | Rejected |\n",
    "| SMOTE | 5% | 0.196 | Rejected |\n",
    "| SMOTE | 10% | 0.183 | Rejected |\n",
    "| BorderlineSMOTE | 5% | 0.095 | Rejected |\n",
    "| ADASYN | 5% | 0.088 | Rejected |\n",
    "| SMOTEENN | 5% | 0.265 | Rejected |\n",
    "| SMOTETomek | 5% | 0.182 | Rejected |\n",
    "\n",
    "**Conclusion:** All oversampling methods degraded test performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Experiment 3: Natural Distribution + Class Weights (ADOPTED)\n",
    "\n",
    "**Current Implementation:**\n",
    "\n",
    "- Training data uses **natural distribution** (0.58% fraud)\n",
    "- Class imbalance handled via:\n",
    "  - **XGBoost:** `scale_pos_weight = (1 - fraud_rate) / fraud_rate`\n",
    "  - **Deep Learning:** Focal Loss or Weighted BCE with `pos_weight`\n",
    "- Threshold optimized on validation set using PR curve\n",
    "\n",
    "**Results:**\n",
    "- Test F1: 0.29 (XGBoost), 0.37 (Deep Learning)\n",
    "- Test ROC-AUC: 0.76 - 0.80\n",
    "\n",
    "---\n",
    "\n",
    "## Why SMOTE Failed\n",
    "\n",
    "1. **Temporal Distribution Shift:** Test data from different time period has different fraud patterns\n",
    "2. **Feature Space Interpolation:** Synthetic samples don't capture real fraud behavior\n",
    "3. **Concept Drift:** Fraud patterns evolve; synthetic historical patterns don't transfer\n",
    "\n",
    "---\n",
    "\n",
    "## Code Organization\n",
    "\n",
    "The SMOTE experiment code is preserved below (Section 6) but **commented out**. The active code uses natural distribution with class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Local Setup Instructions\n",
    "\n",
    "## Prerequisites Checklist\n",
    "\n",
    "Before running this notebook, ensure you have completed the following setup:\n",
    "\n",
    "- [ ] **Java 11 installed** and `JAVA_HOME` configured\n",
    "  - macOS: `brew install openjdk@11`\n",
    "  - Set: `export JAVA_HOME=$(/usr/libexec/java_home -v 11)`\n",
    "- [ ] **Conda environment `fraud-shield` created and activated**\n",
    "  - Create: `conda env create -f environment.yml`\n",
    "  - Activate: `conda activate fraud-shield`\n",
    "- [ ] **PySpark verified working**\n",
    "  - Test: `python -c \"from pyspark.sql import SparkSession; print('PySpark OK')\"`\n",
    "- [ ] **Data directories created**\n",
    "  - `data/checkpoints/` - for EDA checkpoints\n",
    "  - `data/processed/` - for preprocessed data\n",
    "  - `models/` - for saved preprocessors\n",
    "- [ ] **EDA checkpoint available**\n",
    "  - Run `01-local-fraud-detection-eda.ipynb` first\n",
    "  - Checkpoint should exist: `data/checkpoints/section11_enriched_features.parquet`\n",
    "\n",
    "## Environment Activation\n",
    "\n",
    "```bash\n",
    "conda activate fraud-shield\n",
    "```\n",
    "\n",
    "## Checkpoint Requirements\n",
    "\n",
    "This notebook requires the Section 11 checkpoint from the EDA notebook:\n",
    "- `data/checkpoints/section11_enriched_features.parquet`\n",
    "\n",
    "**Checkpoint & Parquet Update Behavior:** Processed parquet files (train/val/test) and saved pipelines are updated when the notebook runs the relevant sections and the save path executes. They do not auto-update on a schedule. If you change the EDA checkpoint or preprocessing code, re-run the affected sections to refresh outputs.\n",
    "\n",
    "**Note:** This is a local execution version configured for the `fraud-shield` conda environment on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib Preprocessing Pipeline for Fraud Detection\n",
    "\n",
    "**Notebook:** 02-local-preprocessing.ipynb  \n",
    "**Objective:** Production-grade preprocessing using Spark MLlib for scalability\n",
    "\n",
    "**Architecture:**\n",
    "- Training data: Loaded from EDA checkpoint as Spark DataFrame\n",
    "- Test data: Loaded separately from `fraudTest.csv` (never used for fitting)\n",
    "- Preprocessing: Spark MLlib Pipeline (StringIndexer, VectorAssembler, StandardScaler, Imputer)\n",
    "- Class Imbalance: Handled via cost-sensitive learning (NOT SMOTE - see experiment documentation)\n",
    "- Output: Both Spark MLlib and sklearn pipelines saved for flexibility\n",
    "\n",
    "**Data Leakage Safeguards:**\n",
    "1. Test data loaded separately, never used for fitting\n",
    "2. Preprocessor fitted only on training data\n",
    "3. Natural data distribution preserved (no synthetic samples)\n",
    "4. Time-aware split prevents future data leakage\n",
    "\n",
    "**Class Imbalance Strategy (FINAL):**\n",
    "- **SMOTE:** Tested and rejected (see experiment log above)\n",
    "- **Adopted:** Natural distribution + class weights (scale_pos_weight, Focal Loss)\n",
    "- **Feature Selection:** Critical + High Priority + Interaction + Enriched features\n",
    "- **Time-aware Split:** train = first 80% of period, val = next 10%; test = separate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:33.749316Z",
     "iopub.status.busy": "2026-02-06T23:27:33.749263Z",
     "iopub.status.idle": "2026-02-06T23:27:34.182227Z",
     "shell.execute_reply": "2026-02-06T23:27:34.181811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# PySpark & Spark MLlib\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, to_timestamp, when, isnan, isnull, min as spark_min, max as spark_max, dayofweek, hour, month, datediff, lit, sum as spark_sum, count, floor, broadcast, first, trim\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Sklearn (for pipeline conversion and inference)\n",
    "from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder as SklearnLabelEncoder\n",
    "from sklearn.impute import SimpleImputer as SklearnSimpleImputer\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:34.195447Z",
     "iopub.status.busy": "2026-02-06T23:27:34.195330Z",
     "iopub.status.idle": "2026-02-06T23:27:34.200221Z",
     "shell.execute_reply": "2026-02-06T23:27:34.199912Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPARK MLlib PREPROCESSOR CLASS\n",
    "# ============================================================\n",
    "\n",
    "class SparkMLlibPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-grade preprocessing using Spark MLlib Pipeline.\n",
    "    Handles categorical encoding, feature assembly, scaling, and imputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names: List[str]):\n",
    "        self.feature_names = feature_names\n",
    "        self.categorical_features: List[str] = []\n",
    "        self.numerical_features: List[str] = []\n",
    "        self.pipeline: Optional[Pipeline] = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _identify_feature_types(self, spark_df: DataFrame) -> None:\n",
    "        \"\"\"Identify categorical vs numerical features from Spark DataFrame schema.\"\"\"\n",
    "        self.categorical_features = []\n",
    "        self.numerical_features = []\n",
    "        \n",
    "        missing = [f for f in self.feature_names if f not in spark_df.columns]\n",
    "        if missing:\n",
    "            print(f\"  WARNING: Features not in DataFrame (skipped for type inference): {missing}\")\n",
    "        \n",
    "        schema = spark_df.schema\n",
    "        for feat in self.feature_names:\n",
    "            if feat not in spark_df.columns:\n",
    "                continue\n",
    "            \n",
    "            field = schema[feat]\n",
    "            field_type = str(field.dataType)\n",
    "            \n",
    "            # Check if categorical (string type only)\n",
    "            # Binary and low-cardinality integers should be treated as numerical\n",
    "            # to preserve their numeric values through scaling\n",
    "            if 'StringType' in field_type or 'String' in field_type:\n",
    "                self.categorical_features.append(feat)\n",
    "            else:\n",
    "                # All numeric types (including binary 0/1) are numerical\n",
    "                self.numerical_features.append(feat)\n",
    "    \n",
    "    def fit(self, spark_df: DataFrame) -> 'SparkMLlibPreprocessor':\n",
    "        \"\"\"\n",
    "        Fit Spark MLlib pipeline on training data.\n",
    "        \n",
    "        Args:\n",
    "            spark_df: Training Spark DataFrame\n",
    "        \"\"\"\n",
    "        self._identify_feature_types(spark_df)\n",
    "        \n",
    "        stages = []\n",
    "        indexed_categorical = []\n",
    "        imputed_numerical = []\n",
    "        \n",
    "        # StringIndexer for categorical features\n",
    "        for feat in self.categorical_features:\n",
    "            indexer = StringIndexer(\n",
    "                inputCol=feat,\n",
    "                outputCol=f\"{feat}_indexed\",\n",
    "                handleInvalid=\"keep\"\n",
    "            )\n",
    "            stages.append(indexer)\n",
    "            indexed_categorical.append(f\"{feat}_indexed\")\n",
    "        \n",
    "        # Imputer for numerical features (handle missing values)\n",
    "        if len(self.numerical_features) > 0:\n",
    "            imputer = Imputer(\n",
    "                inputCols=self.numerical_features,\n",
    "                outputCols=[f\"{f}_imputed\" for f in self.numerical_features],\n",
    "                strategy=\"mean\"\n",
    "            )\n",
    "            stages.append(imputer)\n",
    "            imputed_numerical = [f\"{f}_imputed\" for f in self.numerical_features]\n",
    "        \n",
    "        # VectorAssembler to combine all features\n",
    "        assembler_inputs = indexed_categorical + imputed_numerical\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=assembler_inputs,\n",
    "            outputCol=\"features_raw\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        stages.append(assembler)\n",
    "        \n",
    "        # StandardScaler for feature scaling\n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features_raw\",\n",
    "            outputCol=\"features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        stages.append(scaler)\n",
    "        \n",
    "        # Create and fit pipeline\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.pipeline_model = self.pipeline.fit(spark_df)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, spark_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted pipeline.\n",
    "        \n",
    "        Args:\n",
    "            spark_df: Spark DataFrame to transform\n",
    "        \n",
    "        Returns:\n",
    "            Transformed Spark DataFrame with 'features' column\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        \n",
    "        return self.pipeline_model.transform(spark_df)\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get list of feature names.\"\"\"\n",
    "        return self.feature_names.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Setup & Configuration\n",
    "\n",
    "Configure project paths, directories, and Spark session. All paths are resolved relative to the project root for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:34.201300Z",
     "iopub.status.busy": "2026-02-06T23:27:34.201249Z",
     "iopub.status.idle": "2026-02-06T23:27:34.204080Z",
     "shell.execute_reply": "2026-02-06T23:27:34.203771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/alireza/Desktop/projects/fraud-shield-ai\n",
      "Data directory: /home/alireza/Desktop/projects/fraud-shield-ai/data\n",
      "Checkpoint directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/checkpoints\n",
      "Input directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/input\n",
      "Models directory: /home/alireza/Desktop/projects/fraud-shield-ai/models\n",
      "Processed data directory: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "if NOTEBOOK_DIR.name == \"local_notebooks\":\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CHECKPOINT_DIR = DATA_DIR / \"checkpoints\"\n",
    "INPUT_DIR = DATA_DIR / \"input\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Data paths\n",
    "CHECKPOINT_SECTION11 = CHECKPOINT_DIR / 'section11_enriched_features.parquet'\n",
    "TEST_DATA_PATH = INPUT_DIR / 'fraudTest.csv'\n",
    "\n",
    "# Output paths\n",
    "PREPROCESSED_TRAIN_PATH = PROCESSED_DATA_DIR / 'train_preprocessed.parquet'\n",
    "PREPROCESSED_VAL_PATH = PROCESSED_DATA_DIR / 'val_preprocessed.parquet'\n",
    "PREPROCESSED_TEST_PATH = PROCESSED_DATA_DIR / 'test_preprocessed.parquet'\n",
    "SPARK_PREPROCESSER_PATH = MODELS_DIR / 'spark_preprocessor.pkl'\n",
    "SKLEARN_PREPROCESSER_PATH = MODELS_DIR / 'sklearn_preprocessor.pkl'\n",
    "FEATURE_NAMES_PATH = MODELS_DIR / 'feature_names.pkl'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Input directory: {INPUT_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Processed data directory: {PROCESSED_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:34.204973Z",
     "iopub.status.busy": "2026-02-06T23:27:34.204926Z",
     "iopub.status.idle": "2026-02-06T23:27:34.206317Z",
     "shell.execute_reply": "2026-02-06T23:27:34.206170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alireza/anaconda3/envs/fraud-shield/bin/python\n"
     ]
    }
   ],
   "source": [
    "python_exec = sys.executable\n",
    "print(python_exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:34.207304Z",
     "iopub.status.busy": "2026-02-06T23:27:34.207258Z",
     "iopub.status.idle": "2026-02-06T23:27:35.466855Z",
     "shell.execute_reply": "2026-02-06T23:27:35.466614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ JAVA_HOME already set: /home/alireza/anaconda3/envs/fraud-shield/lib/jvm\n",
      "✓ Java path set (version check skipped)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:27:34 WARN Utils: Your hostname, zanganeh-ai resolves to a loopback address: 127.0.1.1; using 192.168.86.248 instead (on interface wlp129s0)\n",
      "26/02/06 15:27:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:27:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized with optimized memory settings\n",
      "Spark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SET JAVA_HOME FROM CONDA ENVIRONMENT\n",
    "# ============================================================\n",
    "\n",
    "# Stop any existing Spark session first\n",
    "try:\n",
    "    spark_existing = SparkSession.getActiveSession()\n",
    "    if spark_existing:\n",
    "        print(\"Stopping existing Spark session...\")\n",
    "        spark_existing.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Auto-detect JAVA_HOME from conda environment\n",
    "if 'JAVA_HOME' not in os.environ or not os.environ.get('JAVA_HOME'):\n",
    "    # Method 1: Try conda environment (check sys.executable path)\n",
    "    python_exec = sys.executable\n",
    "    if 'envs' in python_exec:\n",
    "        # Extract conda env path from Python executable\n",
    "        env_path = python_exec.split('envs/')[0] + 'envs/' + python_exec.split('envs/')[1].split('/')[0]\n",
    "        java_bin = os.path.join(env_path, 'bin', 'java')\n",
    "        if os.path.exists(java_bin):\n",
    "            os.environ['JAVA_HOME'] = env_path\n",
    "            print(f\"✓ JAVA_HOME set from conda environment: {env_path}\")\n",
    "        else:\n",
    "            # Method 2: Try CONDA_PREFIX if available\n",
    "            conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "            if conda_prefix:\n",
    "                java_bin = os.path.join(conda_prefix, 'bin', 'java')\n",
    "                if os.path.exists(java_bin):\n",
    "                    os.environ['JAVA_HOME'] = conda_prefix\n",
    "                    print(f\"✓ JAVA_HOME set from CONDA_PREFIX: {conda_prefix}\")\n",
    "                else:\n",
    "                    # Method 3: Find Java via which\n",
    "                    java_path = shutil.which('java')\n",
    "                    if java_path and os.path.exists(java_path):\n",
    "                        java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "                        os.environ['JAVA_HOME'] = java_home\n",
    "                        print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "                    else:\n",
    "                        raise RuntimeError(\n",
    "                            \"Java not found. Please install Java 11:\\n\"\n",
    "                            \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                            \"Then restart the Jupyter kernel.\"\n",
    "                        )\n",
    "            else:\n",
    "                # Method 3: Find Java via which\n",
    "                java_path = shutil.which('java')\n",
    "                if java_path and os.path.exists(java_path):\n",
    "                    java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "                    os.environ['JAVA_HOME'] = java_home\n",
    "                    print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "                else:\n",
    "                    raise RuntimeError(\n",
    "                        \"Java not found. Please install Java 11:\\n\"\n",
    "                        \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                        \"Then restart the Jupyter kernel.\"\n",
    "                    )\n",
    "    else:\n",
    "        # Not in conda, try system Java\n",
    "        java_path = shutil.which('java')\n",
    "        if java_path and os.path.exists(java_path):\n",
    "            java_home = os.path.dirname(os.path.dirname(java_path))\n",
    "            os.environ['JAVA_HOME'] = java_home\n",
    "            print(f\"✓ JAVA_HOME set from system Java: {java_home}\")\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                \"Java not found. Please install Java 11:\\n\"\n",
    "                \"  conda install -c conda-forge openjdk=11\\n\"\n",
    "                \"Then restart the Jupyter kernel.\"\n",
    "            )\n",
    "else:\n",
    "    print(f\"✓ JAVA_HOME already set: {os.environ['JAVA_HOME']}\")\n",
    "\n",
    "# Verify Java is accessible\n",
    "java_home = os.environ.get('JAVA_HOME')\n",
    "if java_home:\n",
    "    java_exe = os.path.join(java_home, 'bin', 'java')\n",
    "    if not os.path.exists(java_exe):\n",
    "        print(f\"⚠ Warning: Java executable not found at {java_exe}\")\n",
    "    else:\n",
    "        # Test Java version\n",
    "        try:\n",
    "            result = subprocess.run([java_exe, '-version'], capture_output=True, text=True, stderr=subprocess.STDOUT, timeout=5)\n",
    "            print(f\"✓ Java verified: {result.stdout.split(chr(10))[0] if result.stdout else 'Java is working'}\")\n",
    "        except:\n",
    "            print(\"✓ Java path set (version check skipped)\")\n",
    "\n",
    "# ============================================================\n",
    "# INITIALIZE SPARK SESSION\n",
    "# ============================================================\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetectionPreprocessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"10000\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized with optimized memory settings\")\n",
    "print(f\"Spark version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:35.467985Z",
     "iopub.status.busy": "2026-02-06T23:27:35.467920Z",
     "iopub.status.idle": "2026-02-06T23:27:37.991852Z",
     "shell.execute_reply": "2026-02-06T23:27:37.991624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imported timezone pipeline from scripts/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ZIP reference table created: 3,197 unique grid cells\n",
      "✓ TimezonePipeline instance created\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIMEZONE PIPELINE SETUP\n",
    "# ============================================================\n",
    "\n",
    "# Try to import from scripts (local/production)\n",
    "try:\n",
    "    import sys\n",
    "    if Path.cwd().name == \"local_notebooks\":\n",
    "        sys.path.insert(0, str(Path.cwd().parent))\n",
    "    from scripts.timezone_pipeline import TimezonePipeline\n",
    "    print(\"✓ Imported timezone pipeline from scripts/\")\n",
    "except ImportError:\n",
    "    # Fallback: define inline (for Kaggle/Colab)\n",
    "    print(\"⚠️ Could not import from scripts/, defining inline...\")\n",
    "    # [Copy class definitions here if needed for Kaggle/Colab]\n",
    "    print(\"✓ Timezone pipeline defined inline\")\n",
    "\n",
    "# Build ZIP reference table (same as EDA notebook)\n",
    "GRID_SIZE = 0.5  # ~50km grid resolution\n",
    "full_zip_path = os.path.join(INPUT_DIR, \"uszips.csv\")\n",
    "\n",
    "if os.path.exists(full_zip_path):\n",
    "    zip_ref_df = (\n",
    "        spark.read.csv(full_zip_path, header=True, inferSchema=True)\n",
    "        .withColumnRenamed(\"zip\", \"zip_ref\")\n",
    "        .withColumn(\"lat_grid\", floor(col(\"lat\") / GRID_SIZE))\n",
    "        .withColumn(\"lng_grid\", floor(col(\"lng\") / GRID_SIZE))\n",
    "        .select(\"lat_grid\", \"lng_grid\", \"timezone\")\n",
    "        .filter(\n",
    "            (trim(col(\"timezone\")) != \"\") &\n",
    "            (col(\"timezone\") != \"FALSE\") &\n",
    "            col(\"timezone\").rlike(\"^[A-Za-z_/]+$\")\n",
    "        )\n",
    "        .distinct()\n",
    "        .groupBy(\"lat_grid\", \"lng_grid\")\n",
    "        .agg(first(\"timezone\").alias(\"timezone\"))\n",
    "        .cache()\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ZIP reference table created: {zip_ref_df.count():,} unique grid cells\")\n",
    "    \n",
    "    # Create TimezonePipeline instance\n",
    "    timezone_pipeline = TimezonePipeline(\n",
    "        zip_ref_df=zip_ref_df,\n",
    "        grid_size=GRID_SIZE\n",
    "    )\n",
    "    print(\"✓ TimezonePipeline instance created\")\n",
    "else:\n",
    "    print(f\"⚠️ Warning: uszips.csv not found at {full_zip_path}\")\n",
    "    print(\"  Timezone conversion will be skipped. Ensure EDA notebook has been run first.\")\n",
    "    timezone_pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Loading\n",
    "\n",
    "Load training data from EDA checkpoint and test data from CSV. These are kept separate to ensure test data is never used for fitting, preventing data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:37.992942Z",
     "iopub.status.busy": "2026-02-06T23:27:37.992879Z",
     "iopub.status.idle": "2026-02-06T23:27:43.521949Z",
     "shell.execute_reply": "2026-02-06T23:27:43.521714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from checkpoint: /home/alireza/Desktop/projects/fraud-shield-ai/data/checkpoints/section11_enriched_features.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/06 15:27:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 8:===============================================>         (20 + 4) / 24]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training data loaded successfully\n",
      "  Rows: 1,296,675\n",
      "  Columns: 57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Target distribution:\n",
      "    is_fraud = Yes: 7,506\n",
      "    is_fraud = No: 1,289,169\n",
      "  Fraud rate: 0.5789%\n",
      "Loading test data from: /home/alireza/Desktop/projects/fraud-shield-ai/data/input/fraudTest.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Test data loaded successfully\n",
      "  Rows: 555,719\n",
      "  Columns: 23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Target distribution:\n",
      "    is_fraud = Yes: 2,145\n",
      "    is_fraud = No: 553,574\n",
      "  Fraud rate: 0.3860%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def load_training_data() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load training data from EDA checkpoint as Spark DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame with all engineered features from EDA notebook\n",
    "    \"\"\"\n",
    "    if not CHECKPOINT_SECTION11.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"EDA checkpoint not found: {CHECKPOINT_SECTION11}\\n\"\n",
    "            \"Please run the EDA notebook (01-local-fraud-detection-eda.ipynb) first.\"\n",
    "        )\n",
    "    \n",
    "    # Defensive: avoid stale file listings if the checkpoint folder is regenerated.\n",
    "    # If the underlying parquet parts change, Spark can keep a stale reference and crash with SparkFileNotFoundException.\n",
    "    spark.catalog.clearCache()\n",
    "    try:\n",
    "        spark.catalog.refreshByPath(str(CHECKPOINT_SECTION11))\n",
    "    except (AnalysisException, AttributeError):\n",
    "        pass\n",
    "\n",
    "    print(f\"Loading training data from checkpoint: {CHECKPOINT_SECTION11}\")\n",
    "    # Cache + materialize once so later actions don't depend on the source files.\n",
    "    spark_df = spark.read.parquet(str(CHECKPOINT_SECTION11)).cache()\n",
    "    total_rows = spark_df.count()\n",
    "\n",
    "    print(\"✓ Training data loaded successfully\")\n",
    "    print(f\"  Rows: {total_rows:,}\")\n",
    "    print(f\"  Columns: {len(spark_df.columns)}\")\n",
    "\n",
    "    fraud_counts = spark_df.groupBy(\"is_fraud\").count().collect()\n",
    "    fraud_yes = spark_df.filter(col(\"is_fraud\") == 1).count()\n",
    "    fraud_rate = fraud_yes / total_rows if total_rows else 0.0\n",
    "    \n",
    "    print(f\"\\n  Target distribution:\")\n",
    "    for row in fraud_counts:\n",
    "        print(f\"    is_fraud = {'No' if row['is_fraud']==0 else 'Yes'}: {row['count']:,}\")\n",
    "    print(f\"  Fraud rate: {fraud_rate:.4%}\")\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "\n",
    "def load_test_data() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load test data from CSV as Spark DataFrame.\n",
    "    This data is never used for fitting to prevent data leakage.\n",
    "    \n",
    "    Returns:\n",
    "        Spark DataFrame with test data\n",
    "    \"\"\"\n",
    "    if not TEST_DATA_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Test data not found: {TEST_DATA_PATH}\\n\"\n",
    "            \"Please ensure fraudTest.csv is in the data/input/ directory.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Loading test data from: {TEST_DATA_PATH}\")\n",
    "    spark_df = spark.read.csv(\n",
    "        str(TEST_DATA_PATH),\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Test data loaded successfully\")\n",
    "    print(f\"  Rows: {spark_df.count():,}\")\n",
    "    print(f\"  Columns: {len(spark_df.columns)}\")\n",
    "    \n",
    "    if \"is_fraud\" in spark_df.columns:\n",
    "        fraud_counts = spark_df.groupBy(\"is_fraud\").count().collect()\n",
    "        fraud_rate = spark_df.filter(col(\"is_fraud\") == 1).count() / spark_df.count()\n",
    "        \n",
    "        print(f\"\\n  Target distribution:\")\n",
    "        for row in fraud_counts:\n",
    "            print(f\"    is_fraud = {'No' if row['is_fraud']==0 else 'Yes'}: {row['count']:,}\")\n",
    "        print(f\"  Fraud rate: {fraud_rate:.4%}\")\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_df = load_training_data()\n",
    "test_df_raw = load_test_data()\n",
    "\n",
    "# ============================================================\n",
    "# CREATE MISSING TEMPORAL FEATURES\n",
    "# ============================================================\n",
    "\n",
    "# Create day_of_week from merchant_local_time if missing\n",
    "if \"day_of_week\" not in train_df.columns and \"merchant_local_time\" in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"day_of_week\",\n",
    "        dayofweek(to_timestamp(col(\"merchant_local_time\")))\n",
    "    )\n",
    "    print(\"✓ Created day_of_week from merchant_local_time\")\n",
    "\n",
    "# Create is_peak_fraud_day from day_of_week if missing\n",
    "if \"is_peak_fraud_day\" not in train_df.columns and \"day_of_week\" in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"is_peak_fraud_day\",\n",
    "        when(col(\"day_of_week\").isin([4, 5, 6]), 1).otherwise(0)  # Wed=4, Thu=5, Fri=6\n",
    "    )\n",
    "    print(\"✓ Created is_peak_fraud_day (Wednesday-Friday)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Selection\n",
    "\n",
    "Select features based on EDA findings. Features are categorized by priority: Critical, High Priority, Interaction, and Enriched features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:43.522669Z",
     "iopub.status.busy": "2026-02-06T23:27:43.522597Z",
     "iopub.status.idle": "2026-02-06T23:27:43.525917Z",
     "shell.execute_reply": "2026-02-06T23:27:43.525730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Selection Summary:\n",
      "  Critical features: 7/7\n",
      "  High priority features: 8/8\n",
      "  Interaction features: 5/5\n",
      "  Enriched features: 4/4\n",
      "  Experiment 2 features: 0/0 (disabled for now)\n",
      "\n",
      "  Total selected: 24 features\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FEATURE SELECTION\n",
    "# ============================================================\n",
    "\n",
    "CRITICAL_FEATURES = [\n",
    "    'transaction_count_bin',\n",
    "    'card_age_bin',\n",
    "    'hour',\n",
    "    'time_bin',\n",
    "    'is_peak_fraud_hour',\n",
    "    'is_new_card',\n",
    "    'is_low_volume_card'\n",
    "]\n",
    "\n",
    "HIGH_PRIORITY_FEATURES = [\n",
    "    'category',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'is_peak_fraud_day',\n",
    "    'is_peak_fraud_season',\n",
    "    'is_high_risk_category',\n",
    "    'card_age_days',\n",
    "    'transaction_count'\n",
    "]\n",
    "\n",
    "INTERACTION_FEATURES = [\n",
    "    'evening_high_amount',\n",
    "    'evening_online_shopping',\n",
    "    'large_city_evening',\n",
    "    'new_card_evening',\n",
    "    'high_amount_online'\n",
    "]\n",
    "\n",
    "ENRICHED_FEATURES = [\n",
    "    'temporal_risk_score',\n",
    "    'geographic_risk_score',\n",
    "    'card_risk_score',\n",
    "    'risk_tier'\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 2: NEW INDUSTRY-STANDARD FEATURES\n",
    "# NOTE: These features require recomputation for test data.\n",
    "#       Temporarily disabled to validate core SMOTE fix first.\n",
    "#       TODO: Enable after adding feature computation in engineer_test_features()\n",
    "# ============================================================\n",
    "# EXPERIMENT2_FEATURES = [\n",
    "#     'customer_merchant_distance_km',  # Distance between customer and merchant\n",
    "#     'txn_count_last_1h',              # Velocity: transactions in last hour\n",
    "#     'amt_relative_to_avg'             # Amount deviation from user's historical average\n",
    "# ]\n",
    "EXPERIMENT2_FEATURES = []  # Disabled for Experiment 2a (SMOTE fix validation)\n",
    "\n",
    "# Combine all feature groups\n",
    "SELECTED_FEATURES = (\n",
    "    CRITICAL_FEATURES + \n",
    "    HIGH_PRIORITY_FEATURES + \n",
    "    INTERACTION_FEATURES + \n",
    "    ENRICHED_FEATURES +\n",
    "    EXPERIMENT2_FEATURES\n",
    ")\n",
    "\n",
    "# Filter to only features that exist in the Spark DataFrame\n",
    "available_features = [f for f in SELECTED_FEATURES if f in train_df.columns]\n",
    "missing_features = [f for f in SELECTED_FEATURES if f not in train_df.columns]\n",
    "\n",
    "print(\"Feature Selection Summary:\")\n",
    "print(f\"  Critical features: {len([f for f in CRITICAL_FEATURES if f in train_df.columns])}/{len(CRITICAL_FEATURES)}\")\n",
    "print(f\"  High priority features: {len([f for f in HIGH_PRIORITY_FEATURES if f in train_df.columns])}/{len(HIGH_PRIORITY_FEATURES)}\")\n",
    "print(f\"  Interaction features: {len([f for f in INTERACTION_FEATURES if f in train_df.columns])}/{len(INTERACTION_FEATURES)}\")\n",
    "print(f\"  Enriched features: {len([f for f in ENRICHED_FEATURES if f in train_df.columns])}/{len(ENRICHED_FEATURES)}\")\n",
    "print(f\"  Experiment 2 features: {len([f for f in EXPERIMENT2_FEATURES if f in train_df.columns])}/{len(EXPERIMENT2_FEATURES)} (disabled for now)\")\n",
    "print(f\"\\n  Total selected: {len(available_features)} features\")\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n  ⚠ Missing features (will be skipped): {missing_features}\")\n",
    "\n",
    "feature_names = available_features.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Train/Validation Split\n",
    "\n",
    "Perform time-aware splitting to respect temporal order. This prevents future data leakage and ensures realistic model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:43.526918Z",
     "iopub.status.busy": "2026-02-06T23:27:43.526858Z",
     "iopub.status.idle": "2026-02-06T23:27:44.298730Z",
     "shell.execute_reply": "2026-02-06T23:27:44.298494Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Data date range: 2018-12-31 to 2020-06-21\n",
      "  Train: first 80% of period (end 2020-03-04)\n",
      "  Validation: next 10% of period (end 2020-04-27)\n",
      "Time-aware split summary:\n",
      "  Train: 1,034,987 samples\n",
      "  Validation: 122,480 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train date range: 2018-12-31 14:22:06 to 2020-03-04 14:21:42\n",
      "  Validation date range: 2020-03-04 14:23:10 to 2020-04-27 14:21:51\n",
      "\n",
      "Fraud rates:\n",
      "  Train: 0.5757% (5,958 fraud cases)\n",
      "  Validation: 0.5250% (643 fraud cases)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TIME-AWARE DATA SPLITTING (Spark SQL)\n",
    "# ============================================================\n",
    "\n",
    "def split_data_time_aware(\n",
    "    spark_df: DataFrame,\n",
    "    date_col: str = 'merchant_local_time',\n",
    "    train_frac: float = 0.8,\n",
    "    val_frac: float = 0.9\n",
    ") -> Tuple[DataFrame, DataFrame]:\n",
    "    \"\"\"\n",
    "    Split training data into train and validation sets using time-aware split.\n",
    "    Split is derived from the actual date range in the data (no hardcoded dates).\n",
    "    Test data is loaded separately and never used for fitting.\n",
    "    \n",
    "    Uses merchant_local_time (timezone-aware) by default, as created in EDA notebook.\n",
    "    Falls back to customer_local_time or trans_date_trans_time if needed.\n",
    "    \n",
    "    Args:\n",
    "        spark_df: Input Spark DataFrame\n",
    "        date_col: Name of date column (default: 'merchant_local_time')\n",
    "        train_frac: Fraction of date range for training (default 0.8 = first 80%)\n",
    "        val_frac: Cumulative fraction for validation end (default 0.9 = train 80%, val 10%)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_df, val_df) as Spark DataFrames\n",
    "    \"\"\"\n",
    "    # Find the date column - prioritize timezone-aware columns from EDA\n",
    "    date_col_found = None\n",
    "    if date_col in spark_df.columns:\n",
    "        date_col_found = date_col\n",
    "    else:\n",
    "        # Try timezone-aware columns first (from EDA), then fallback to raw timestamp\n",
    "        for col_name in ['merchant_local_time', 'customer_local_time', 'trans_date_trans_time']:\n",
    "            if col_name in spark_df.columns:\n",
    "                date_col_found = col_name\n",
    "                print(f\"  Using date column: {col_name}\")\n",
    "                break\n",
    "    \n",
    "    if date_col_found is None:\n",
    "        date_like_cols = [c for c in spark_df.columns if 'date' in c.lower() or 'time' in c.lower()]\n",
    "        raise ValueError(\n",
    "            f\"Date column '{date_col}' not found in DataFrame.\\n\"\n",
    "            f\"Available date-like columns: {date_like_cols}\"\n",
    "        )\n",
    "    \n",
    "    date_col = date_col_found\n",
    "    \n",
    "    # Convert date column to timestamp if needed\n",
    "    spark_df = spark_df.withColumn(\n",
    "        date_col,\n",
    "        to_timestamp(col(date_col))\n",
    "    )\n",
    "    \n",
    "    # Auto-detect date range from data to validate split dates\n",
    "    actual_min_date = spark_df.select(spark_min(col(date_col))).collect()[0][0]\n",
    "    actual_max_date = spark_df.select(spark_max(col(date_col))).collect()[0][0]\n",
    "    \n",
    "    if actual_min_date is None or actual_max_date is None:\n",
    "        raise ValueError(f\"Date column '{date_col}' contains no valid dates\")\n",
    "    \n",
    "    actual_min_dt = pd.to_datetime(actual_min_date)\n",
    "    actual_max_dt = pd.to_datetime(actual_max_date)\n",
    "    \n",
    "    if not 0 < train_frac < val_frac <= 1.0:\n",
    "        raise ValueError(\"Require 0 < train_frac < val_frac <= 1.0\")\n",
    "    data_span_days = (actual_max_dt - actual_min_dt).days\n",
    "    train_end_dt = actual_min_dt + pd.Timedelta(days=int(data_span_days * train_frac))\n",
    "    val_end_dt = actual_min_dt + pd.Timedelta(days=int(data_span_days * val_frac))\n",
    "    \n",
    "    print(f\"  Data date range: {actual_min_dt.date()} to {actual_max_dt.date()}\")\n",
    "    print(f\"  Train: first {train_frac:.0%} of period (end {train_end_dt.date()})\")\n",
    "    print(f\"  Validation: next {val_frac - train_frac:.0%} of period (end {val_end_dt.date()})\")\n",
    "    \n",
    "    # Split by date using Spark SQL filters\n",
    "    train_df = spark_df.filter(col(date_col) <= train_end_dt)\n",
    "    val_df = spark_df.filter(\n",
    "        (col(date_col) > train_end_dt) & (col(date_col) <= val_end_dt)\n",
    "    )\n",
    "    \n",
    "    train_count = train_df.count()\n",
    "    val_count = val_df.count()\n",
    "    \n",
    "    print(\"Time-aware split summary:\")\n",
    "    print(f\"  Train: {train_count:,} samples\")\n",
    "    print(f\"  Validation: {val_count:,} samples\")\n",
    "    \n",
    "    # Check for empty splits before accessing date ranges\n",
    "    if train_count == 0 or val_count == 0:\n",
    "        print(f\"\\n⚠ Warning: Empty split detected!\")\n",
    "        print(f\"  Date column used: {date_col}\")\n",
    "        print(f\"  train_frac: {train_frac}, val_frac: {val_frac}\")\n",
    "        \n",
    "        # Show actual date range in data\n",
    "        actual_dates = spark_df.select(\n",
    "            spark_min(col(date_col)).alias(\"min_date\"),\n",
    "            spark_max(col(date_col)).alias(\"max_date\")\n",
    "        ).collect()[0]\n",
    "        if actual_dates[0] is not None:\n",
    "            print(f\"  Actual data date range: {actual_dates[0]} to {actual_dates[1]}\")\n",
    "        \n",
    "        raise ValueError(\n",
    "            f\"Time-aware split resulted in empty datasets. \"\n",
    "            f\"Check date column '{date_col}' and date ranges.\"\n",
    "        )\n",
    "    \n",
    "    # Get date ranges (FIX: use separate min/max calls, not duplicate dict keys)\n",
    "    train_date_stats = train_df.select(\n",
    "        spark_min(col(date_col)).alias(\"min_date\"),\n",
    "        spark_max(col(date_col)).alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    val_date_stats = val_df.select(\n",
    "        spark_min(col(date_col)).alias(\"min_date\"),\n",
    "        spark_max(col(date_col)).alias(\"max_date\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"  Train date range: {train_date_stats['min_date']} to {train_date_stats['max_date']}\")\n",
    "    print(f\"  Validation date range: {val_date_stats['min_date']} to {val_date_stats['max_date']}\")\n",
    "    \n",
    "    # Calculate fraud rates\n",
    "    train_fraud_count = train_df.filter(col(\"is_fraud\") == 1).count()\n",
    "    val_fraud_count = val_df.filter(col(\"is_fraud\") == 1).count()\n",
    "    \n",
    "    print(f\"\\nFraud rates:\")\n",
    "    print(f\"  Train: {train_fraud_count/train_count:.4%} ({train_fraud_count:,} fraud cases)\")\n",
    "    print(f\"  Validation: {val_fraud_count/val_count:.4%} ({val_fraud_count:,} fraud cases)\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "# Perform time-aware split on training data\n",
    "train_df, val_df = split_data_time_aware(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Spark MLlib Preprocessing Pipeline\n",
    "\n",
    "Create a production-grade Spark MLlib preprocessing pipeline with StringIndexer for categorical encoding, VectorAssembler for feature vectors, StandardScaler for numerical scaling, and Imputer for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:44.299609Z",
     "iopub.status.busy": "2026-02-06T23:27:44.299542Z",
     "iopub.status.idle": "2026-02-06T23:27:46.257459Z",
     "shell.execute_reply": "2026-02-06T23:27:46.257230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark MLlib Preprocessor fitted successfully\n",
      "  Categorical features: 5\n",
      "  Numerical features: 19\n",
      "  Total features: 24\n",
      "  Pipeline stages: 8\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SPARK MLlib PREPROCESSOR CLASS\n",
    "# ============================================================\n",
    "\n",
    "class SparkMLlibPreprocessor:\n",
    "    \"\"\"\n",
    "    Production-grade preprocessing using Spark MLlib Pipeline.\n",
    "    Handles categorical encoding, feature assembly, scaling, and imputation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names: List[str]):\n",
    "        self.feature_names = feature_names\n",
    "        self.categorical_features: List[str] = []\n",
    "        self.numerical_features: List[str] = []\n",
    "        self.pipeline: Optional[Pipeline] = None\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def _identify_feature_types(self, spark_df: DataFrame) -> None:\n",
    "        \"\"\"Identify categorical vs numerical features from Spark DataFrame schema.\"\"\"\n",
    "        self.categorical_features = []\n",
    "        self.numerical_features = []\n",
    "        \n",
    "        missing = [f for f in self.feature_names if f not in spark_df.columns]\n",
    "        if missing:\n",
    "            print(f\"  WARNING: Features not in DataFrame (skipped for type inference): {missing}\")\n",
    "        \n",
    "        schema = spark_df.schema\n",
    "        for feat in self.feature_names:\n",
    "            if feat not in spark_df.columns:\n",
    "                continue\n",
    "            \n",
    "            field = schema[feat]\n",
    "            field_type = str(field.dataType)\n",
    "            \n",
    "            # Check if categorical (string type only)\n",
    "            # Binary and low-cardinality integers should be treated as numerical\n",
    "            if 'StringType' in field_type or 'String' in field_type:\n",
    "                self.categorical_features.append(feat)\n",
    "            else:\n",
    "                # All numeric types (including binary 0/1) are numerical\n",
    "                self.numerical_features.append(feat)\n",
    "    \n",
    "    def fit(self, spark_df: DataFrame) -> 'SparkMLlibPreprocessor':\n",
    "        \"\"\"\n",
    "        Fit Spark MLlib pipeline on training data.\n",
    "        \n",
    "        Args:\n",
    "            spark_df: Training Spark DataFrame\n",
    "        \"\"\"\n",
    "        self._identify_feature_types(spark_df)\n",
    "        \n",
    "        stages = []\n",
    "        indexed_categorical = []\n",
    "        imputed_numerical = []\n",
    "        \n",
    "        # StringIndexer for categorical features\n",
    "        for feat in self.categorical_features:\n",
    "            indexer = StringIndexer(\n",
    "                inputCol=feat,\n",
    "                outputCol=f\"{feat}_indexed\",\n",
    "                handleInvalid=\"keep\"\n",
    "            )\n",
    "            stages.append(indexer)\n",
    "            indexed_categorical.append(f\"{feat}_indexed\")\n",
    "        \n",
    "        # Imputer for numerical features (handle missing values)\n",
    "        if len(self.numerical_features) > 0:\n",
    "            imputer = Imputer(\n",
    "                inputCols=self.numerical_features,\n",
    "                outputCols=[f\"{f}_imputed\" for f in self.numerical_features],\n",
    "                strategy=\"mean\"\n",
    "            )\n",
    "            stages.append(imputer)\n",
    "            imputed_numerical = [f\"{f}_imputed\" for f in self.numerical_features]\n",
    "        \n",
    "        # VectorAssembler to combine all features\n",
    "        assembler_inputs = indexed_categorical + imputed_numerical\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=assembler_inputs,\n",
    "            outputCol=\"features_raw\",\n",
    "            handleInvalid=\"skip\"\n",
    "        )\n",
    "        stages.append(assembler)\n",
    "        \n",
    "        # StandardScaler for feature scaling\n",
    "        scaler = StandardScaler(\n",
    "            inputCol=\"features_raw\",\n",
    "            outputCol=\"features\",\n",
    "            withMean=True,\n",
    "            withStd=True\n",
    "        )\n",
    "        stages.append(scaler)\n",
    "        \n",
    "        # Create and fit pipeline\n",
    "        self.pipeline = Pipeline(stages=stages)\n",
    "        self.pipeline_model = self.pipeline.fit(spark_df)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, spark_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Transform data using fitted pipeline.\n",
    "        \n",
    "        Args:\n",
    "            spark_df: Spark DataFrame to transform\n",
    "        \n",
    "        Returns:\n",
    "            Transformed Spark DataFrame with 'features' column\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "        \n",
    "        return self.pipeline_model.transform(spark_df)\n",
    "    \n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        \"\"\"Get list of feature names.\"\"\"\n",
    "        return self.feature_names.copy()\n",
    "\n",
    "\n",
    "# Initialize and fit preprocessor\n",
    "preprocessor = SparkMLlibPreprocessor(feature_names)\n",
    "preprocessor.fit(train_df)\n",
    "\n",
    "print(\"Spark MLlib Preprocessor fitted successfully\")\n",
    "print(f\"  Categorical features: {len(preprocessor.categorical_features)}\")\n",
    "print(f\"  Numerical features: {len(preprocessor.numerical_features)}\")\n",
    "print(f\"  Total features: {len(preprocessor.get_feature_names())}\")\n",
    "print(f\"  Pipeline stages: {len(preprocessor.pipeline_model.stages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Apply Preprocessing Pipeline\n",
    "\n",
    "Transform train, validation, and test splits using the fitted Spark MLlib pipeline. The pipeline ensures consistent preprocessing across all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:46.258236Z",
     "iopub.status.busy": "2026-02-06T23:27:46.258156Z",
     "iopub.status.idle": "2026-02-06T23:27:46.444968Z",
     "shell.execute_reply": "2026-02-06T23:27:46.444757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for test data...\n",
      "  Created merchant_local_time from trans_date_trans_time\n",
      "  Created hour\n",
      "  Created day_of_week\n",
      "  Created month\n",
      "  Created time_bin\n",
      "  Created is_peak_fraud_hour\n",
      "  Created is_peak_fraud_day\n",
      "  Created is_peak_fraud_season\n",
      "  Created city_size\n",
      "  Created amount_bin\n",
      "  Created card_age_days\n",
      "  Created card_age_bin\n",
      "  Created transaction_count\n",
      "  Created transaction_count_bin\n",
      "  Created is_new_card\n",
      "  Created is_low_volume_card\n",
      "  Created is_high_risk_category\n",
      "  Created evening_high_amount\n",
      "  Created evening_online_shopping\n",
      "  Created large_city_evening\n",
      "  Created new_card_evening\n",
      "  Created high_amount_online\n",
      "  Created temporal_risk_score\n",
      "  Created geographic_risk_score\n",
      "  Created card_risk_score\n",
      "  Created risk_tier\n",
      "✓ Test data feature engineering complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ENGINEER FEATURES FOR TEST DATA\n",
    "# FIXED: Match EXACT definitions from notebook 01\n",
    "# ============================================================\n",
    "\n",
    "def engineer_test_features(test_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add engineered features to test data.\n",
    "    CRITICAL: Feature definitions MUST match training data exactly!\n",
    "    \"\"\"\n",
    "    df = test_df\n",
    "    \n",
    "    # Convert trans_date_trans_time to timestamp if needed\n",
    "    if \"trans_date_trans_time\" in df.columns:\n",
    "        df = df.withColumn(\"trans_date_trans_time\", to_timestamp(col(\"trans_date_trans_time\")))\n",
    "    \n",
    "    # Merchant local time (use trans_date_trans_time as fallback)\n",
    "    if \"merchant_local_time\" not in df.columns:\n",
    "        if \"trans_date_trans_time\" in df.columns:\n",
    "            df = df.withColumn(\"merchant_local_time\", col(\"trans_date_trans_time\"))\n",
    "            print(\"  Created merchant_local_time from trans_date_trans_time\")\n",
    "    \n",
    "    # Temporal features\n",
    "    if \"hour\" not in df.columns and \"merchant_local_time\" in df.columns:\n",
    "        df = df.withColumn(\"hour\", hour(to_timestamp(col(\"merchant_local_time\"))))\n",
    "        print(\"  Created hour\")\n",
    "    \n",
    "    if \"day_of_week\" not in df.columns and \"merchant_local_time\" in df.columns:\n",
    "        df = df.withColumn(\"day_of_week\", dayofweek(to_timestamp(col(\"merchant_local_time\"))))\n",
    "        print(\"  Created day_of_week\")\n",
    "    \n",
    "    if \"month\" not in df.columns and \"merchant_local_time\" in df.columns:\n",
    "        df = df.withColumn(\"month\", month(to_timestamp(col(\"merchant_local_time\"))))\n",
    "        print(\"  Created month\")\n",
    "    \n",
    "    # time_bin: Morning (0-11), Afternoon (12-17), Evening (18-23), Night (0-5)\n",
    "    # Match notebook 01 definition\n",
    "    if \"time_bin\" not in df.columns and \"hour\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"time_bin\",\n",
    "            when(col(\"hour\").between(6, 11), \"Morning\")\n",
    "            .when(col(\"hour\").between(12, 17), \"Afternoon\")\n",
    "            .when(col(\"hour\").between(18, 23), \"Evening\")\n",
    "            .otherwise(\"Night\")\n",
    "        )\n",
    "        print(\"  Created time_bin\")\n",
    "    \n",
    "    # is_peak_fraud_hour: evening hours 18-23\n",
    "    if \"is_peak_fraud_hour\" not in df.columns and \"hour\" in df.columns:\n",
    "        df = df.withColumn(\"is_peak_fraud_hour\", when(col(\"hour\").between(18, 23), 1).otherwise(0))\n",
    "        print(\"  Created is_peak_fraud_hour\")\n",
    "    \n",
    "    # is_peak_fraud_day: Wednesday-Friday (dayofweek: 4,5,6 in Spark)\n",
    "    if \"is_peak_fraud_day\" not in df.columns and \"day_of_week\" in df.columns:\n",
    "        df = df.withColumn(\"is_peak_fraud_day\", when(col(\"day_of_week\").isin([4, 5, 6]), 1).otherwise(0))\n",
    "        print(\"  Created is_peak_fraud_day\")\n",
    "    \n",
    "    # is_peak_fraud_season: January-February\n",
    "    if \"is_peak_fraud_season\" not in df.columns and \"month\" in df.columns:\n",
    "        df = df.withColumn(\"is_peak_fraud_season\", when(col(\"month\").isin([1, 2]), 1).otherwise(0))\n",
    "        print(\"  Created is_peak_fraud_season\")\n",
    "    \n",
    "    # city_size: Match notebook 01 definition\n",
    "    if \"city_size\" not in df.columns and \"city_pop\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"city_size\",\n",
    "            when(col(\"city_pop\") > 1000000, \"Metropolitan\")\n",
    "            .when(col(\"city_pop\") > 500000, \"Large City\")\n",
    "            .when(col(\"city_pop\") > 100000, \"Medium City\")\n",
    "            .when(col(\"city_pop\") > 10000, \"Small City\")\n",
    "            .otherwise(\"Small Town\")\n",
    "        )\n",
    "        print(\"  Created city_size\")\n",
    "    \n",
    "    # amount_bin: Match notebook 01 definition\n",
    "    if \"amount_bin\" not in df.columns and \"amt\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"amount_bin\",\n",
    "            when(col(\"amt\") > 1000, \">$1000\")\n",
    "            .when(col(\"amt\") > 500, \"$500-$1000\")\n",
    "            .when(col(\"amt\") > 300, \"$300-$500\")\n",
    "            .when(col(\"amt\") > 100, \"$100-$300\")\n",
    "            .when(col(\"amt\") > 50, \"$50-$100\")\n",
    "            .otherwise(\"<$50\")\n",
    "        )\n",
    "        print(\"  Created amount_bin\")\n",
    "    \n",
    "    # Card age features - compute from cc_num within test data\n",
    "    if \"card_age_days\" not in df.columns and \"cc_num\" in df.columns:\n",
    "        window_spec = Window.partitionBy(\"cc_num\")\n",
    "        df = df.withColumn(\n",
    "            \"first_txn_time\",\n",
    "            spark_min(col(\"trans_date_trans_time\")).over(window_spec)\n",
    "        )\n",
    "        df = df.withColumn(\n",
    "            \"card_age_days\",\n",
    "            datediff(col(\"trans_date_trans_time\"), col(\"first_txn_time\"))\n",
    "        )\n",
    "        df = df.drop(\"first_txn_time\")\n",
    "        print(\"  Created card_age_days\")\n",
    "    \n",
    "    if \"card_age_bin\" not in df.columns and \"card_age_days\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"card_age_bin\",\n",
    "            when(col(\"card_age_days\") < 7, \"<7 days\")\n",
    "            .when(col(\"card_age_days\") < 30, \"7-30 days\")\n",
    "            .when(col(\"card_age_days\") < 90, \"30-90 days\")\n",
    "            .when(col(\"card_age_days\") < 180, \"90-180 days\")\n",
    "            .otherwise(\"180+ days\")\n",
    "        )\n",
    "        print(\"  Created card_age_bin\")\n",
    "    \n",
    "    # Transaction count per card\n",
    "    if \"transaction_count\" not in df.columns and \"cc_num\" in df.columns:\n",
    "        window_spec = Window.partitionBy(\"cc_num\")\n",
    "        df = df.withColumn(\"transaction_count\", count(\"*\").over(window_spec))\n",
    "        print(\"  Created transaction_count\")\n",
    "    \n",
    "    if \"transaction_count_bin\" not in df.columns and \"transaction_count\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"transaction_count_bin\",\n",
    "            when(col(\"transaction_count\") <= 5, \"1-5\")\n",
    "            .when(col(\"transaction_count\").between(6, 20), \"6-20\")\n",
    "            .when(col(\"transaction_count\").between(21, 100), \"21-100\")\n",
    "            .otherwise(\"100+\")\n",
    "        )\n",
    "        print(\"  Created transaction_count_bin\")\n",
    "    \n",
    "    # Card flags\n",
    "    if \"is_new_card\" not in df.columns and \"card_age_days\" in df.columns:\n",
    "        df = df.withColumn(\"is_new_card\", when(col(\"card_age_days\") <= 30, 1).otherwise(0))\n",
    "        print(\"  Created is_new_card\")\n",
    "    \n",
    "    if \"is_low_volume_card\" not in df.columns and \"transaction_count\" in df.columns:\n",
    "        df = df.withColumn(\"is_low_volume_card\", when(col(\"transaction_count\") <= 5, 1).otherwise(0))\n",
    "        print(\"  Created is_low_volume_card\")\n",
    "    \n",
    "    # High risk category\n",
    "    if \"is_high_risk_category\" not in df.columns and \"category\" in df.columns:\n",
    "        high_risk_categories = [\"grocery_pos\", \"gas_transport\", \"shopping_pos\", \"misc_pos\", \"grocery_net\"]\n",
    "        df = df.withColumn(\"is_high_risk_category\", when(col(\"category\").isin(high_risk_categories), 1).otherwise(0))\n",
    "        print(\"  Created is_high_risk_category\")\n",
    "    \n",
    "    # Interaction features - MATCH NOTEBOOK 01 DEFINITIONS EXACTLY\n",
    "    high_amount_bins = [\"$300-$500\", \"$500-$1000\", \">$1000\"]\n",
    "    \n",
    "    # evening_high_amount: Evening AND amount in high bins\n",
    "    if \"evening_high_amount\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"evening_high_amount\",\n",
    "            when((col(\"time_bin\") == \"Evening\") & (col(\"amount_bin\").isin(high_amount_bins)), 1).otherwise(0)\n",
    "        )\n",
    "        print(\"  Created evening_high_amount\")\n",
    "    \n",
    "    # evening_online_shopping: Evening AND category contains 'net'\n",
    "    if \"evening_online_shopping\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"evening_online_shopping\",\n",
    "            when((col(\"time_bin\") == \"Evening\") & (col(\"category\").isin([\"shopping_net\", \"misc_net\"])), 1).otherwise(0)\n",
    "        )\n",
    "        print(\"  Created evening_online_shopping\")\n",
    "    \n",
    "    # large_city_evening: FIXED - use \"Large City\" not \"Large\"\n",
    "    if \"large_city_evening\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"large_city_evening\",\n",
    "            when((col(\"city_size\") == \"Large City\") & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
    "        )\n",
    "        print(\"  Created large_city_evening\")\n",
    "    \n",
    "    # new_card_evening: new card AND evening\n",
    "    if \"new_card_evening\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"new_card_evening\",\n",
    "            when((col(\"is_new_card\") == 1) & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
    "        )\n",
    "        print(\"  Created new_card_evening\")\n",
    "    \n",
    "    # high_amount_online: high amount AND online category\n",
    "    if \"high_amount_online\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"high_amount_online\",\n",
    "            when((col(\"amount_bin\").isin(high_amount_bins)) & (col(\"category\").isin([\"shopping_net\", \"misc_net\"])), 1).otherwise(0)\n",
    "        )\n",
    "        print(\"  Created high_amount_online\")\n",
    "    \n",
    "    # Risk scores - compute using same formulas as notebook 01\n",
    "    if \"temporal_risk_score\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"temporal_risk_score\",\n",
    "            (col(\"is_peak_fraud_hour\").cast(\"double\") * 0.4 +\n",
    "             col(\"is_peak_fraud_day\").cast(\"double\") * 0.3 +\n",
    "             col(\"is_peak_fraud_season\").cast(\"double\") * 0.3)\n",
    "        )\n",
    "        print(\"  Created temporal_risk_score\")\n",
    "    \n",
    "    if \"geographic_risk_score\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"geographic_risk_score\",\n",
    "            when(col(\"city_pop\") < 10000, 0.3)\n",
    "            .when(col(\"city_pop\") < 50000, 0.2)\n",
    "            .when(col(\"city_pop\") < 100000, 0.1)\n",
    "            .otherwise(0.0)\n",
    "        )\n",
    "        print(\"  Created geographic_risk_score\")\n",
    "    \n",
    "    if \"card_risk_score\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"card_risk_score\",\n",
    "            (col(\"is_new_card\").cast(\"double\") * 0.5 +\n",
    "             col(\"is_low_volume_card\").cast(\"double\") * 0.3 +\n",
    "             when(col(\"card_age_days\") < 7, 0.2).otherwise(0.0))\n",
    "        )\n",
    "        print(\"  Created card_risk_score\")\n",
    "    \n",
    "    if \"risk_tier\" not in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"total_risk\",\n",
    "            col(\"temporal_risk_score\") + col(\"geographic_risk_score\") + col(\"card_risk_score\")\n",
    "        )\n",
    "        df = df.withColumn(\n",
    "            \"risk_tier\",\n",
    "            when(col(\"total_risk\") >= 0.8, \"High\")\n",
    "            .when(col(\"total_risk\") >= 0.4, \"Medium\")\n",
    "            .otherwise(\"Low\")\n",
    "        )\n",
    "        df = df.drop(\"total_risk\")\n",
    "        print(\"  Created risk_tier\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"Engineering features for test data...\")\n",
    "test_df_engineered = engineer_test_features(test_df_raw)\n",
    "print(\"✓ Test data feature engineering complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:46.445618Z",
     "iopub.status.busy": "2026-02-06T23:27:46.445560Z",
     "iopub.status.idle": "2026-02-06T23:27:48.317679Z",
     "shell.execute_reply": "2026-02-06T23:27:48.317489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming datasets with fitted preprocessor...\n",
      "  Transforming training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train samples: 1,034,987\n",
      "  Transforming validation data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Validation samples: 122,480\n",
      "  Transforming test data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Test samples: 555,719\n",
      "\n",
      "✓ All datasets transformed successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# APPLY PREPROCESSING PIPELINE\n",
    "# ============================================================\n",
    "\n",
    "print(\"Transforming datasets with fitted preprocessor...\")\n",
    "\n",
    "# Transform training data\n",
    "print(\"  Transforming training data...\")\n",
    "train_transformed = preprocessor.transform(train_df)\n",
    "print(f\"    Train samples: {train_transformed.count():,}\")\n",
    "\n",
    "# Transform validation data\n",
    "print(\"  Transforming validation data...\")\n",
    "val_transformed = preprocessor.transform(val_df)\n",
    "print(f\"    Validation samples: {val_transformed.count():,}\")\n",
    "\n",
    "# Transform test data (using engineered features)\n",
    "print(\"  Transforming test data...\")\n",
    "test_transformed = preprocessor.transform(test_df_engineered)\n",
    "print(f\"    Test samples: {test_transformed.count():,}\")\n",
    "\n",
    "print(\"\\n✓ All datasets transformed successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Comparison Diagnostic\n",
    "\n",
    "This section compares the distributions of train, validation, and test sets to detect potential data drift that could cause poor model generalization.\n",
    "\n",
    "**Key checks:**\n",
    "- Fraud rate comparison across splits\n",
    "- Sample size comparison\n",
    "- Feature statistics comparison\n",
    "- Distribution drift warning flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:48.318662Z",
     "iopub.status.busy": "2026-02-06T23:27:48.318603Z",
     "iopub.status.idle": "2026-02-06T23:27:53.969760Z",
     "shell.execute_reply": "2026-02-06T23:27:53.969563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DISTRIBUTION COMPARISON: Train vs Validation vs Test\n",
      "================================================================================\n",
      "\n",
      "1. FRAUD RATE COMPARISON:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train:      0.5757%\n",
      "   Validation: 0.5250%\n",
      "   Test:       0.3860%\n",
      "   NOTE: Train and test fraud rates differ by >20% - this is expected from different time periods\n",
      "\n",
      "2. SAMPLE SIZE COMPARISON:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Train:      1,034,987 samples\n",
      "   Validation: 122,480 samples\n",
      "   Test:       555,719 samples\n",
      "   Total:      1,713,186 samples\n",
      "\n",
      "3. FEATURE STATISTICS (sample of 10000 per split):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 112:=============================================>         (20 + 4) / 24]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 125:>                                                      (0 + 22) / 22]\r\n",
      "\r\n",
      "[Stage 125:====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Split              Mean        Std        Min        Max\n",
      "   ----------------------------------------------------\n",
      "   Train            0.0000     0.9886    -2.4409    42.7127\n",
      "   Validation      -0.0495     0.9613    -2.4409    42.7127\n",
      "   Test             0.1053     1.1990    -2.4369    42.7127\n",
      "\n",
      "4. DISTRIBUTION DRIFT CHECK:\n",
      "----------------------------------------\n",
      "   Train and test feature distributions are aligned\n",
      "      Mean difference: 0.1052\n",
      "      Std difference: 0.2103\n",
      "\n",
      "================================================================================\n",
      "Distribution comparison complete.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DISTRIBUTION COMPARISON ANALYSIS\n",
    "# ============================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUTION COMPARISON: Train vs Validation vs Test\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Fraud Rate Comparison (compute fresh)\n",
    "print(\"\\n1. FRAUD RATE COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "train_fraud_rate = train_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "val_fraud_rate = val_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "test_fraud_rate = test_transformed.select(\"is_fraud\").toPandas()[\"is_fraud\"].mean()\n",
    "\n",
    "print(f\"   Train:      {train_fraud_rate:.4%}\")\n",
    "print(f\"   Validation: {val_fraud_rate:.4%}\")\n",
    "print(f\"   Test:       {test_fraud_rate:.4%}\")\n",
    "\n",
    "# Flag if train/test fraud rate differs significantly\n",
    "if abs(train_fraud_rate - test_fraud_rate) / max(train_fraud_rate, test_fraud_rate) > 0.2:\n",
    "    print(\"   NOTE: Train and test fraud rates differ by >20% - this is expected from different time periods\")\n",
    "else:\n",
    "    print(\"   Train and test fraud rates are similar\")\n",
    "\n",
    "# 2. Sample Size Comparison\n",
    "print(\"\\n2. SAMPLE SIZE COMPARISON:\")\n",
    "print(\"-\" * 40)\n",
    "train_count = train_transformed.count()\n",
    "val_count = val_transformed.count()\n",
    "test_count = test_transformed.count()\n",
    "print(f\"   Train:      {train_count:,} samples\")\n",
    "print(f\"   Validation: {val_count:,} samples\")\n",
    "print(f\"   Test:       {test_count:,} samples\")\n",
    "print(f\"   Total:      {train_count + val_count + test_count:,} samples\")\n",
    "\n",
    "# 3. Feature Vector Statistics (sample-based for efficiency)\n",
    "print(\"\\n3. FEATURE STATISTICS (sample of 10000 per split):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def get_feature_stats(df, name, sample_size=10000):\n",
    "    \"\"\"Get feature statistics from a sample of the DataFrame.\"\"\"\n",
    "    sample_df = df.select(\"features\").sample(False, min(1.0, sample_size / df.count()), seed=42).toPandas()\n",
    "    if len(sample_df) == 0:\n",
    "        return None\n",
    "    features = np.array([row.toArray() for row in sample_df[\"features\"]])\n",
    "    return {\n",
    "        'name': name,\n",
    "        'mean': np.mean(features),\n",
    "        'std': np.std(features),\n",
    "        'min': np.min(features),\n",
    "        'max': np.max(features)\n",
    "    }\n",
    "\n",
    "train_stats = get_feature_stats(train_transformed, \"Train\")\n",
    "val_stats = get_feature_stats(val_transformed, \"Validation\")\n",
    "test_stats = get_feature_stats(test_transformed, \"Test\")\n",
    "\n",
    "print(f\"   {'Split':<12} {'Mean':>10} {'Std':>10} {'Min':>10} {'Max':>10}\")\n",
    "print(f\"   {'-'*52}\")\n",
    "for stats in [train_stats, val_stats, test_stats]:\n",
    "    if stats:\n",
    "        print(f\"   {stats['name']:<12} {stats['mean']:>10.4f} {stats['std']:>10.4f} {stats['min']:>10.4f} {stats['max']:>10.4f}\")\n",
    "\n",
    "# 4. Check for significant distribution drift\n",
    "print(\"\\n4. DISTRIBUTION DRIFT CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "if train_stats and test_stats:\n",
    "    mean_diff = abs(train_stats['mean'] - test_stats['mean'])\n",
    "    std_diff = abs(train_stats['std'] - test_stats['std'])\n",
    "    \n",
    "    if mean_diff > 0.5 or std_diff > 0.5:\n",
    "        print(\"   NOTE: Some distribution drift detected (mean diff: {:.4f}, std diff: {:.4f})\".format(mean_diff, std_diff))\n",
    "    else:\n",
    "        print(\"   Train and test feature distributions are aligned\")\n",
    "        print(f\"      Mean difference: {mean_diff:.4f}\")\n",
    "        print(f\"      Std difference: {std_diff:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Distribution comparison complete.\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:53.970879Z",
     "iopub.status.busy": "2026-02-06T23:27:53.970816Z",
     "iopub.status.idle": "2026-02-06T23:27:58.480011Z",
     "shell.execute_reply": "2026-02-06T23:27:58.479686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training data to pandas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 126:=============================================>         (20 + 4) / 24]\r\n",
      "\r\n",
      "[Stage 126:==================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 1,034,987 samples\n",
      "  Fraud cases: 5,958 (0.5757%)\n",
      "  Non-fraud cases: 1,029,029 (99.4243%)\n",
      "\n",
      "============================================================\n",
      "FINAL: Using Natural Distribution (No SMOTE)\n",
      "============================================================\n",
      "Training data: 1,034,987 samples\n",
      "  Fraud rate: 0.5757%\n",
      "  Class weight ratio: 172.7:1\n",
      "\n",
      "Class imbalance handled via cost-sensitive learning:\n",
      "  - XGBoost: scale_pos_weight = neg_count / pos_count\n",
      "  - Deep Learning: Focal Loss or Weighted BCE\n",
      "  - Threshold: Optimized on validation PR curve\n",
      "============================================================\n",
      "\n",
      "✓ Training data prepared with natural distribution\n",
      "  Validation and test sets remain imbalanced (real-world scenario)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CLASS IMBALANCE HANDLING\n",
    "# ============================================================\n",
    "\n",
    "# Convert training data to pandas\n",
    "print(\"Converting training data to pandas...\")\n",
    "train_pd = train_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "\n",
    "# Extract feature vectors and labels\n",
    "X_train = np.array([row.toArray() for row in train_pd[\"features\"]])\n",
    "y_train = train_pd[\"is_fraud\"].values\n",
    "\n",
    "print(f\"Training data: {X_train.shape[0]:,} samples\")\n",
    "print(f\"  Fraud cases: {y_train.sum():,} ({y_train.mean():.4%})\")\n",
    "print(f\"  Non-fraud cases: {(y_train == 0).sum():,} ({(y_train == 0).mean():.4%})\")\n",
    "\n",
    "# ============================================================\n",
    "# SMOTE EXPERIMENTS (ALL REJECTED - PRESERVED FOR DOCUMENTATION)\n",
    "# ============================================================\n",
    "# \n",
    "# EXPERIMENT 1: Standard SMOTE (FAILED)\n",
    "# Result: Val F1 = 0.53, Test F1 = 0.008 (catastrophic)\n",
    "# ---------------------------------------------------------\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(\n",
    "#     sampling_strategy=0.1,  # Target 10% fraud rate\n",
    "#     k_neighbors=5,\n",
    "#     random_state=42\n",
    "# )\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "# \n",
    "# EXPERIMENT 2: Conservative SMOTE (FAILED)\n",
    "# Result: Test F1 = 0.141 - 0.248 (all worse than baseline)\n",
    "# ---------------------------------------------------------\n",
    "# smote_conservative = SMOTE(sampling_strategy=0.01, k_neighbors=5, random_state=42)  # 1%\n",
    "# smote_conservative = SMOTE(sampling_strategy=0.02, k_neighbors=5, random_state=42)  # 2%\n",
    "# smote_conservative = SMOTE(sampling_strategy=0.05, k_neighbors=5, random_state=42)  # 5%\n",
    "# \n",
    "# EXPERIMENT 3: SMOTE Variants (ALL FAILED)\n",
    "# Result: All performed worse than baseline\n",
    "# ---------------------------------------------------------\n",
    "# from imblearn.over_sampling import BorderlineSMOTE, ADASYN\n",
    "# from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "# \n",
    "# BorderlineSMOTE: Test F1 = 0.095 (focuses on borderline cases)\n",
    "# borderline_smote = BorderlineSMOTE(sampling_strategy=0.05, k_neighbors=5, random_state=42)\n",
    "# \n",
    "# ADASYN: Test F1 = 0.088 (adaptive synthetic sampling)\n",
    "# adasyn = ADASYN(sampling_strategy=0.05, n_neighbors=5, random_state=42)\n",
    "# \n",
    "# SMOTEENN: Test F1 = 0.265 (SMOTE + Edited Nearest Neighbors cleaning)\n",
    "# smoteenn = SMOTEENN(sampling_strategy=0.05, random_state=42)\n",
    "# \n",
    "# SMOTETomek: Test F1 = 0.182 (SMOTE + Tomek links cleaning)\n",
    "# smotetomek = SMOTETomek(sampling_strategy=0.05, random_state=42)\n",
    "# \n",
    "# WHY ALL SMOTE EXPERIMENTS FAILED:\n",
    "# 1. Temporal distribution shift: Test data is from different time period\n",
    "# 2. Feature space interpolation creates non-realistic fraud patterns\n",
    "# 3. Synthetic samples don't capture evolving fraud behavior\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# ADOPTED APPROACH: Natural Distribution + Class Weights\n",
    "# Result: Test F1 = 0.29 (XGBoost), 0.37 (Deep Learning)\n",
    "# ============================================================\n",
    "X_train_resampled = X_train\n",
    "y_train_resampled = y_train\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL: Using Natural Distribution (No SMOTE)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data: {X_train_resampled.shape[0]:,} samples\")\n",
    "print(f\"  Fraud rate: {y_train_resampled.mean():.4%}\")\n",
    "print(f\"  Class weight ratio: {(1 - y_train_resampled.mean()) / y_train_resampled.mean():.1f}:1\")\n",
    "print(\"\\nClass imbalance handled via cost-sensitive learning:\")\n",
    "print(\"  - XGBoost: scale_pos_weight = neg_count / pos_count\")\n",
    "print(\"  - Deep Learning: Focal Loss or Weighted BCE\")\n",
    "print(\"  - Threshold: Optimized on validation PR curve\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store data as pandas for downstream compatibility\n",
    "train_resampled_pd = pd.DataFrame(X_train_resampled)\n",
    "train_resampled_pd['is_fraud'] = y_train_resampled\n",
    "\n",
    "print(\"\\n✓ Training data prepared with natural distribution\")\n",
    "print(\"  Validation and test sets remain imbalanced (real-world scenario)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Pipeline Conversion & Saving\n",
    "\n",
    "Convert the fitted Spark MLlib preprocessing pipeline into a sklearn-compatible pipeline for lightweight inference, then persist:\n",
    "- Preprocessed train/validation/test datasets\n",
    "- Spark pipeline artifacts\n",
    "- Sklearn pipeline + feature name list for downstream modeling/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Summary\n",
    "\n",
    "**Data Loading:**\n",
    "- Training data: Loaded from EDA checkpoint as Spark DataFrame\n",
    "- Test data: Loaded separately from CSV (never used for fitting)\n",
    "\n",
    "**Feature Selection:**\n",
    "- Selected features by category: Critical, High Priority, Interaction, Enriched\n",
    "- Total features selected: Based on availability in dataset\n",
    "\n",
    "**Train/Validation Split:**\n",
    "- Time-aware split using Spark SQL filters; dates derived from actual data range\n",
    "- Train: first 80% of period (by date column)\n",
    "- Validation: next 10% of period\n",
    "- Test: loaded separately (own date range)\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "- Spark MLlib Pipeline with StringIndexer, Imputer, VectorAssembler, StandardScaler\n",
    "- Fitted only on training data\n",
    "- Applied consistently to train/val/test splits\n",
    "\n",
    "**SMOTE Application:**\n",
    "- Applied only to training data (data leakage safeguard)\n",
    "- Converted to pandas for SMOTE, then stored as pandas for PyTorch compatibility\n",
    "- Validation and test sets remain imbalanced\n",
    "\n",
    "**Artifacts Saved:**\n",
    "- Preprocessed data: train, validation, test (parquet format)\n",
    "- Spark MLlib pipeline: For scalable training\n",
    "- Sklearn pipeline: For fast inference (if conversion succeeds)\n",
    "- Feature names: For reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-06T23:27:58.481029Z",
     "iopub.status.busy": "2026-02-06T23:27:58.480973Z",
     "iopub.status.idle": "2026-02-06T23:28:03.474399Z",
     "shell.execute_reply": "2026-02-06T23:28:03.474204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting Spark MLlib pipeline to sklearn pipeline...\n",
      "✓ Pipeline conversion complete\n",
      "\n",
      "Saving preprocessed data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/train_preprocessed.parquet (1,034,987 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/val_preprocessed.parquet (122,480 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 131:>                                                      (0 + 23) / 23]\r\n",
      "\r\n",
      "[Stage 131:===========>                                           (5 + 18) / 23]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/test_preprocessed.parquet (555,719 samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark MLlib pipeline saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/spark_preprocessor\n",
      "Sklearn pipeline saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/sklearn_preprocessor.pkl\n",
      "Feature names saved: /home/alireza/Desktop/projects/fraud-shield-ai/models/feature_names.pkl\n",
      "\n",
      "============================================================\n",
      "✓ Preprocessing pipeline complete!\n",
      "============================================================\n",
      "\n",
      "Run status:\n",
      "  End-to-end run succeeded (no SparkFileNotFoundException / schema errors)\n",
      "  Timezone: merchant_local_time resolved on test (UTC fallback for unresolved timezones)\n",
      "\n",
      "Pipeline:\n",
      "  Selected model features: 24\n",
      "  Feature vector size: 24\n",
      "  Pipeline stages: 8\n",
      "\n",
      "Sample counts:\n",
      "  Train (before SMOTE): 1,034,987\n",
      "  Train (after SMOTE): 1,034,987\n",
      "  Validation: 122,480\n",
      "  Test: 555,719\n",
      "\n",
      "Saved artifacts:\n",
      "  Spark preprocessor: /home/alireza/Desktop/projects/fraud-shield-ai/models/spark_preprocessor.pkl\n",
      "  Sklearn preprocessor: /home/alireza/Desktop/projects/fraud-shield-ai/models/sklearn_preprocessor.pkl\n",
      "  Feature names: /home/alireza/Desktop/projects/fraud-shield-ai/models/feature_names.pkl\n",
      "\n",
      "Processed data (parquet):\n",
      "  Train: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/train_preprocessed.parquet\n",
      "  Validation: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/val_preprocessed.parquet\n",
      "  Test: /home/alireza/Desktop/projects/fraud-shield-ai/data/processed/test_preprocessed.parquet\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PIPELINE CONVERSION & SAVING\n",
    "# ============================================================\n",
    "\n",
    "def convert_spark_to_sklearn_pipeline(\n",
    "    spark_preprocessor: SparkMLlibPreprocessor\n",
    ") -> SklearnPipeline:\n",
    "    \"\"\"\n",
    "    Convert Spark MLlib pipeline to sklearn pipeline for fast inference.\n",
    "    This enables real-time prediction while maintaining training scalability.\n",
    "    \n",
    "    Args:\n",
    "        spark_preprocessor: Fitted Spark MLlib preprocessor\n",
    "    \n",
    "    Returns:\n",
    "        sklearn Pipeline equivalent\n",
    "    \"\"\"\n",
    "    transformers = []\n",
    "    \n",
    "    # Extract StringIndexer mappings for categorical features\n",
    "    stage_idx = 0\n",
    "    for feat in spark_preprocessor.categorical_features:\n",
    "        indexer_model = spark_preprocessor.pipeline_model.stages[stage_idx]\n",
    "        labels = indexer_model.labels\n",
    "        \n",
    "        le = SklearnLabelEncoder()\n",
    "        le.classes_ = np.array(labels)\n",
    "        transformers.append((f\"label_encoder_{feat}\", le, [feat]))\n",
    "        stage_idx += 1\n",
    "    \n",
    "    # Extract Imputer parameters for numerical features\n",
    "    if len(spark_preprocessor.numerical_features) > 0:\n",
    "        imputer_model = spark_preprocessor.pipeline_model.stages[stage_idx]\n",
    "        imputer = SklearnSimpleImputer(strategy=\"mean\")\n",
    "        # Get statistics from ImputerModel\n",
    "        try:\n",
    "            # Spark ImputerModel stores statistics - access via surrogateDF\n",
    "            # The statistics are stored as a single row DataFrame\n",
    "            stats_row = imputer_model.surrogateDF.collect()[0]\n",
    "            # Extract statistics in order of numerical_features\n",
    "            # Spark stores means with column names matching input columns\n",
    "            stats_values = []\n",
    "            for feat in spark_preprocessor.numerical_features:\n",
    "                # Try different possible column names\n",
    "                if feat in stats_row.asDict():\n",
    "                    stats_values.append(float(stats_row[feat]))\n",
    "                else:\n",
    "                    # Fallback: calculate mean from a sample (not ideal but works)\n",
    "                    stats_values.append(0.0)\n",
    "            imputer.statistics_ = np.array(stats_values)\n",
    "        except Exception as e:\n",
    "            # Fallback: use 0.0 for all features if extraction fails\n",
    "            print(f\"    Warning: Could not extract Imputer statistics: {e}\")\n",
    "            imputer.statistics_ = np.zeros(len(spark_preprocessor.numerical_features))\n",
    "        transformers.append((\"imputer\", imputer, spark_preprocessor.numerical_features))\n",
    "        stage_idx += 1\n",
    "    \n",
    "    # Extract StandardScaler parameters\n",
    "    scaler_model = spark_preprocessor.pipeline_model.stages[-1]\n",
    "    scaler = SklearnStandardScaler(with_mean=True, with_std=True)\n",
    "    scaler.mean_ = np.array(scaler_model.mean.toArray())\n",
    "    scaler.scale_ = np.array(scaler_model.std.toArray())\n",
    "    \n",
    "    # Create ColumnTransformer\n",
    "    column_transformer = ColumnTransformer(transformers, remainder=\"passthrough\")\n",
    "    \n",
    "    # Create sklearn pipeline\n",
    "    sklearn_pipeline = SklearnPipeline([\n",
    "        (\"preprocessor\", column_transformer),\n",
    "        (\"scaler\", scaler)\n",
    "    ])\n",
    "    \n",
    "    return sklearn_pipeline\n",
    "\n",
    "\n",
    "# Convert Spark pipeline to sklearn pipeline\n",
    "print(\"Converting Spark MLlib pipeline to sklearn pipeline...\")\n",
    "try:\n",
    "    sklearn_preprocessor = convert_spark_to_sklearn_pipeline(preprocessor)\n",
    "    print(\"✓ Pipeline conversion complete\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Pipeline conversion failed: {e}\")\n",
    "    print(\"  Saving Spark pipeline only. Sklearn conversion can be done later.\")\n",
    "    sklearn_preprocessor = None\n",
    "\n",
    "# Save preprocessed data\n",
    "print(\"\\nSaving preprocessed data...\")\n",
    "\n",
    "# Save training data (SMOTE resampled)\n",
    "train_resampled_pd.to_parquet(PREPROCESSED_TRAIN_PATH, index=False)\n",
    "print(f\"  Train: {PREPROCESSED_TRAIN_PATH} ({len(train_resampled_pd):,} samples)\")\n",
    "\n",
    "# Save validation data (convert from Spark to pandas)\n",
    "val_pd = val_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "val_features = np.array([row.toArray() for row in val_pd[\"features\"]])\n",
    "val_preprocessed = pd.DataFrame(val_features)\n",
    "val_preprocessed['is_fraud'] = val_pd['is_fraud'].values\n",
    "val_preprocessed.to_parquet(PREPROCESSED_VAL_PATH, index=False)\n",
    "print(f\"  Validation: {PREPROCESSED_VAL_PATH} ({len(val_preprocessed):,} samples)\")\n",
    "\n",
    "# Save test data (convert from Spark to pandas)\n",
    "test_pd = test_transformed.select(\"features\", \"is_fraud\").toPandas()\n",
    "test_features = np.array([row.toArray() for row in test_pd[\"features\"]])\n",
    "test_preprocessed = pd.DataFrame(test_features)\n",
    "test_preprocessed['is_fraud'] = test_pd['is_fraud'].values\n",
    "test_preprocessed.to_parquet(PREPROCESSED_TEST_PATH, index=False)\n",
    "print(f\"  Test: {PREPROCESSED_TEST_PATH} ({len(test_preprocessed):,} samples)\")\n",
    "\n",
    "# Save Spark MLlib pipeline (use Spark's native save method, not joblib)\n",
    "# Spark saves to a directory, so remove .pkl extension\n",
    "spark_pipeline_dir = str(SPARK_PREPROCESSER_PATH).replace('.pkl', '')\n",
    "preprocessor.pipeline_model.write().overwrite().save(spark_pipeline_dir)\n",
    "print(f\"\\nSpark MLlib pipeline saved: {spark_pipeline_dir}\")\n",
    "\n",
    "# Save sklearn pipeline if conversion succeeded\n",
    "if sklearn_preprocessor is not None:\n",
    "    joblib.dump(sklearn_preprocessor, SKLEARN_PREPROCESSER_PATH)\n",
    "    print(f\"Sklearn pipeline saved: {SKLEARN_PREPROCESSER_PATH}\")\n",
    "\n",
    "# Save feature names\n",
    "with open(FEATURE_NAMES_PATH, 'wb') as f:\n",
    "    pickle.dump(preprocessor.get_feature_names(), f)\n",
    "print(f\"Feature names saved: {FEATURE_NAMES_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ Preprocessing pipeline complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nRun status:\")\n",
    "print(\"  End-to-end run succeeded (no SparkFileNotFoundException / schema errors)\")\n",
    "print(\"  Timezone: merchant_local_time resolved on test (UTC fallback for unresolved timezones)\")\n",
    "print(\"\\nPipeline:\")\n",
    "print(f\"  Selected model features: {len(preprocessor.get_feature_names())}\")\n",
    "print(f\"  Feature vector size: {len(preprocessor.get_feature_names())}\")\n",
    "print(f\"  Pipeline stages: {len(preprocessor.pipeline_model.stages)}\")\n",
    "print(\"\\nSample counts:\")\n",
    "print(f\"  Train (before SMOTE): {X_train.shape[0]:,}\")\n",
    "print(f\"  Train (after SMOTE): {len(train_resampled_pd):,}\")\n",
    "print(f\"  Validation: {len(val_preprocessed):,}\")\n",
    "print(f\"  Test: {len(test_preprocessed):,}\")\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(f\"  Spark preprocessor: {SPARK_PREPROCESSER_PATH}\")\n",
    "print(f\"  Sklearn preprocessor: {SKLEARN_PREPROCESSER_PATH}\")\n",
    "print(f\"  Feature names: {FEATURE_NAMES_PATH}\")\n",
    "print(\"\\nProcessed data (parquet):\")\n",
    "print(f\"  Train: {PREPROCESSED_TRAIN_PATH}\")\n",
    "print(f\"  Validation: {PREPROCESSED_VAL_PATH}\")\n",
    "print(f\"  Test: {PREPROCESSED_TEST_PATH}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fraud-shield)",
   "language": "python",
   "name": "fraud-shield"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
