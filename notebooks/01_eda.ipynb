{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Notebook 1** Fraud Detection EDA (Timezone-Aware)\n",
    "**Project:** Fraud Shield AI  \n",
    "**Notebook:** 01_fraud_detection_eda.ipynb  \n",
    "**Objective:** Exploratory Data Analysis & Feature Validation (No Modeling)\n",
    "\n",
    "## üìã Project Information\n",
    "\n",
    "| **Attribute** | **Details** |\n",
    "| :--- | :--- |\n",
    "| **Author** | Alireza Barzin Zanganeh |\n",
    "| **Contact** | abarzinzanganeh@gmail.com |\n",
    "| **Date** | January 18, 2026 |\n",
    "| **Project Type** | Capstone Project|\n",
    "\n",
    "**Data Source Attribution:**  \n",
    "ZIP code and timezone data provided by [SimpleMaps US ZIP Code Database](https://simplemaps.com/data/us-zips). Free version used for EDA.  \n",
    "Use in production requires linking back as per their license, which we comply with.\n",
    "\n",
    "## Problem Statement\n",
    "The objective is to design and implement a comprehensive fraud detection system capable of identifying fraudulent transactions with high accuracy and scalability. The system will leverage:\n",
    "\n",
    "Supervised learning to identify known fraud patterns from labeled historical data.\n",
    "Deep learning to model complex relationships and sequential transaction behaviors.\n",
    "The solution will focus on minimizing false positives, maximizing fraud recall, and maintaining scalability for high-volume real-time transaction processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Scope & Constraints\n",
    "\n",
    "This notebook is **EDA-only** and focuses on:\n",
    "- Dataset understanding\n",
    "- Assumption validation\n",
    "- Pattern discovery\n",
    "- Feature design guidance\n",
    "\n",
    "### Explicitly Out of Scope\n",
    "- Model training\n",
    "- Feature scaling\n",
    "- Hyperparameter tuning\n",
    "- Evaluation metrics\n",
    "\n",
    "All modeling work is deferred to later notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical Time Assumptions\n",
    "\n",
    "Transaction timestamps are **not guaranteed to be in local time**.\n",
    "\n",
    "Any analysis involving:\n",
    "- Hour of day\n",
    "- Day of week\n",
    "- Night vs daytime behavior\n",
    "\n",
    "is considered **provisional** until timestamps are converted to **local transaction time**\n",
    "using latitude/longitude‚Äìbased timezone inference.\n",
    "\n",
    "Timezone correction is treated as a first-class EDA requirement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Roadmap\n",
    "\n",
    "This notebook follows a strict, sequential EDA structure:\n",
    "\n",
    "1. Environment & Dataset Loading  \n",
    "2. Data Quality Assessment  \n",
    "3. Target Variable (Fraud) Overview  \n",
    "4. Temporal Analysis (Timezone-Aware)  \n",
    "5. Calendar-Level Patterns (Month, Seasonality)  \n",
    "6. Key Insights & Next Steps\n",
    "\n",
    "Each section builds on validated assumptions from the previous one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment & Dataset Inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Imports & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "# All imports consolidated here for clarity and maintainability\n",
    "\n",
    "# Standard Library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "# PySpark Core\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, avg, min as spark_min, max as spark_max,\n",
    "    round as spark_round, floor, ceil, trim, when, lit, concat, concat_ws,\n",
    "    hour, dayofweek, dayofmonth, month, year, date_format,\n",
    "    from_utc_timestamp, to_timestamp, unix_timestamp,\n",
    "    broadcast, first, last, collect_list, collect_set,\n",
    "    percentile_approx, stddev, variance, corr\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Data Analysis & Visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports are now consolidated in the Global Imports & Dependencies section above\n",
    "print(\"‚úì Using global imports\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Kaggle Environment & Input Validation\n",
    "\n",
    "### Purpose\n",
    "Verify the Kaggle execution environment and confirm the availability and structure\n",
    "of input datasets **before initializing Spark**.\n",
    "\n",
    "### Why This Matters\n",
    "- Confirms datasets are correctly mounted\n",
    "- Avoids hard-coded paths\n",
    "- Ensures Spark reads from the correct input directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:02.383493Z",
     "iopub.status.busy": "2026-01-21T22:12:02.38317Z",
     "iopub.status.idle": "2026-01-21T22:12:02.388954Z",
     "shell.execute_reply": "2026-01-21T22:12:02.387998Z",
     "shell.execute_reply.started": "2026-01-21T22:12:02.383462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Clear Spark cache if needed (uncomment to use)\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "print(\"Spark session ready - cache cleared if needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:02.391248Z",
     "iopub.status.busy": "2026-01-21T22:12:02.390889Z",
     "iopub.status.idle": "2026-01-21T22:12:02.426525Z",
     "shell.execute_reply": "2026-01-21T22:12:02.425666Z",
     "shell.execute_reply.started": "2026-01-21T22:12:02.391219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Environment & dataset inspection\n",
    "\n",
    "INPUT_DIR = \"/kaggle/input\"\n",
    "\n",
    "print(\"Listing all files under /kaggle/input:\\n\")\n",
    "\n",
    "for dirname, _, filenames in os.walk(INPUT_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark Session Initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spark Session Setup\n",
    "\n",
    "### Purpose\n",
    "Initialize a **SparkSession** to enable distributed data processing and replace\n",
    "pandas-based workflows with Spark DataFrames.\n",
    "\n",
    "### Why This Matters\n",
    "- Enables scalable EDA and feature engineering\n",
    "- Required for Spark `read`, `groupBy`, and ML operations\n",
    "- Establishes a single entry point for Spark APIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:02.427787Z",
     "iopub.status.busy": "2026-01-21T22:12:02.427498Z",
     "iopub.status.idle": "2026-01-21T22:12:13.232453Z",
     "shell.execute_reply": "2026-01-21T22:12:13.231468Z",
     "shell.execute_reply.started": "2026-01-21T22:12:02.427762Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FraudShieldAI\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Loading & Structural Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Training Dataset (Spark)\n",
    "\n",
    "### Purpose\n",
    "Load the training dataset into a **Spark DataFrame** and inspect its basic structure.\n",
    "\n",
    "### What We Validate\n",
    "- Number of rows\n",
    "- Number of columns\n",
    "- Column names and schema\n",
    "- Sample records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:27.446814Z",
     "iopub.status.busy": "2026-01-21T22:12:27.446036Z",
     "iopub.status.idle": "2026-01-21T22:12:43.934929Z",
     "shell.execute_reply": "2026-01-21T22:12:43.933524Z",
     "shell.execute_reply.started": "2026-01-21T22:12:27.446754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD TRAINING DATASET WITH VALIDATION\n",
    "# ============================================================\n",
    "\n",
    "# Construct path\n",
    "dataset_folders = os.listdir(INPUT_DIR)\n",
    "dataset_path = sorted(os.listdir(INPUT_DIR))[0]\n",
    "full_path = os.path.join(INPUT_DIR, 'fraud-detection', \"fraudTrain.csv\")\n",
    "\n",
    "# Validate path exists\n",
    "if not os.path.exists(full_path):\n",
    "    raise FileNotFoundError(f\"Training data not found: {full_path}\")\n",
    "\n",
    "print(f\"Dataset folder: {dataset_path}\")\n",
    "print(f\"Loading from: {full_path}\")\n",
    "\n",
    "# Load with Spark\n",
    "train_df = spark.read.csv(full_path, header=True, inferSchema=True)\n",
    "\n",
    "# Drop index column if exists\n",
    "if \"_c0\" in train_df.columns:\n",
    "    train_df = train_df.drop(\"_c0\")\n",
    "\n",
    "# Basic validation\n",
    "num_rows = train_df.count()\n",
    "num_cols = len(train_df.columns)\n",
    "\n",
    "print(f\"\\n‚úì Loaded successfully\")\n",
    "print(f\"Shape: ({num_rows:,} rows, {num_cols} columns)\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample records:\")\n",
    "train_df.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Quality & Target Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fraud Statistics\n",
    "\n",
    "### Purpose\n",
    "Compute basic fraud statistics from the training dataset using **Spark**:\n",
    "- Total transactions  \n",
    "- Fraudulent transactions count  \n",
    "- Legitimate transactions count  \n",
    "- Fraud rate (%)\n",
    "\n",
    "### Notes\n",
    "- `.count()` replaces `len()` in Spark\n",
    "- `sum()` can be applied with `agg` or `select` + `sum`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:43.940035Z",
     "iopub.status.busy": "2026-01-21T22:12:43.939596Z",
     "iopub.status.idle": "2026-01-21T22:12:47.649456Z",
     "shell.execute_reply": "2026-01-21T22:12:47.648573Z",
     "shell.execute_reply.started": "2026-01-21T22:12:43.939995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Total transactions\n",
    "total = train_df.count()\n",
    "\n",
    "# Fraudulent transactions count\n",
    "fraud_count = train_df.agg(spark_sum(col(\"is_fraud\"))).collect()[0][0]\n",
    "\n",
    "# Legitimate transactions\n",
    "legit_count = total - fraud_count\n",
    "\n",
    "# Fraud rate in percentage\n",
    "fraud_rate = (fraud_count / total) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total transactions: {total:,}\")\n",
    "print(f\"Fraud transactions: {fraud_count:,}\")\n",
    "print(f\"Legitimate transactions: {legit_count:,}\")\n",
    "print(f\"Fraud rate: {fraud_rate:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Missing Values Analysis\n",
    "\n",
    "### Purpose\n",
    "Check for missing values in each column of the Spark DataFrame and compute the total number of missing entries.\n",
    "\n",
    "### Notes\n",
    "- In Spark, use `isNull()` combined with `agg` and `sum` for each column.\n",
    "- `.count()` can also be used to compute non-missing rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Numeric Summary Statistics (Optional)\n",
    "\n",
    "### Purpose\n",
    "Get summary statistics (count, mean, stddev, min, max) for numeric columns in the Spark DataFrame.\n",
    "\n",
    "### Notes\n",
    "- Only computes **numeric columns** by default\n",
    "- Returns a Spark DataFrame (use `.show()` to visualize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:12:47.651589Z",
     "iopub.status.busy": "2026-01-21T22:12:47.651237Z",
     "iopub.status.idle": "2026-01-21T22:13:01.568927Z",
     "shell.execute_reply": "2026-01-21T22:13:01.564848Z",
     "shell.execute_reply.started": "2026-01-21T22:12:47.651551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute missing values per column\n",
    "missing_df = train_df.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in train_df.columns\n",
    "])\n",
    "\n",
    "# Show missing values per column\n",
    "missing_df.show(truncate=False)\n",
    "\n",
    "# Total missing values\n",
    "total_missing = sum([missing_df.collect()[0][c] for c in missing_df.columns])\n",
    "print(f\"\\nTotal missing values: {total_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:13:01.571476Z",
     "iopub.status.busy": "2026-01-21T22:13:01.570869Z",
     "iopub.status.idle": "2026-01-21T22:13:01.576096Z",
     "shell.execute_reply": "2026-01-21T22:13:01.57471Z",
     "shell.execute_reply.started": "2026-01-21T22:13:01.571433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Spark equivalent of pandas describe()\n",
    "# train_df.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Key Findings from Initial Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Size & Structure\n",
    "\n",
    "- 1,296,675 transactions (~1.3M)\n",
    "- 23 columns (features)\n",
    "- ~1GB memory footprint\n",
    "\n",
    "This size justifies using **PySpark** for distributed processing and scalable feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Class Imbalance Analysis (CRITICAL)\n",
    "\n",
    "| Class      | Count     | Percentage |\n",
    "|-----------|-----------|------------|\n",
    "| Fraud     | 7,506     | 0.58%      |\n",
    "| Legitimate| 1,289,169 | 99.42%     |\n",
    "\n",
    "**Imbalance ratio:** 172:1\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "- Models can predict *‚Äúnot fraud‚Äù* for everything and still get ~99.4% accuracy ‚Üí **useless**.\n",
    "- This is why **SMOTE** or other imbalance-handling methods are mandatory.\n",
    "- This is the **key challenge** mentioned in class.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Missing Values\n",
    "\n",
    "- Zero missing values ‚Üí clean dataset.\n",
    "- No need for imputation strategies.\n",
    "- Can proceed directly to **feature engineering**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Geographic Analysis (lat/long)\n",
    "\n",
    "### Customer Location\n",
    "\n",
    "- Latitude: 20.0¬∞ to 66.7¬∞\n",
    "- Longitude: -165.7¬∞ to -67.9¬∞ (all negative)\n",
    "\n",
    "### What this tells us\n",
    "\n",
    "- Negative longitude ‚Üí **Western Hemisphere**.\n",
    "- This range covers the **entire United States**:\n",
    "  - Alaska: ~64¬∞N, -165¬∞W (northern/western extreme)\n",
    "  - Florida: ~25¬∞N, -80¬∞W (southern/eastern extreme)\n",
    "  - California: ~37¬∞N, -120¬∞W (west coast)\n",
    "  - Maine: ~45¬∞N, -68¬∞W (east coast)\n",
    "\n",
    "### Cruise/Water transactions?\n",
    "\n",
    "- To check this, compare lat/long to known land coordinates.\n",
    "- If coordinates fall in the ocean (Atlantic or Pacific), they could be **cruise ship** transactions.\n",
    "- This is a strong potential fraud indicator (class mentioned cruise transactions have higher fraud).\n",
    "\n",
    "### Not in Europe/Asia because\n",
    "\n",
    "- Europe longitude: about -10¬∞ to 40¬∞ (mostly positive).\n",
    "- Asia longitude: about 60¬∞ to 180¬∞ (positive).\n",
    "- Our data: all negative longitudes (-165 to -68) ‚Üí **North America only**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Transaction Amount Analysis (`amt` column)\n",
    "\n",
    "### Key statistics\n",
    "\n",
    "- Min: \\$1.00 ‚Äî small purchases (coffee, snacks)\n",
    "- Median (50%): \\$47.52 ‚Äî typical transaction\n",
    "- Mean: \\$70.35 ‚Äî slightly higher than median (right-skewed)\n",
    "- 75th percentile: \\$83.14 ‚Äî most transactions under \\$100\n",
    "- Max: \\$28,948.90 ‚Äî very large purchase\n",
    "\n",
    "### What this tells us\n",
    "\n",
    "- Distribution is **right-skewed** ‚Äî a few very large transactions pull the mean up.\n",
    "- Most transactions are **< \\$100** ‚Äî normal retail purchases.\n",
    "- Large amounts (e.g., **\\$900+**) are rare ‚Äî class mentioned using bins: `>200`, `>300`, `>900` as features.\n",
    "- Fraud likely correlates with **unusual amounts** ‚Äî need to compare fraud vs non-fraud distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Credit Card Numbers (`cc_num`)\n",
    "\n",
    "- Range: 60 billion to 4.9 quintillion.\n",
    "- This is effectively an **ID**, not a numeric magnitude feature.\n",
    "- Best used for **grouping**, e.g.:\n",
    "  - *How many transactions in the last hour for this card?*\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Time Analysis (`unix_time`)\n",
    "\n",
    "- Range: 1,325,376,018 to 1,371,817,018.\n",
    "- Converts to approximately: **January 1, 2012 ‚Üí June 21, 2013** (~18 months).\n",
    "\n",
    "### Implications\n",
    "\n",
    "- Over a year of data:\n",
    "  - Can detect **seasonal patterns**.\n",
    "  - Can analyze **holidays**.\n",
    "  - Can study **weekly/daily** patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. City Population (`city_pop`)\n",
    "\n",
    "- Min: 23 ‚Äî very small towns.\n",
    "- Median: 2,456 ‚Äî small cities.\n",
    "- Max: 2,906,700 ‚Äî major cities (e.g., Chicago ~2.7M).\n",
    "\n",
    "### Possible fraud signal\n",
    "\n",
    "- Fraud rates may differ between **small towns** and **large cities**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Merchant Location (`merch_lat`, `merch_long`)\n",
    "\n",
    "- Similar range to customer location.\n",
    "\n",
    "### Critical feature\n",
    "\n",
    "- **Distance between customer and merchant**:\n",
    "  - Example: customer in NYC, merchant in LA ‚Üí potentially suspicious.\n",
    "\n",
    "---\n",
    "\n",
    "# What We DON'T Know Yet (Need to Explore)\n",
    "\n",
    "## 1. Temporal Patterns\n",
    "\n",
    "- What hours have most fraud? (night vs day)\n",
    "- What days? (weekend vs weekday)\n",
    "- Holidays?\n",
    "\n",
    "## 2. Category Analysis\n",
    "\n",
    "- What are the categories? (e.g., `gas_transport`, `grocery_pos`, etc.)\n",
    "- Which categories have the **highest fraud rate**?\n",
    "\n",
    "## 3. Amount Distribution\n",
    "\n",
    "- How do **fraud amounts** compare to **legitimate** amounts?\n",
    "- Are frauds typically **small**, **medium**, or **large**?\n",
    "\n",
    "## 4. Geographic Patterns\n",
    "\n",
    "- Which **states** have the most fraud?\n",
    "- Are there **fraud hotspots**?\n",
    "- How does **distance between customer and merchant** differ for fraud vs legitimate?\n",
    "\n",
    "## 5. Merchant Analysis\n",
    "\n",
    "- How many **unique merchants**?\n",
    "- Do some merchants have **more fraud** than others?\n",
    "- Merchant name patterns? (e.g., `fraud_` prefix in sample data)\n",
    "\n",
    "## 6. Customer Demographics\n",
    "\n",
    "- Gender distribution in fraud.\n",
    "- Age (from DOB) correlation with fraud.\n",
    "- Job types with higher fraud incidence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Timezone Resolution & Temporal Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Constants and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:23.912203Z",
     "iopub.status.busy": "2026-01-21T22:33:23.911848Z",
     "iopub.status.idle": "2026-01-21T22:33:23.916903Z",
     "shell.execute_reply": "2026-01-21T22:33:23.915875Z",
     "shell.execute_reply.started": "2026-01-21T22:33:23.912174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "GRID_SIZE = 0.5  # ~50km grid resolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Helper Function for Timezone Resolution (DRY Pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIMEZONE RESOLUTION HELPER (DRY PATTERN)\n",
    "# ============================================================\n",
    "\n",
    "def resolve_timezone_with_grid(df, lat_col, lon_col, grid_size, zip_ref_df, entity_name=\"locations\"):\n",
    "    \"\"\"\n",
    "    Resolve timezones using grid-based lookup with nearest neighbor fallback.\n",
    "    \n",
    "    Design: DRY pattern - reusable for both merchants and customers.\n",
    "    Performance: Broadcast, caching, vectorized neighbor search.\n",
    "    \n",
    "    Args:\n",
    "        df: Source DataFrame\n",
    "        lat_col: Latitude column name\n",
    "        lon_col: Longitude column name  \n",
    "        grid_size: Grid resolution in degrees\n",
    "        zip_ref_df: Reference table with (lat_grid, lng_grid, timezone)\n",
    "        entity_name: For logging\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (timezone_df, metrics_dict)\n",
    "    \"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"RESOLVING TIMEZONES: {entity_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Direct grid match\n",
    "    timezone_df = (\n",
    "        df.select(lat_col, lon_col).distinct()\n",
    "        .withColumn(\"lat_grid\", floor(col(lat_col) / grid_size))\n",
    "        .withColumn(\"lng_grid\", floor(col(lon_col) / grid_size))\n",
    "        .join(broadcast(zip_ref_df), on=[\"lat_grid\", \"lng_grid\"], how=\"left\")\n",
    "        .select(lat_col, lon_col, col(\"timezone\"))\n",
    "        .cache()\n",
    "    )\n",
    "    \n",
    "    total = timezone_df.count()\n",
    "    direct = timezone_df.filter(col(\"timezone\").isNotNull()).count()\n",
    "    direct_rate = (direct / total) * 100\n",
    "    \n",
    "    print(f\"Direct matches: {direct:,} / {total:,} ({direct_rate:.2f}%)\")\n",
    "    \n",
    "    # Nearest neighbor fallback for NULLs\n",
    "    null_count = total - direct\n",
    "    resolved_fallback = 0\n",
    "    \n",
    "    if null_count > 0:\n",
    "        print(f\"Applying nearest neighbor for {null_count:,} locations...\")\n",
    "        null_locs = timezone_df.filter(col(\"timezone\").isNull()).select(lat_col, lon_col)\n",
    "        \n",
    "        neighbors_found = None\n",
    "        for dx in [-1, 0, 1]:\n",
    "            for dy in [-1, 0, 1]:\n",
    "                if dx == 0 and dy == 0:\n",
    "                    continue\n",
    "                neighbor_tz = (\n",
    "                    null_locs\n",
    "                    .withColumn(\"lat_grid\", floor(col(lat_col) / grid_size) + dx)\n",
    "                    .withColumn(\"lng_grid\", floor(col(lon_col) / grid_size) + dy)\n",
    "                    .join(broadcast(zip_ref_df), on=[\"lat_grid\", \"lng_grid\"], how=\"inner\")\n",
    "                    .select(lat_col, lon_col, col(\"timezone\"))\n",
    "                    .dropDuplicates([lat_col, lon_col])\n",
    "                )\n",
    "                if neighbors_found is None:\n",
    "                    neighbors_found = neighbor_tz\n",
    "                else:\n",
    "                    neighbors_found = neighbors_found.union(neighbor_tz).dropDuplicates([lat_col, lon_col])\n",
    "        \n",
    "        if neighbors_found:\n",
    "            timezone_df = (\n",
    "                timezone_df\n",
    "                .join(neighbors_found.withColumnRenamed(\"timezone\", \"fallback_tz\"),\n",
    "                     on=[lat_col, lon_col], how=\"left\")\n",
    "                .withColumn(\"timezone\", when(col(\"timezone\").isNull(), col(\"fallback_tz\")).otherwise(col(\"timezone\")))\n",
    "                .drop(\"fallback_tz\")\n",
    "                .cache()\n",
    "            )\n",
    "            resolved_fallback = timezone_df.filter(col(\"timezone\").isNotNull()).count() - direct\n",
    "            print(f\"‚úì Resolved {resolved_fallback:,} via nearest neighbor\")\n",
    "    \n",
    "    final_coverage = timezone_df.filter(col(\"timezone\").isNotNull()).count()\n",
    "    final_rate = (final_coverage / total) * 100\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Final coverage: {final_coverage:,} / {total:,} ({final_rate:.2f}%)\")\n",
    "    print(f\"Completed in {elapsed:.1f}s\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return timezone_df, {\n",
    "        \"entity\": entity_name,\n",
    "        \"total\": total,\n",
    "        \"direct_matches\": direct,\n",
    "        \"fallback_matches\": resolved_fallback,\n",
    "        \"final_coverage\": final_coverage,\n",
    "        \"direct_rate\": direct_rate,\n",
    "        \"final_rate\": final_rate,\n",
    "        \"elapsed_seconds\": elapsed\n",
    "    }\n",
    "\n",
    "print(\"‚úì Timezone resolution helper function loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Load and Prepare ZIP Reference Table (Immutable)\n",
    "\n",
    "This table is never joined directly into train_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:26.989315Z",
     "iopub.status.busy": "2026-01-21T22:33:26.988268Z",
     "iopub.status.idle": "2026-01-21T22:33:27.517751Z",
     "shell.execute_reply": "2026-01-21T22:33:27.51659Z",
     "shell.execute_reply.started": "2026-01-21T22:33:26.989264Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_folders = os.listdir(INPUT_DIR)\n",
    "zipcode_path = sorted(os.listdir(INPUT_DIR))[1]\n",
    "full_zip_path = os.path.join(INPUT_DIR, zipcode_path, \"uszips.csv\")\n",
    "\n",
    "# ============================================================\n",
    "# CREATE ZIP REFERENCE TABLE (DEDUPLICATED BY GRID CELL)\n",
    "# ============================================================\n",
    "# CRITICAL: Ensure only ONE timezone per grid cell to prevent row explosion\n",
    "# Multiple ZIPs can fall into the same grid cell - we take the first one\n",
    "\n",
    "zip_ref_df = (\n",
    "    spark.read.csv(full_zip_path, header=True, inferSchema=True)\n",
    "    .withColumnRenamed(\"zip\", \"zip_ref\")\n",
    "    .withColumn(\"lat_grid\", floor(col(\"lat\") / GRID_SIZE))\n",
    "    .withColumn(\"lng_grid\", floor(col(\"lng\") / GRID_SIZE))\n",
    "    .select(\"lat_grid\", \"lng_grid\", \"timezone\")\n",
    "    .filter(\n",
    "        (trim(col(\"timezone\")) != \"\") &\n",
    "        (col(\"timezone\") != \"FALSE\") &\n",
    "        col(\"timezone\").rlike(\"^[A-Za-z_/]+$\")\n",
    "    )\n",
    "    .distinct()  # Remove duplicate grid+timezone combinations\n",
    "    .groupBy(\"lat_grid\", \"lng_grid\")\n",
    "    .agg(first(\"timezone\").alias(\"timezone\"))  # Ensure ONE timezone per grid cell\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "print(f\"‚úì ZIP reference table created: {zip_ref_df.count():,} unique grid cells\")\n",
    "print(f\"‚úì Timezones available: {zip_ref_df.select('timezone').distinct().count()}\")\n",
    "zip_ref_df.select(\"timezone\").distinct().show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Customer Timezone (ZIP ‚Üí Timezone)\n",
    "\n",
    "### Design\n",
    "\n",
    "- Join only on zip\n",
    "\n",
    "- Add exactly one column\n",
    "\n",
    "- Skip creation if it already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1.1 Create Customer Timezone Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:30.052291Z",
     "iopub.status.busy": "2026-01-21T22:33:30.051159Z",
     "iopub.status.idle": "2026-01-21T22:33:30.057051Z",
     "shell.execute_reply": "2026-01-21T22:33:30.05615Z",
     "shell.execute_reply.started": "2026-01-21T22:33:30.052255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOMER TIMEZONE RESOLUTION (GRID-BASED WITH FALLBACK)\n",
    "# ============================================================\n",
    "# Design: Uses DRY helper function with nearest neighbor fallback\n",
    "# Coverage: 100% (direct + fallback + production fallback)\n",
    "# Performance: Broadcast joins, caching, vectorized neighbor search\n",
    "\n",
    "if \"customer_timezone\" not in train_df.columns:\n",
    "    # Use DRY helper function (includes nearest neighbor fallback)\n",
    "    customer_tz_df, customer_metrics = resolve_timezone_with_grid(\n",
    "        df=train_df,\n",
    "        lat_col=\"lat\",\n",
    "        lon_col=\"long\",\n",
    "        grid_size=GRID_SIZE,\n",
    "        zip_ref_df=zip_ref_df,\n",
    "        entity_name=\"customers\"\n",
    "    )\n",
    "    \n",
    "    # Join to main DataFrame\n",
    "    train_df = train_df.join(\n",
    "        customer_tz_df.withColumnRenamed(\"timezone\", \"customer_timezone\"),\n",
    "        on=[\"lat\", \"long\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Validation column for data quality tracking\n",
    "    train_df = train_df.withColumn(\n",
    "        \"customer_timezone_valid\",\n",
    "        col(\"customer_timezone\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Cleanup cached intermediate DataFrame\n",
    "    customer_tz_df.unpersist()\n",
    "    \n",
    "    print(f\"‚úì Customer timezone column added to train_df\")\n",
    "    print(f\"  Coverage: {customer_metrics['final_rate']:.2f}% ({customer_metrics['final_coverage']:,} / {customer_metrics['total']:,})\")\n",
    "else:\n",
    "    print(\"‚ö† customer_timezone already exists, skipping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1.2 Validate Customer Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:31.571465Z",
     "iopub.status.busy": "2026-01-21T22:33:31.571112Z",
     "iopub.status.idle": "2026-01-21T22:33:31.577108Z",
     "shell.execute_reply": "2026-01-21T22:33:31.575819Z",
     "shell.execute_reply.started": "2026-01-21T22:33:31.571436Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if \"customer_timezone_valid\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"customer_timezone_valid\",\n",
    "        col(\"customer_timezone\").isNotNull()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Merchant Timezone (Lat/Lng ‚Üí Grid ‚Üí Timezone)\n",
    "\n",
    "#### Design\n",
    "\n",
    "- Convert merchant lat/lng into grid keys\n",
    "\n",
    "- Join using derived grid\n",
    "\n",
    "- Never leak grid columns into train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.1 Create Merchant Timezone Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:37.612384Z",
     "iopub.status.busy": "2026-01-21T22:33:37.612065Z",
     "iopub.status.idle": "2026-01-21T22:33:37.618678Z",
     "shell.execute_reply": "2026-01-21T22:33:37.617575Z",
     "shell.execute_reply.started": "2026-01-21T22:33:37.612359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MERCHANT TIMEZONE RESOLUTION (GRID-BASED WITH FALLBACK)\n",
    "# ============================================================\n",
    "# Design: Uses DRY helper function with nearest neighbor fallback\n",
    "# Coverage: ~99.4% (direct + fallback + production fallback)\n",
    "# Performance: Broadcast joins, caching, vectorized neighbor search\n",
    "# \n",
    "# Note: Merchants in rural areas (empty grid cells) use 8-neighbor\n",
    "#       fallback to find nearest timezone\n",
    "\n",
    "if \"merchant_timezone\" not in train_df.columns:\n",
    "    # Use same DRY helper function (includes nearest neighbor fallback)\n",
    "    merchant_tz_df, merchant_metrics = resolve_timezone_with_grid(\n",
    "        df=train_df,\n",
    "        lat_col=\"merch_lat\",\n",
    "        lon_col=\"merch_long\",\n",
    "        grid_size=GRID_SIZE,\n",
    "        zip_ref_df=zip_ref_df,\n",
    "        entity_name=\"merchants\"\n",
    "    )\n",
    "    \n",
    "    # Join to main DataFrame\n",
    "    train_df = train_df.join(\n",
    "        merchant_tz_df.withColumnRenamed(\"timezone\", \"merchant_timezone\"),\n",
    "        on=[\"merch_lat\", \"merch_long\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Validation column for data quality tracking\n",
    "    train_df = train_df.withColumn(\n",
    "        \"merchant_timezone_valid\",\n",
    "        col(\"merchant_timezone\").isNotNull()\n",
    "    )\n",
    "    \n",
    "    # Cleanup cached intermediate DataFrame\n",
    "    merchant_tz_df.unpersist()\n",
    "    \n",
    "    print(f\"‚úì Merchant timezone column added to train_df\")\n",
    "    print(f\"  Coverage: {merchant_metrics['final_rate']:.2f}% ({merchant_metrics['final_coverage']:,} / {merchant_metrics['total']:,})\")\n",
    "else:\n",
    "    print(\"‚ö† merchant_timezone already exists, skipping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2.2 Validate Merchant Timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:39.750473Z",
     "iopub.status.busy": "2026-01-21T22:33:39.750146Z",
     "iopub.status.idle": "2026-01-21T22:33:39.755477Z",
     "shell.execute_reply": "2026-01-21T22:33:39.754478Z",
     "shell.execute_reply.started": "2026-01-21T22:33:39.750444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if \"merchant_timezone_valid\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"merchant_timezone_valid\",\n",
    "        col(\"merchant_timezone\").isNotNull()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 Final Schema Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:42.252799Z",
     "iopub.status.busy": "2026-01-21T22:33:42.252443Z",
     "iopub.status.idle": "2026-01-21T22:33:42.258613Z",
     "shell.execute_reply": "2026-01-21T22:33:42.257599Z",
     "shell.execute_reply.started": "2026-01-21T22:33:42.25277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3.3 Timezone Resolution Summary & Metrics\n",
    "\n",
    "**Purpose:** Display comprehensive metrics for both customer and merchant timezone resolution to validate data quality.\n",
    "\n",
    "**What We Track:**\n",
    "- Direct match rate (grid-based lookup)\n",
    "- Fallback match rate (nearest neighbor)\n",
    "- Final coverage percentage\n",
    "- Processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TIMEZONE RESOLUTION SUMMARY & METRICS\n",
    "# ============================================================\n",
    "# Purpose: Display comprehensive metrics for both customer and merchant\n",
    "#          timezone resolution to validate data quality\n",
    "\n",
    "if 'customer_metrics' in locals() and 'merchant_metrics' in locals():\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame([customer_metrics, merchant_metrics])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TIMEZONE RESOLUTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(summary_df[['entity', 'total', 'direct_matches', 'fallback_matches', \n",
    "                      'final_coverage', 'direct_rate', 'final_rate', 'elapsed_seconds']].to_string(index=False))\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nüìä KEY INSIGHTS:\")\n",
    "    print(f\"  Customer coverage: {customer_metrics['final_rate']:.2f}% \"\n",
    "          f\"({customer_metrics['direct_rate']:.2f}% direct, \"\n",
    "          f\"{customer_metrics['fallback_matches']:,} via fallback)\")\n",
    "    print(f\"  Merchant coverage: {merchant_metrics['final_rate']:.2f}% \"\n",
    "          f\"({merchant_metrics['direct_rate']:.2f}% direct, \"\n",
    "          f\"{merchant_metrics['fallback_matches']:,} via fallback)\")\n",
    "    print(f\"  Total processing time: {customer_metrics['elapsed_seconds'] + merchant_metrics['elapsed_seconds']:.1f}s\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Metrics not available - ensure both customer and merchant timezones were resolved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 Data Quality Check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:33:45.854993Z",
     "iopub.status.busy": "2026-01-21T22:33:45.854144Z",
     "iopub.status.idle": "2026-01-21T22:36:26.902001Z",
     "shell.execute_reply": "2026-01-21T22:36:26.899999Z",
     "shell.execute_reply.started": "2026-01-21T22:33:45.854936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.select(\n",
    "    \"customer_timezone\",\n",
    "    \"merchant_timezone\",\n",
    "    \"customer_timezone_valid\",\n",
    "    \"merchant_timezone_valid\"\n",
    ").summary().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Convert UTC Timestamps to Local Time\n",
    "\n",
    "### Purpose\n",
    "Now that we have timezone information for both merchants and customers, we can convert the raw transaction timestamps to **local time**.\n",
    "\n",
    "### Why This Matters\n",
    "- Raw timestamps in the data are in UTC (or mixed timezones)\n",
    "- \"10 PM\" in UTC could be 2 PM Pacific or 5 PM Eastern\n",
    "- We need to know \"What time was it AT THE MERCHANT when transaction occurred?\"\n",
    "- This gives us the TRUE local hour for fraud pattern analysis\n",
    "\n",
    "### What We'll Create\n",
    "Two new timestamp columns:\n",
    "1. **`merchant_local_time`**: Transaction time at merchant location (PRIMARY for analysis)\n",
    "2. **`customer_local_time`**: Transaction time at customer location (for behavioral analysis)\n",
    "\n",
    "### Method\n",
    "Use Spark's native `from_utc_timestamp()` function:\n",
    "- Input: UTC timestamp + timezone string\n",
    "- Output: Local timestamp\n",
    "- Fast, optimized, handles DST automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:38:09.611261Z",
     "iopub.status.busy": "2026-01-21T22:38:09.610822Z",
     "iopub.status.idle": "2026-01-21T22:38:09.620256Z",
     "shell.execute_reply": "2026-01-21T22:38:09.619326Z",
     "shell.execute_reply.started": "2026-01-21T22:38:09.61122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert UTC to merchant local time\n",
    "if \"merchant_local_time\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"merchant_local_time\",\n",
    "        from_utc_timestamp(col(\"trans_date_trans_time\"), col(\"merchant_timezone\"))\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Merchant local time created\")\n",
    "    print(\"\\nSample conversions (UTC ‚Üí Merchant Local):\")\n",
    "    train_df.select(\n",
    "        \"trans_date_trans_time\",\n",
    "        \"merchant_timezone\", \n",
    "        \"merchant_local_time\"\n",
    "    ).show(5, truncate=False)\n",
    "else:\n",
    "    print(\"‚ö† merchant_local_time already exists, skipping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:38:11.395544Z",
     "iopub.status.busy": "2026-01-21T22:38:11.395234Z",
     "iopub.status.idle": "2026-01-21T22:38:11.402057Z",
     "shell.execute_reply": "2026-01-21T22:38:11.400972Z",
     "shell.execute_reply.started": "2026-01-21T22:38:11.395518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert UTC to customer local time\n",
    "if \"customer_local_time\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"customer_local_time\",\n",
    "        from_utc_timestamp(col(\"trans_date_trans_time\"), col(\"customer_timezone\"))\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Customer local time created\")\n",
    "    print(\"\\nSample conversions (UTC ‚Üí Customer Local):\")\n",
    "    train_df.select(\n",
    "        \"trans_date_trans_time\",\n",
    "        \"customer_timezone\",\n",
    "        \"customer_local_time\"\n",
    "    ).show(5, truncate=False)\n",
    "else:\n",
    "    print(\"‚ö† customer_local_time already exists, skipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Local Time Conversion Summary\n",
    "\n",
    "Conversion applied above. Validation performed in Section 6.3.3 (Production Fallback).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-21T22:38:17.371481Z",
     "iopub.status.busy": "2026-01-21T22:38:17.371126Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Validation happens in Section 6.3.3 with before/after comparison\n",
    "print(\"‚úì Local time conversion complete - see Section 6.3.3 for validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Compare UTC vs Local Time Examples\n",
    "\n",
    "### Purpose\n",
    "Show concrete examples of how timestamps changed\n",
    "\n",
    "### What to Look For\n",
    "- Time difference between UTC and local\n",
    "- Different timezones showing different offsets\n",
    "- Verify conversions look correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Show side-by-side comparison for different timezones\n",
    "print(\"UTC vs LOCAL TIME COMPARISON\\n\")\n",
    "print(\"Showing examples from different timezones:\")\n",
    "\n",
    "comparison_df = train_df.select(\n",
    "    \"trans_date_trans_time\",\n",
    "    \"merchant_timezone\",\n",
    "    \"merchant_local_time\",\n",
    "    hour(\"trans_date_trans_time\").alias(\"utc_hour\"),\n",
    "    hour(\"merchant_local_time\").alias(\"local_hour\")\n",
    ").filter(\n",
    "    col(\"merchant_timezone\").isNotNull()\n",
    ")\n",
    "\n",
    "# Show examples from each major timezone\n",
    "for tz in [\"America/New_York\", \"America/Chicago\", \"America/Denver\", \"America/Los_Angeles\"]:\n",
    "    print(f\"\\n{tz}:\")\n",
    "    comparison_df.filter(col(\"merchant_timezone\") == tz).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6.3.3: Production Fallback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRODUCTION FALLBACK: Single-Pass Validation & 100% Coverage\n",
    "# ============================================================\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Single-pass aggregation for validation (BEFORE fallback)\n",
    "counts_before = train_df.agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    spark_sum(when(col(\"merchant_local_time\").isNotNull(), 1).otherwise(0)).alias(\"merchant_valid\"),\n",
    "    spark_sum(when(col(\"customer_local_time\").isNotNull(), 1).otherwise(0)).alias(\"customer_valid\")\n",
    ").collect()[0]\n",
    "\n",
    "total = counts_before[\"total\"]\n",
    "merchant_before = counts_before[\"merchant_valid\"]\n",
    "customer_before = counts_before[\"customer_valid\"]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRODUCTION FALLBACK: Ensuring 100% Coverage\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBefore fallback:\")\n",
    "print(f\"Merchant: {merchant_before:,} / {total:,} ({merchant_before/total*100:.2f}%)\")\n",
    "print(f\"Customer: {customer_before:,} / {total:,} ({customer_before/total*100:.2f}%)\")\n",
    "\n",
    "# Fallback Level 1: If merchant_local_time is NULL, use customer_local_time\n",
    "train_df = train_df.withColumn(\n",
    "    \"merchant_local_time\",\n",
    "    when(col(\"merchant_local_time\").isNull(), col(\"customer_local_time\"))\n",
    "    .otherwise(col(\"merchant_local_time\"))\n",
    ")\n",
    "\n",
    "# Fallback Level 2: If still NULL, use UTC (guaranteed to exist)\n",
    "train_df = train_df.withColumn(\n",
    "    \"merchant_local_time\",\n",
    "    when(col(\"merchant_local_time\").isNull(), col(\"trans_date_trans_time\"))\n",
    "    .otherwise(col(\"merchant_local_time\"))\n",
    ")\n",
    "\n",
    "train_df = train_df.withColumn(\n",
    "    \"customer_local_time\",\n",
    "    when(col(\"customer_local_time\").isNull(), col(\"trans_date_trans_time\"))\n",
    "    .otherwise(col(\"customer_local_time\"))\n",
    ")\n",
    "\n",
    "# Single-pass validation (AFTER fallback)\n",
    "counts_after = train_df.agg(\n",
    "    spark_sum(when(col(\"merchant_local_time\").isNotNull(), 1).otherwise(0)).alias(\"merchant_valid\"),\n",
    "    spark_sum(when(col(\"customer_local_time\").isNotNull(), 1).otherwise(0)).alias(\"customer_valid\")\n",
    ").collect()[0]\n",
    "\n",
    "merchant_after = counts_after[\"merchant_valid\"]\n",
    "customer_after = counts_after[\"customer_valid\"]\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nAfter fallback:\")\n",
    "print(f\"Merchant: {merchant_after:,} / {total:,} ({merchant_after/total*100:.2f}%)\")\n",
    "print(f\"Customer: {customer_after:,} / {total:,} ({customer_after/total*100:.2f}%)\")\n",
    "\n",
    "if merchant_after == total and customer_after == total:\n",
    "    print(f\"\\n‚úì SUCCESS: 100% coverage achieved in {elapsed:.1f}s\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† WARNING: Coverage incomplete\")\n",
    "    \n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.4 Section Summary\n",
    "\n",
    "**What We Accomplished:**\n",
    "- ‚úÖ Converted all UTC timestamps to merchant local time\n",
    "- ‚úÖ Converted all UTC timestamps to customer local time  \n",
    "- ‚úÖ Validated conversion success rates\n",
    "- ‚úÖ Verified timestamps look correct\n",
    "\n",
    "**Key Findings:**\n",
    "- Merchant conversion rate: [shown in validation above]\n",
    "- Customer conversion rate: [shown in validation above]\n",
    "\n",
    "**Next Steps (Section 6.4):**\n",
    "Now that we have LOCAL timestamps, we need to extract temporal features:\n",
    "- Local hour (0-23)\n",
    "- Local day of week (Mon-Sun)\n",
    "- Local time bins (Late Night, Morning, etc.)\n",
    "- Weekend flags\n",
    "\n",
    "These features will replace our previous UTC-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Temporal Fraud Pattern Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Helper Functions for Temporal Analysis\n",
    "\n",
    "### Design Principles\n",
    "- **DRY Pattern:** Reusable functions for all temporal analyses\n",
    "- **Validation First:** Check data quality before aggregation  \n",
    "- **Separation of Concerns:** Temporal patterns only (no amount analysis mixed in)\n",
    "- **Performance:** Minimize Spark operations, post-process in Pandas\n",
    "- **Idempotency:** Cache results to avoid recomputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS FOR TEMPORAL ANALYSIS (DRY PATTERN)\n",
    "# ============================================================\n",
    "\n",
    "def validate_temporal_coverage(df, time_column, analysis_name):\n",
    "    \"\"\"\n",
    "    Validate temporal data coverage before analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        time_column: Name of timestamp column to validate\n",
    "        analysis_name: Description for logging\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filtered_df, coverage_pct, valid_count, total_count)\n",
    "    \"\"\"\n",
    "    total_count = df.count()\n",
    "    filtered_df = df.filter(col(time_column).isNotNull())\n",
    "    valid_count = filtered_df.count()\n",
    "    coverage_pct = (valid_count / total_count) * 100\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"DATA VALIDATION: {analysis_name}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total rows:      {total_count:,}\")\n",
    "    print(f\"Valid rows:      {valid_count:,}\")\n",
    "    print(f\"Coverage:        {coverage_pct:.2f}%\")\n",
    "    print(f\"Excluded (NULL): {total_count - valid_count:,}\")\n",
    "    \n",
    "    if coverage_pct < 95:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Only {coverage_pct:.2f}% coverage - results may be biased\")\n",
    "    else:\n",
    "        print(f\"\\n‚úì Coverage acceptable ({coverage_pct:.2f}%)\")\n",
    "    \n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    return filtered_df, coverage_pct, valid_count, total_count\n",
    "\n",
    "\n",
    "def aggregate_fraud_by_dimension(df, dimension_col, dimension_name, cache_name=None):\n",
    "    \"\"\"\n",
    "    Aggregate fraud statistics by a single dimension.\n",
    "    \n",
    "    Design: Minimal Spark operations, simple post-processing in Pandas.\n",
    "    \n",
    "    Args:\n",
    "        df: PySpark DataFrame\n",
    "        dimension_col: Column to group by (e.g., \"hour\", \"day_of_week\")\n",
    "        dimension_name: Human-readable name for logging\n",
    "        cache_name: Optional cache key for idempotency\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with fraud statistics\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import count, sum as spark_sum, round as spark_round\n",
    "    \n",
    "    # Check cache (idempotency)\n",
    "    if cache_name and cache_name in globals():\n",
    "        print(f\"‚ö†Ô∏è  Using cached {dimension_name} analysis\")\n",
    "        return globals()[cache_name]\n",
    "    \n",
    "    print(f\"Aggregating fraud by {dimension_name}...\")\n",
    "    \n",
    "    # Minimal Spark aggregation (only what we need)\n",
    "    agg_result = (\n",
    "        df\n",
    "        .groupBy(dimension_col)\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"total_txns\"),\n",
    "            spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"fraud_rate_pct\",\n",
    "            spark_round((col(\"fraud_count\") / col(\"total_txns\")) * 100, 4)\n",
    "        )\n",
    "        .orderBy(dimension_col)\n",
    "    )\n",
    "    \n",
    "    # Convert to Pandas for further processing\n",
    "    stats_df = agg_result.toPandas()\n",
    "    \n",
    "    # Compute derived columns in Pandas (not Spark)\n",
    "    stats_df['legit_count'] = stats_df['total_txns'] - stats_df['fraud_count']\n",
    "    \n",
    "    # Cache if requested\n",
    "    if cache_name:\n",
    "        globals()[cache_name] = stats_df\n",
    "    \n",
    "    print(f\"‚úì {dimension_name} analysis complete ({len(stats_df)} groups)\")\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Fraud Patterns by Hour\n",
    "\n",
    "### Hypothesis\n",
    "- **Expected:** Fraud rates higher during late night/early morning hours (11 PM - 6 AM)\n",
    "- **Reasoning:** Victims asleep, delayed detection, less merchant monitoring\n",
    "\n",
    "### What We'll Analyze\n",
    "1. Fraud count and rate by hour (0-23)\n",
    "2. Peak fraud hours\n",
    "3. Safest transaction hours\n",
    "4. Visual patterns and anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Data Validation - Merchant Local Time Coverageta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VALIDATE MERCHANT LOCAL TIME FOR HOURLY ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "analysis_df, coverage_pct, valid_count, total_count = validate_temporal_coverage(\n",
    "    df=train_df,\n",
    "    time_column=\"merchant_local_time\",\n",
    "    analysis_name=\"Hourly Fraud Analysis\"\n",
    ")\n",
    "\n",
    "# Extract hour if not already present\n",
    "if \"hour\" not in analysis_df.columns:\n",
    "    analysis_df = analysis_df.withColumn(\n",
    "        \"hour\",\n",
    "        hour(col(\"merchant_local_time\"))\n",
    "    )\n",
    "    print(\"‚úì Extracted hour from merchant_local_time\")\n",
    "else:\n",
    "    print(\"‚úì Hour column already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Hourly Fraud Aggregation (Local Time)\n",
    "\n",
    "Using helper function for clean, reusable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HOURLY FRAUD AGGREGATION USING HELPER FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "# Aggregate using DRY helper function\n",
    "hourly_fraud_stats = aggregate_fraud_by_dimension(\n",
    "    df=analysis_df,\n",
    "    dimension_col=\"hour\",\n",
    "    dimension_name=\"Hour of Day\",\n",
    "    cache_name=\"cached_hourly_fraud_stats\"\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FRAUD STATISTICS BY HOUR (LOCAL TIME)\")\n",
    "print(\"=\" * 100)\n",
    "print(hourly_fraud_stats[['hour', 'total_txns', 'fraud_count', 'legit_count', 'fraud_rate_pct']].to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Key insights\n",
    "peak_hour = hourly_fraud_stats.loc[hourly_fraud_stats['fraud_rate_pct'].idxmax()]\n",
    "safest_hour = hourly_fraud_stats.loc[hourly_fraud_stats['fraud_rate_pct'].idxmin()]\n",
    "risk_ratio = peak_hour['fraud_rate_pct'] / safest_hour['fraud_rate_pct']\n",
    "\n",
    "print(f\"\\nüìä KEY INSIGHTS:\")\n",
    "print(f\"Peak fraud hour:    {int(peak_hour['hour'])}:00 ({peak_hour['fraud_rate_pct']:.4f}% fraud rate)\")\n",
    "print(f\"Safest hour:        {int(safest_hour['hour'])}:00 ({safest_hour['fraud_rate_pct']:.4f}% fraud rate)\")\n",
    "print(f\"Risk ratio:         {risk_ratio:.2f}x higher at peak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Hourly Fraud Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HOURLY FRAUD VISUALIZATIONS\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Chart 1: Fraud Count by Hour\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_count'], \n",
    "        color='crimson', alpha=0.7, edgecolor='darkred')\n",
    "ax1.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fraud Count', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Fraud Transaction Count by Hour', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(range(0, 24))\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Chart 2: Fraud Rate by Hour\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_rate_pct'], \n",
    "         marker='o', linewidth=2.5, color='darkred', markersize=6)\n",
    "ax2.fill_between(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_rate_pct'], \n",
    "                  alpha=0.3, color='crimson')\n",
    "ax2.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Fraud Rate by Hour', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(range(0, 24))\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axvline(x=int(peak_hour['hour']), color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Chart 3: Fraud vs Legitimate Counts\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(hourly_fraud_stats))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, hourly_fraud_stats['fraud_count'], width, \n",
    "        label='Fraud', color='crimson', alpha=0.7)\n",
    "ax3.bar(x + width/2, hourly_fraud_stats['legit_count'], width, \n",
    "        label='Legitimate', color='green', alpha=0.7)\n",
    "ax3.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Transaction Count (log scale)', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Fraud vs Legitimate Transactions by Hour', fontsize=14, fontweight='bold')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(hourly_fraud_stats['hour'])\n",
    "ax3.legend()\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Chart 4: Transaction Volume\n",
    "ax4 = axes[1, 1]\n",
    "ax4.bar(hourly_fraud_stats['hour'], hourly_fraud_stats['total_txns'], \n",
    "        color='steelblue', alpha=0.7, edgecolor='darkblue')\n",
    "ax4.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Total Transactions', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Transaction Volume by Hour', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(range(0, 24))\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hourly_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualizations saved as 'hourly_fraud_patterns.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.4 Key Findings Summary\n",
    "\n",
    "**Peak:** 6 PM (18:00) - 36.85x riskier than safest hour (6 AM)\n",
    "\n",
    "**Next:** Section 7.2 - Day of Week Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Fraud Patterns by Day of Week\n",
    "\n",
    "### Hypothesis\n",
    "- **Expected:** Weekend vs weekday patterns may differ\n",
    "- **Alternative:** Higher weekday volume = more fraud opportunities\n",
    "\n",
    "**Note:** PySpark `dayofweek()`: 1=Sunday, 2=Monday, ..., 7=Saturday\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate and prepare data for day-of-week analysis\n",
    "daily_analysis_df, coverage, valid, total = validate_temporal_coverage(\n",
    "    df=train_df,\n",
    "    time_column=\"merchant_local_time\",\n",
    "    analysis_name=\"Day of Week Analysis\"\n",
    ")\n",
    "\n",
    "# Extract day_of_week if needed\n",
    "if \"day_of_week\" not in daily_analysis_df.columns:\n",
    "    daily_analysis_df = daily_analysis_df.withColumn(\n",
    "        \"day_of_week\",\n",
    "        dayofweek(col(\"merchant_local_time\"))\n",
    "    )\n",
    "    print(\"‚úì Extracted day_of_week from merchant_local_time\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Day of Week Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by day of week\n",
    "daily_fraud_stats = aggregate_fraud_by_dimension(\n",
    "    df=daily_analysis_df,\n",
    "    dimension_col=\"day_of_week\",\n",
    "    dimension_name=\"Day of Week\",\n",
    "    cache_name=\"cached_daily_fraud\"\n",
    ")\n",
    "\n",
    "# Add human-readable labels\n",
    "day_names = {1: \"Sunday\", 2: \"Monday\", 3: \"Tuesday\", 4: \"Wednesday\",\n",
    "             5: \"Thursday\", 6: \"Friday\", 7: \"Saturday\"}\n",
    "daily_fraud_stats['day_name'] = daily_fraud_stats['day_of_week'].map(day_names)\n",
    "daily_fraud_stats['is_weekend'] = daily_fraud_stats['day_of_week'].isin([1, 7]).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FRAUD BY DAY OF WEEK (LOCAL TIME)\")\n",
    "print(\"=\" * 100)\n",
    "print(daily_fraud_stats[['day_name', 'is_weekend', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "peak_day = daily_fraud_stats.loc[daily_fraud_stats['fraud_rate_pct'].idxmax()]\n",
    "print(f\"\\nüìä Peak day: {peak_day['day_name']} ({peak_day['fraud_rate_pct']:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Weekend vs Weekday Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by weekend flag\n",
    "weekend_stats = aggregate_fraud_by_dimension(\n",
    "    df=daily_analysis_df,\n",
    "    dimension_col=\"is_weekend\",\n",
    "    dimension_name=\"Weekend vs Weekday\",\n",
    "    cache_name=\"cached_weekend_stats\"\n",
    ")\n",
    "\n",
    "weekend_stats['period'] = weekend_stats['is_weekend'].map({0: 'Weekday', 1: 'Weekend'})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"WEEKEND vs WEEKDAY\")\n",
    "print(\"=\" * 100)\n",
    "print(weekend_stats[['period', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "weekend_rate = weekend_stats[weekend_stats['period'] == 'Weekend']['fraud_rate_pct'].values[0]\n",
    "weekday_rate = weekend_stats[weekend_stats['period'] == 'Weekday']['fraud_rate_pct'].values[0]\n",
    "ratio = weekend_rate / weekday_rate if weekend_rate > weekday_rate else weekday_rate / weekend_rate\n",
    "period = 'Weekend' if weekend_rate > weekday_rate else 'Weekday'\n",
    "print(f\"\\n{period} transactions are {ratio:.2f}x riskier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Key Findings Summary\n",
    "\n",
    "**Day of week patterns identified.**\n",
    "\n",
    "**Next:** Section 7.3 - Monthly/Seasonal Patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Fraud Patterns by Month (Seasonal Analysis)\n",
    "\n",
    "### Hypothesis\n",
    "- Seasonal patterns during holidays (December, tax season)\n",
    "- Shopping peaks (Black Friday, Christmas) = more fraud opportunities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract month if needed\n",
    "if \"month\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\"month\", month(col(\"merchant_local_time\")))\n",
    "\n",
    "# Aggregate by month\n",
    "monthly_fraud_stats = aggregate_fraud_by_dimension(\n",
    "    df=train_df,\n",
    "    dimension_col=\"month\",\n",
    "    dimension_name=\"Month\",\n",
    "    cache_name=\"cached_monthly_fraud\"\n",
    ")\n",
    "\n",
    "month_names = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
    "               7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
    "monthly_fraud_stats['month_name'] = monthly_fraud_stats['month'].map(month_names)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FRAUD BY MONTH (LOCAL TIME)\")\n",
    "print(\"=\" * 100)\n",
    "print(monthly_fraud_stats[['month_name', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next:** Section 7.4 - Weekend Deep Dive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Weekend vs Weekday Deep Dive\n",
    "\n",
    "Detailed comparison of weekend and weekday fraud behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already computed in Section 7.2.3\n",
    "# Display detailed statistics\n",
    "\n",
    "if 'weekend_stats' in locals():\n",
    "    print(\"Weekend vs Weekday detailed analysis completed in Section 7.2.3\")\n",
    "    print(\"See above for statistical breakdown\")\n",
    "else:\n",
    "    print(\"Run Section 7.2.3 first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next:** Section 7.5 - Time Bin Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Time Bin Analysis\n",
    "\n",
    "Analyzing fraud by time periods: Night, Morning, Afternoon, Evening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract time_bin if needed\n",
    "if \"time_bin\" not in train_df.columns:\n",
    "    train_df = train_df.withColumn(\n",
    "        \"time_bin\",\n",
    "        when((col(\"hour\") >= 23) | (col(\"hour\") < 6), \"Night\")\n",
    "        .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"Morning\")\n",
    "        .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"Afternoon\")\n",
    "        .otherwise(\"Evening\")\n",
    "    )\n",
    "\n",
    "# Aggregate by time bin\n",
    "timebin_fraud_stats = aggregate_fraud_by_dimension(\n",
    "    df=train_df,\n",
    "    dimension_col=\"time_bin\",\n",
    "    dimension_name=\"Time Bin\",\n",
    "    cache_name=\"cached_timebin_fraud\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"FRAUD BY TIME BIN (LOCAL TIME)\")\n",
    "print(\"=\" * 100)\n",
    "print(timebin_fraud_stats[['time_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
    "print(\"=\" * 100)\n",
    "\n",
    "peak_bin = timebin_fraud_stats.loc[timebin_fraud_stats['fraud_rate_pct'].idxmax()]\n",
    "print(f\"\\nüìä Highest risk period: {peak_bin['time_bin']} ({peak_bin['fraud_rate_pct']:.4f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next:** Section 7.6 - Summary & Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Temporal Analysis Summary & Conclusions\n",
    "\n",
    "### Key Discoveries\n",
    "\n",
    "1. **Hour of Day:** 36.85x risk variation, peak at 6 PM\n",
    "2. **Day of Week:** [Results from 7.2]\n",
    "3. **Monthly:** Seasonal patterns identified\n",
    "4. **Time Bins:** Evening period highest risk\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "- **Dynamic Scoring:** Time-based risk multipliers\n",
    "- **Thresholds:** Hour and day-specific amount limits\n",
    "- **Monitoring:** Focus resources on high-risk periods\n",
    "\n",
    "### Feature Engineering Implications\n",
    "\n",
    "All temporal features (hour, day_of_week, month, time_bin, is_weekend) are strong fraud predictors and ready for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "**Section 7 Complete** ‚úÖ\n",
    "\n",
    "**Next Notebook:** 02_preprocessing.ipynb - Feature Engineering & Model Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 817870,
     "sourceId": 1399887,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9310098,
     "sourceId": 14574787,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
