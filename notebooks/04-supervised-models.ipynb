{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Models for Fraud Detection\n",
    "\n",
    "**Notebook:** 04_supervised_models.ipynb  \n",
    "**Objective:** Develop and evaluate baseline XGBoost model with iterative feature addition\n",
    "\n",
    "## Modeling Strategy\n",
    "\n",
    "**Baseline Approach:**\n",
    "- Start with Critical Priority features only (transaction_count_bin, card_age_bin, hour)\n",
    "- Use XGBoost with class weights to handle imbalance\n",
    "- Evaluate using Precision-Recall Curve, F1-Score, and AUC-PR\n",
    "\n",
    "**Iterative Feature Addition:**\n",
    "- Add features based on importance rankings\n",
    "- Monitor performance improvements\n",
    "- Stop when diminishing returns are observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GLOBAL IMPORTS & DEPENDENCIES\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION & PATHS\n",
    "# ============================================================\n",
    "\n",
    "# Base paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Preprocessed data paths\n",
    "PREPROCESSED_TRAIN_PATH = PROCESSED_DATA_DIR / 'train_preprocessed.parquet'\n",
    "PREPROCESSED_VAL_PATH = PROCESSED_DATA_DIR / 'val_preprocessed.parquet'\n",
    "PREPROCESSED_TEST_PATH = PROCESSED_DATA_DIR / 'test_preprocessed.parquet'\n",
    "PREPROCESSER_PATH = MODELS_DIR / 'preprocessor.pkl'\n",
    "FEATURE_NAMES_PATH = MODELS_DIR / 'feature_names.pkl'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LOAD PREPROCESSED DATA\n",
    "# ============================================================\n",
    "\n",
    "def load_preprocessed_data() -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load preprocessed train, validation, and test datasets.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "    if not PREPROCESSED_TRAIN_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Preprocessed data not found: {PREPROCESSED_TRAIN_PATH}\\n\"\n",
    "            \"Please run the preprocessing notebook (02_preprocessing.ipynb) first.\"\n",
    "        )\n",
    "    \n",
    "    print(\"Loading preprocessed data...\")\n",
    "    train_df = pd.read_parquet(PREPROCESSED_TRAIN_PATH)\n",
    "    val_df = pd.read_parquet(PREPROCESSED_VAL_PATH)\n",
    "    test_df = pd.read_parquet(PREPROCESSED_TEST_PATH)\n",
    "    \n",
    "    print(f\"\\n✓ Data loaded successfully\")\n",
    "    print(f\"  Train: {train_df.shape[0]:,} samples, {train_df.shape[1]} features\")\n",
    "    print(f\"  Validation: {val_df.shape[0]:,} samples, {val_df.shape[1]} features\")\n",
    "    print(f\"  Test: {test_df.shape[0]:,} samples, {test_df.shape[1]} features\")\n",
    "    \n",
    "    print(f\"\\nFraud rates:\")\n",
    "    print(f\"  Train: {train_df['is_fraud'].mean():.4%}\")\n",
    "    print(f\"  Validation: {val_df['is_fraud'].mean():.4%}\")\n",
    "    print(f\"  Test: {test_df['is_fraud'].mean():.4%}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, test_df = load_preprocessed_data()\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [col for col in train_df.columns if col != 'is_fraud']\n",
    "print(f\"\\nAvailable features: {len(feature_cols)}\")\n",
    "print(f\"Feature names: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE SET DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "# Critical Priority features (from EDA findings)\n",
    "CRITICAL_FEATURES = [\n",
    "    'transaction_count_bin',\n",
    "    'card_age_bin',\n",
    "    'hour',\n",
    "    'time_bin',\n",
    "    'is_peak_fraud_hour',\n",
    "    'is_new_card',\n",
    "    'is_low_volume_card'\n",
    "]\n",
    "\n",
    "# High Priority features\n",
    "HIGH_PRIORITY_FEATURES = [\n",
    "    'category',\n",
    "    'day_of_week',\n",
    "    'month',\n",
    "    'is_peak_fraud_day',\n",
    "    'is_peak_fraud_season',\n",
    "    'is_high_risk_category',\n",
    "    'card_age_days',\n",
    "    'transaction_count'\n",
    "]\n",
    "\n",
    "# Interaction features\n",
    "INTERACTION_FEATURES = [\n",
    "    'evening_high_amount',\n",
    "    'evening_online_shopping',\n",
    "    'large_city_evening',\n",
    "    'new_card_evening',\n",
    "    'high_amount_online'\n",
    "]\n",
    "\n",
    "# Enriched features\n",
    "ENRICHED_FEATURES = [\n",
    "    'temporal_risk_score',\n",
    "    'geographic_risk_score',\n",
    "    'card_risk_score',\n",
    "    'risk_tier'\n",
    "]\n",
    "\n",
    "# Filter to only features that exist in the dataset\n",
    "def filter_available_features(feature_list: List[str], available_cols: List[str]) -> List[str]:\n",
    "    \"\"\"Filter feature list to only include features that exist in the dataset.\"\"\"\n",
    "    return [f for f in feature_list if f in available_cols]\n",
    "\n",
    "CRITICAL_FEATURES_AVAIL = filter_available_features(CRITICAL_FEATURES, feature_cols)\n",
    "HIGH_PRIORITY_FEATURES_AVAIL = filter_available_features(HIGH_PRIORITY_FEATURES, feature_cols)\n",
    "INTERACTION_FEATURES_AVAIL = filter_available_features(INTERACTION_FEATURES, feature_cols)\n",
    "ENRICHED_FEATURES_AVAIL = filter_available_features(ENRICHED_FEATURES, feature_cols)\n",
    "\n",
    "print(\"Feature Set Summary:\")\n",
    "print(f\"  Critical Priority: {len(CRITICAL_FEATURES_AVAIL)}/{len(CRITICAL_FEATURES)} available\")\n",
    "print(f\"  High Priority: {len(HIGH_PRIORITY_FEATURES_AVAIL)}/{len(HIGH_PRIORITY_FEATURES)} available\")\n",
    "print(f\"  Interaction: {len(INTERACTION_FEATURES_AVAIL)}/{len(INTERACTION_FEATURES)} available\")\n",
    "print(f\"  Enriched: {len(ENRICHED_FEATURES_AVAIL)}/{len(ENRICHED_FEATURES)} available\")\n",
    "\n",
    "if CRITICAL_FEATURES_AVAIL:\n",
    "    print(f\"\\nCritical features to use: {CRITICAL_FEATURES_AVAIL}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Warning: No critical features found in dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATION METRICS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_model(\n",
    "    model: xgb.XGBClassifier,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    dataset_name: str = \"Dataset\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model performance on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        X: Feature matrix\n",
    "        y: True labels\n",
    "        dataset_name: Name of dataset for display\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'f1_score': f1_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'roc_auc': roc_auc_score(y, y_pred_proba),\n",
    "        'pr_auc': average_precision_score(y, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{dataset_name} Performance:\")\n",
    "    print(f\"  F1-Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  PR-AUC: {metrics['pr_auc']:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"    TN: {cm[0,0]:,}  FP: {cm[0,1]:,}\")\n",
    "    print(f\"    FN: {cm[1,0]:,}  TP: {cm[1,1]:,}\")\n",
    "    print(f\"    False Positive Rate: {cm[0,1]/(cm[0,0]+cm[0,1]):.4%}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_precision_recall_curve(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    dataset_name: str = \"Dataset\",\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> None:\n",
    "    \"\"\"Plot Precision-Recall curve.\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.plot(recall, precision, label=f'{dataset_name} (AUC={pr_auc:.4f})')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall Curve')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def plot_roc_curve(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    dataset_name: str = \"Dataset\",\n",
    "    ax: Optional[plt.Axes] = None\n",
    ") -> None:\n",
    "    \"\"\"Plot ROC curve.\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    ax.plot(fpr, tpr, label=f'{dataset_name} (AUC={roc_auc:.4f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('ROC Curve')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model: XGBoost with Critical Priority Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BASELINE MODEL: XGBOOST WITH CRITICAL FEATURES ONLY\n",
    "# ============================================================\n",
    "\n",
    "# Prepare data with Critical Priority features only\n",
    "baseline_features = CRITICAL_FEATURES_AVAIL if CRITICAL_FEATURES_AVAIL else feature_cols[:7]\n",
    "\n",
    "X_train_baseline = train_df[baseline_features].values\n",
    "y_train_baseline = train_df['is_fraud'].values\n",
    "\n",
    "X_val_baseline = val_df[baseline_features].values\n",
    "y_val_baseline = val_df['is_fraud'].values\n",
    "\n",
    "X_test_baseline = test_df[baseline_features].values\n",
    "y_test_baseline = test_df['is_fraud'].values\n",
    "\n",
    "print(f\"Baseline model using {len(baseline_features)} features:\")\n",
    "print(f\"  Features: {baseline_features}\")\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"  Train: X={X_train_baseline.shape}, y={y_train_baseline.shape}\")\n",
    "print(f\"  Validation: X={X_val_baseline.shape}, y={y_val_baseline.shape}\")\n",
    "print(f\"  Test: X={X_test_baseline.shape}, y={y_test_baseline.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAIN BASELINE XGBOOST MODEL WITH CLASS WEIGHTS\n",
    "# ============================================================\n",
    "\n",
    "# Calculate class weights (inverse of class frequency)\n",
    "n_samples = len(y_train_baseline)\n",
    "n_fraud = y_train_baseline.sum()\n",
    "n_legitimate = n_samples - n_fraud\n",
    "\n",
    "# XGBoost uses scale_pos_weight parameter\n",
    "# scale_pos_weight = (number of negative samples) / (number of positive samples)\n",
    "scale_pos_weight = n_legitimate / n_fraud\n",
    "\n",
    "print(f\"Class distribution:\")\n",
    "print(f\"  Legitimate: {n_legitimate:,} ({n_legitimate/n_samples:.2%})\")\n",
    "print(f\"  Fraud: {n_fraud:,} ({n_fraud/n_samples:.2%})\")\n",
    "print(f\"  scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# Train baseline model\n",
    "baseline_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "print(\"\\nTraining baseline model...\")\n",
    "baseline_model.fit(\n",
    "    X_train_baseline,\n",
    "    y_train_baseline,\n",
    "    eval_set=[(X_val_baseline, y_val_baseline)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"✓ Baseline model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EVALUATE BASELINE MODEL\n",
    "# ============================================================\n",
    "\n",
    "baseline_train_metrics = evaluate_model(baseline_model, X_train_baseline, y_train_baseline, \"Train\")\n",
    "baseline_val_metrics = evaluate_model(baseline_model, X_val_baseline, y_val_baseline, \"Validation\")\n",
    "baseline_test_metrics = evaluate_model(baseline_model, X_test_baseline, y_test_baseline, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZE BASELINE MODEL PERFORMANCE\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Precision-Recall curves\n",
    "y_train_proba = baseline_model.predict_proba(X_train_baseline)[:, 1]\n",
    "y_val_proba = baseline_model.predict_proba(X_val_baseline)[:, 1]\n",
    "y_test_proba = baseline_model.predict_proba(X_test_baseline)[:, 1]\n",
    "\n",
    "plot_precision_recall_curve(y_train_baseline, y_train_proba, \"Train\", axes[0])\n",
    "plot_precision_recall_curve(y_val_baseline, y_val_proba, \"Validation\", axes[0])\n",
    "plot_precision_recall_curve(y_test_baseline, y_test_proba, \"Test\", axes[0])\n",
    "\n",
    "# ROC curves\n",
    "plot_roc_curve(y_train_baseline, y_train_proba, \"Train\", axes[1])\n",
    "plot_roc_curve(y_val_baseline, y_val_proba, \"Validation\", axes[1])\n",
    "plot_roc_curve(y_test_baseline, y_test_proba, \"Test\", axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE ANALYSIS (BASELINE)\n",
    "# ============================================================\n",
    "\n",
    "feature_importance = baseline_model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': baseline_features,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Baseline Model Feature Importance:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance_df, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Baseline Model - Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store baseline results\n",
    "baseline_results = {\n",
    "    'features': baseline_features,\n",
    "    'train_metrics': baseline_train_metrics,\n",
    "    'val_metrics': baseline_val_metrics,\n",
    "    'test_metrics': baseline_test_metrics,\n",
    "    'feature_importance': importance_df.to_dict('records')\n",
    "}\n",
    "\n",
    "print(\"\\n✓ Baseline model evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Iterative Feature Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITERATIVE FEATURE ADDITION FUNCTION\n",
    "# ============================================================\n",
    "\n",
    "def train_and_evaluate_model(\n",
    "    features: List[str],\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    scale_pos_weight: float\n",
    ") -> Tuple[xgb.XGBClassifier, Dict[str, Dict[str, float]], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Train XGBoost model with given features and return metrics.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (model, metrics_dict, feature_importance_df)\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X_train = train_df[features].values\n",
    "    y_train = train_df['is_fraud'].values\n",
    "    X_val = val_df[features].values\n",
    "    y_val = val_df['is_fraud'].values\n",
    "    X_test = test_df[features].values\n",
    "    y_test = test_df['is_fraud'].values\n",
    "    \n",
    "    # Train model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    train_metrics = evaluate_model(model, X_train, y_train, \"Train\")\n",
    "    val_metrics = evaluate_model(model, X_val, y_val, \"Validation\")\n",
    "    test_metrics = evaluate_model(model, X_test, y_test, \"Test\")\n",
    "    \n",
    "    metrics = {\n",
    "        'train': train_metrics,\n",
    "        'val': val_metrics,\n",
    "        'test': test_metrics\n",
    "    }\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return model, metrics, importance_df\n",
    "\n",
    "print(\"Iterative feature addition function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITERATIVE FEATURE ADDITION: BUILD FEATURE SETS\n",
    "# ============================================================\n",
    "\n",
    "# Define feature addition order based on priority\n",
    "feature_sets = []\n",
    "\n",
    "# Step 1: Critical features only (baseline)\n",
    "feature_sets.append({\n",
    "    'name': 'Critical Only',\n",
    "    'features': CRITICAL_FEATURES_AVAIL\n",
    "})\n",
    "\n",
    "# Step 2: Critical + High Priority\n",
    "if HIGH_PRIORITY_FEATURES_AVAIL:\n",
    "    feature_sets.append({\n",
    "        'name': 'Critical + High Priority',\n",
    "        'features': CRITICAL_FEATURES_AVAIL + HIGH_PRIORITY_FEATURES_AVAIL\n",
    "    })\n",
    "\n",
    "# Step 3: Critical + High Priority + Interactions\n",
    "if INTERACTION_FEATURES_AVAIL:\n",
    "    feature_sets.append({\n",
    "        'name': 'Critical + High Priority + Interactions',\n",
    "        'features': CRITICAL_FEATURES_AVAIL + HIGH_PRIORITY_FEATURES_AVAIL + INTERACTION_FEATURES_AVAIL\n",
    "    })\n",
    "\n",
    "# Step 4: All features\n",
    "if ENRICHED_FEATURES_AVAIL:\n",
    "    feature_sets.append({\n",
    "        'name': 'All Features',\n",
    "        'features': CRITICAL_FEATURES_AVAIL + HIGH_PRIORITY_FEATURES_AVAIL + INTERACTION_FEATURES_AVAIL + ENRICHED_FEATURES_AVAIL\n",
    "    })\n",
    "\n",
    "print(f\"Feature addition sequence ({len(feature_sets)} steps):\")\n",
    "for i, fs in enumerate(feature_sets, 1):\n",
    "    print(f\"  Step {i}: {fs['name']} ({len(fs['features'])} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ITERATIVE FEATURE ADDITION: TRAIN MODELS\n",
    "# ============================================================\n",
    "\n",
    "all_results = []\n",
    "all_models = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ITERATIVE FEATURE ADDITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, feature_set in enumerate(feature_sets):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Step {i+1}/{len(feature_sets)}: {feature_set['name']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Features ({len(feature_set['features'])}): {feature_set['features']}\")\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model, metrics, importance_df = train_and_evaluate_model(\n",
    "        feature_set['features'],\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        scale_pos_weight\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    result = {\n",
    "        'step': i + 1,\n",
    "        'name': feature_set['name'],\n",
    "        'n_features': len(feature_set['features']),\n",
    "        'features': feature_set['features'],\n",
    "        'metrics': metrics,\n",
    "        'feature_importance': importance_df,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    all_results.append(result)\n",
    "    all_models.append(model)\n",
    "    \n",
    "    # Print improvement over previous step\n",
    "    if i > 0:\n",
    "        prev_val_f1 = all_results[i-1]['metrics']['val']['f1_score']\n",
    "        curr_val_f1 = metrics['val']['f1_score']\n",
    "        improvement = curr_val_f1 - prev_val_f1\n",
    "        print(f\"\\n  Improvement in Validation F1: {improvement:+.4f} ({improvement/prev_val_f1*100:+.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ITERATIVE FEATURE ADDITION COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARE MODEL PERFORMANCE ACROSS FEATURE SETS\n",
    "# ============================================================\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "for result in all_results:\n",
    "    comparison_data.append({\n",
    "        'Feature Set': result['name'],\n",
    "        'N Features': result['n_features'],\n",
    "        'Train F1': result['metrics']['train']['f1_score'],\n",
    "        'Val F1': result['metrics']['val']['f1_score'],\n",
    "        'Test F1': result['metrics']['test']['f1_score'],\n",
    "        'Val PR-AUC': result['metrics']['val']['pr_auc'],\n",
    "        'Test PR-AUC': result['metrics']['test']['pr_auc'],\n",
    "        'Val ROC-AUC': result['metrics']['val']['roc_auc'],\n",
    "        'Test ROC-AUC': result['metrics']['test']['roc_auc']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize performance comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# F1-Score comparison\n",
    "axes[0, 0].plot(comparison_df['N Features'], comparison_df['Train F1'], 'o-', label='Train', linewidth=2)\n",
    "axes[0, 0].plot(comparison_df['N Features'], comparison_df['Val F1'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 0].plot(comparison_df['N Features'], comparison_df['Test F1'], '^-', label='Test', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Number of Features')\n",
    "axes[0, 0].set_ylabel('F1-Score')\n",
    "axes[0, 0].set_title('F1-Score vs Number of Features')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# PR-AUC comparison\n",
    "axes[0, 1].plot(comparison_df['N Features'], comparison_df['Val PR-AUC'], 's-', label='Validation', linewidth=2)\n",
    "axes[0, 1].plot(comparison_df['N Features'], comparison_df['Test PR-AUC'], '^-', label='Test', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Number of Features')\n",
    "axes[0, 1].set_ylabel('PR-AUC')\n",
    "axes[0, 1].set_title('Precision-Recall AUC vs Number of Features')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1, 0].plot(comparison_df['N Features'], comparison_df['Val ROC-AUC'], 's-', label='Validation', linewidth=2)\n",
    "axes[1, 0].plot(comparison_df['N Features'], comparison_df['Test ROC-AUC'], '^-', label='Test', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Number of Features')\n",
    "axes[1, 0].set_ylabel('ROC-AUC')\n",
    "axes[1, 0].set_title('ROC-AUC vs Number of Features')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance for best model (highest validation F1)\n",
    "best_idx = comparison_df['Val F1'].idxmax()\n",
    "best_result = all_results[best_idx]\n",
    "importance_df_best = best_result['feature_importance'].head(15)\n",
    "\n",
    "axes[1, 1].barh(range(len(importance_df_best)), importance_df_best['importance'].values)\n",
    "axes[1, 1].set_yticks(range(len(importance_df_best)))\n",
    "axes[1, 1].set_yticklabels(importance_df_best['feature'].values)\n",
    "axes[1, 1].set_xlabel('Importance')\n",
    "axes[1, 1].set_title(f'Top 15 Features - {best_result[\"name\"]}')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# IDENTIFY BEST MODEL\n",
    "# ============================================================\n",
    "\n",
    "# Find best model based on validation F1-score\n",
    "best_idx = comparison_df['Val F1'].idxmax()\n",
    "best_result = all_results[best_idx]\n",
    "best_model = best_result['model']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BEST MODEL SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBest Model: {best_result['name']}\")\n",
    "print(f\"  Number of Features: {best_result['n_features']}\")\n",
    "print(f\"\\nValidation Performance:\")\n",
    "print(f\"  F1-Score: {best_result['metrics']['val']['f1_score']:.4f}\")\n",
    "print(f\"  PR-AUC: {best_result['metrics']['val']['pr_auc']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_result['metrics']['val']['roc_auc']:.4f}\")\n",
    "print(f\"\\nTest Performance:\")\n",
    "print(f\"  F1-Score: {best_result['metrics']['test']['f1_score']:.4f}\")\n",
    "print(f\"  PR-AUC: {best_result['metrics']['test']['pr_auc']:.4f}\")\n",
    "print(f\"  ROC-AUC: {best_result['metrics']['test']['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(best_result['feature_importance'].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE BEST MODEL AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "# Save best model\n",
    "best_model_path = MODELS_DIR / 'xgb_best_model.pkl'\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"Best model saved: {best_model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'feature_set_name': best_result['name'],\n",
    "    'features': best_result['features'],\n",
    "    'n_features': best_result['n_features'],\n",
    "    'metrics': best_result['metrics'],\n",
    "    'feature_importance': best_result['feature_importance'].to_dict('records'),\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metadata_path = MODELS_DIR / 'xgb_best_model_metadata.pkl'\n",
    "joblib.dump(model_metadata, metadata_path)\n",
    "print(f\"Model metadata saved: {metadata_path}\")\n",
    "\n",
    "# Save all results for comparison\n",
    "all_results_path = RESULTS_DIR / 'iterative_feature_addition_results.pkl'\n",
    "joblib.dump(all_results, all_results_path)\n",
    "print(f\"All results saved: {all_results_path}\")\n",
    "\n",
    "# Save comparison DataFrame\n",
    "comparison_df_path = RESULTS_DIR / 'model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_df_path, index=False)\n",
    "print(f\"Comparison DataFrame saved: {comparison_df_path}\")\n",
    "\n",
    "print(\"\\n✓ Model saving complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
