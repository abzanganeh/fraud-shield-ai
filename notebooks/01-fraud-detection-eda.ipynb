{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Notebook 1** Fraud Detection EDA (Timezone-Aware)\n",
        "**Project:** Fraud Shield AI  \n",
        "**Notebook:** 01_fraud_detection_eda.ipynb  \n",
        "**Objective:** Exploratory Data Analysis & Feature Validation (No Modeling)\n",
        "\n",
        "## ðŸ“‹ Project Information\n",
        "\n",
        "| **Attribute** | **Details** |\n",
        "| :--- | :--- |\n",
        "| **Author** | Alireza Barzin Zanganeh |\n",
        "| **Contact** | abarzinzanganeh@gmail.com |\n",
        "| **Date** | January 18, 2026 |\n",
        "| **Project Type** | Capstone Project|\n",
        "\n",
        "**Data Source Attribution:**  \n",
        "ZIP code and timezone data provided by [SimpleMaps US ZIP Code Database](https://simplemaps.com/data/us-zips). Free version used for EDA.  \n",
        "Use in production requires linking back as per their license, which we comply with.\n",
        "\n",
        "## Problem Statement\n",
        "The objective is to design and implement a comprehensive fraud detection system capable of identifying fraudulent transactions with high accuracy and scalability. The system will leverage:\n",
        "\n",
        "Supervised learning to identify known fraud patterns from labeled historical data.\n",
        "Deep learning to model complex relationships and sequential transaction behaviors.\n",
        "The solution will focus on minimizing false positives, maximizing fraud recall, and maintaining scalability for high-volume real-time transaction processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Scope & Constraints\n",
        "\n",
        "This notebook is **EDA-only** and focuses on:\n",
        "- Dataset understanding\n",
        "- Assumption validation\n",
        "- Pattern discovery\n",
        "- Feature design guidance\n",
        "\n",
        "### Explicitly Out of Scope\n",
        "- Model training\n",
        "- Feature scaling\n",
        "- Hyperparameter tuning\n",
        "- Evaluation metrics\n",
        "\n",
        "All modeling work is deferred to later notebooks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Critical Time Assumptions\n",
        "\n",
        "Transaction timestamps are **not guaranteed to be in local time**.\n",
        "\n",
        "Any analysis involving:\n",
        "- Hour of day\n",
        "- Day of week\n",
        "- Night vs daytime behavior\n",
        "\n",
        "is considered **provisional** until timestamps are converted to **local transaction time**\n",
        "using latitude/longitudeâ€“based timezone inference.\n",
        "\n",
        "Timezone correction is treated as a first-class EDA requirement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis Roadmap\n",
        "\n",
        "This notebook follows a strict, sequential EDA structure:\n",
        "\n",
        "1. Environment & Dataset Loading  \n",
        "2. Data Quality Assessment  \n",
        "3. Target Variable (Fraud) Overview  \n",
        "4. Temporal Analysis (Timezone-Aware)  \n",
        "5. Calendar-Level Patterns (Month, Seasonality)  \n",
        "6. Key Insights & Next Steps\n",
        "\n",
        "Each section builds on validated assumptions from the previous one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Environment & Dataset Inspection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Global Imports & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.441584Z",
          "iopub.status.busy": "2026-01-24T00:43:26.441295Z",
          "iopub.status.idle": "2026-01-24T00:43:26.447583Z",
          "shell.execute_reply": "2026-01-24T00:43:26.446768Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.441566Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GLOBAL IMPORTS & DEPENDENCIES\n",
        "# ============================================================\n",
        "# All imports consolidated here for clarity and maintainability\n",
        "\n",
        "# Standard Library\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from typing import Tuple, Dict, List\n",
        "from pathlib import Path\n",
        "\n",
        "# PySpark Core\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql.functions import (\n",
        "    col, count, sum as spark_sum, avg, min as spark_min, max as spark_max,\n",
        "    sqrt, sin, cos, radians, atan2,\n",
        "    round, round as spark_round, floor, ceil, trim, when, lit, concat, concat_ws,\n",
        "    hour, dayofweek, dayofmonth, month, year, date_format,\n",
        "    from_utc_timestamp, to_timestamp, unix_timestamp,\n",
        "    broadcast, first, last, collect_list, collect_set,\n",
        "    percentile_approx, stddev, variance, corr\n",
        ")\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Data Analysis & Visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress warnings for clean output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All dependencies loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.450286Z",
          "iopub.status.busy": "2026-01-24T00:43:26.449518Z",
          "iopub.status.idle": "2026-01-24T00:43:26.473953Z",
          "shell.execute_reply": "2026-01-24T00:43:26.473238Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.450265Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Imports are now consolidated in the Global Imports & Dependencies section above\n",
        "print(\"âœ“ Using global imports\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Checkpoint Configuration\n",
        "\n",
        "### Purpose\n",
        "Define checkpoint paths and directories for saving/loading intermediate DataFrames.\n",
        "This enables running sections independently and avoiding recomputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.47512Z",
          "iopub.status.busy": "2026-01-24T00:43:26.474851Z",
          "iopub.status.idle": "2026-01-24T00:43:26.497738Z",
          "shell.execute_reply": "2026-01-24T00:43:26.496566Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.475101Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CHECKPOINT CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Kaggle working directory (persists across sessions)\n",
        "CHECKPOINT_BASE_DIR = \"/kaggle/working/data/checkpoints\"\n",
        "\n",
        "# Alternative (relative path - also works in Kaggle):\n",
        "# CHECKPOINT_BASE_DIR = \"data/checkpoints\"\n",
        "\n",
        "# Section-level checkpoints (major milestones - explicit constants)\n",
        "CHECKPOINT_SECTION3 = f\"{CHECKPOINT_BASE_DIR}/section3_base_train.parquet\"\n",
        "CHECKPOINT_SECTION5 = f\"{CHECKPOINT_BASE_DIR}/section5_amount_features.parquet\"\n",
        "CHECKPOINT_SECTION6 = f\"{CHECKPOINT_BASE_DIR}/section6_timezone_resolved.parquet\"\n",
        "CHECKPOINT_SECTION7 = f\"{CHECKPOINT_BASE_DIR}/section7_temporal_features.parquet\"\n",
        "CHECKPOINT_SECTION8 = f\"{CHECKPOINT_BASE_DIR}/section8_geographic_features.parquet\"\n",
        "CHECKPOINT_SECTION9 = f\"{CHECKPOINT_BASE_DIR}/section9_demographics_features.parquet\"\n",
        "CHECKPOINT_SECTION10 = f\"{CHECKPOINT_BASE_DIR}/section10_credit_card_features.parquet\"\n",
        "CHECKPOINT_SECTION11 = f\"{CHECKPOINT_BASE_DIR}/section11_enriched_features.parquet\"\n",
        "\n",
        "# Create checkpoint directory if it doesn't exist\n",
        "os.makedirs(CHECKPOINT_BASE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Checkpoint directory: {CHECKPOINT_BASE_DIR}\")\n",
        "print(f\"âœ“ Checkpoint paths configured for Sections 3, 5, 6, 7, 8, 9, 10, 11\")\n",
        "print(f\"âœ“ Using Kaggle working directory - checkpoints will persist across sessions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Kaggle Environment & Input Validation\n",
        "\n",
        "### Purpose\n",
        "Verify the Kaggle execution environment and confirm the availability and structure\n",
        "of input datasets **before initializing Spark**.\n",
        "\n",
        "### Why This Matters\n",
        "- Confirms datasets are correctly mounted\n",
        "- Avoids hard-coded paths\n",
        "- Ensures Spark reads from the correct input directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.499934Z",
          "iopub.status.busy": "2026-01-24T00:43:26.499646Z",
          "iopub.status.idle": "2026-01-24T00:43:26.524534Z",
          "shell.execute_reply": "2026-01-24T00:43:26.523587Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.499915Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Clear Spark cache if needed (uncomment to use)\n",
        "# spark.catalog.clearCache()\n",
        "\n",
        "print(\"Spark session ready - cache cleared if needed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.525989Z",
          "iopub.status.busy": "2026-01-24T00:43:26.52573Z",
          "iopub.status.idle": "2026-01-24T00:43:26.547663Z",
          "shell.execute_reply": "2026-01-24T00:43:26.546783Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.525971Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Environment & dataset inspection\n",
        "\n",
        "INPUT_DIR = \"/kaggle/input\"\n",
        "\n",
        "print(\"Listing all files under /kaggle/input:\\n\")\n",
        "\n",
        "for dirname, _, filenames in os.walk(INPUT_DIR):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Checkpoint Verification (Optional)\n",
        "\n",
        "Run this at the start of a session to confirm your saved Parquet checkpoints exist and see their sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.549209Z",
          "iopub.status.busy": "2026-01-24T00:43:26.54891Z",
          "iopub.status.idle": "2026-01-24T00:43:26.569495Z",
          "shell.execute_reply": "2026-01-24T00:43:26.568453Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.54915Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VERIFY CHECKPOINTS EXIST\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CHECKPOINT VERIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "checkpoints = [\n",
        "    (\"Section 3\", CHECKPOINT_SECTION3),\n",
        "    (\"Section 5\", CHECKPOINT_SECTION5),\n",
        "    (\"Section 6\", CHECKPOINT_SECTION6),\n",
        "    (\"Section 7\", CHECKPOINT_SECTION7),\n",
        "    (\"Section 8\", CHECKPOINT_SECTION8),\n",
        "    (\"Section 9\", CHECKPOINT_SECTION9),\n",
        "    (\"Section 10\", CHECKPOINT_SECTION10),\n",
        "    (\"Section 11\", CHECKPOINT_SECTION11)\n",
        "]\n",
        "\n",
        "for name, path in checkpoints:\n",
        "    exists = os.path.exists(path)\n",
        "    status = \"âœ“\" if exists else \"âœ—\"\n",
        "    size = \"\"\n",
        "    if exists:\n",
        "        # Get file size (parquet files are single files, not directories)\n",
        "        if os.path.isfile(path):\n",
        "            total_size = os.path.getsize(path)\n",
        "        else:\n",
        "            # If it's a directory (shouldn't happen for parquet, but handle it)\n",
        "            total_size = sum(\n",
        "                os.path.getsize(os.path.join(dirpath, filename))\n",
        "                for dirpath, dirnames, filenames in os.walk(path)\n",
        "                for filename in filenames\n",
        "            )\n",
        "        \n",
        "        size_mb = total_size / (1024 * 1024)\n",
        "        size = f\" ({size_mb:.1f} MB)\"\n",
        "    print(f\"{status} {name}: {path}{size}\")\n",
        "\n",
        "\n",
        "results_dir = Path(CHECKPOINT_BASE_DIR) / \"results\"\n",
        "if results_dir.exists():\n",
        "    result_files = list(results_dir.glob(\"*\"))\n",
        "    print(f\"âœ“ EDA results directory: {len(result_files)} saved results\")\n",
        "else:\n",
        "    print(\"âš ï¸  EDA results directory not found\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Checkpoints in /kaggle/working/ will persist across Kaggle sessions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Spark Session Initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Spark Session Setup\n",
        "\n",
        "### Purpose\n",
        "Initialize a **SparkSession** to enable distributed data processing and replace\n",
        "pandas-based workflows with Spark DataFrames.\n",
        "\n",
        "### Why This Matters\n",
        "- Enables scalable EDA and feature engineering\n",
        "- Required for Spark `read`, `groupBy`, and ML operations\n",
        "- Establishes a single entry point for Spark APIs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T01:06:43.658527Z",
          "iopub.status.busy": "2026-01-24T01:06:43.657534Z",
          "iopub.status.idle": "2026-01-24T01:06:43.677825Z",
          "shell.execute_reply": "2026-01-24T01:06:43.677147Z",
          "shell.execute_reply.started": "2026-01-24T01:06:43.658479Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"FraudShieldAI\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.driver.memory\", \"16g\")\n",
        "    .config(\"spark.driver.maxResultSize\", \"4g\")\n",
        "    .config(\"spark.executor.memory\", \"16g\")\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "    .config(\"spark.default.parallelism\", \"4\")\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "    .getOrCreate()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Checkpoint Helper Functions\n",
        "\n",
        "### Purpose\n",
        "Reusable functions for loading and saving checkpoints with validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.603817Z",
          "iopub.status.busy": "2026-01-24T00:43:26.603538Z",
          "iopub.status.idle": "2026-01-24T00:43:26.622576Z",
          "shell.execute_reply": "2026-01-24T00:43:26.621288Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.603773Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CHECKPOINT MANAGER CLASS \n",
        "# ============================================================\n",
        "\n",
        "from typing import Optional, List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Manages Spark DataFrame checkpoints with validation and automatic save/load.\n",
        "    \n",
        "    Design Pattern: Singleton-like (one instance per notebook)\n",
        "    Responsibility: Checkpoint operations only\n",
        "    Thread Safety: Spark operations are thread-safe\n",
        "    \n",
        "    Usage:\n",
        "        manager = CheckpointManager(spark, CHECKPOINT_BASE_DIR)\n",
        "        train_df = manager.load_or_compute(\n",
        "            checkpoint_path=\"data/checkpoints/cell_6_2_1_1.parquet\",\n",
        "            compute_func=lambda: compute_customer_timezone(train_df),\n",
        "            required_columns=[\"customer_timezone\"],\n",
        "            cell_name=\"Section 6.2.1.1\"\n",
        "        )\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, spark_session, base_dir: str = \"data/checkpoints\"):\n",
        "        \"\"\"\n",
        "        Initialize checkpoint manager.\n",
        "        \n",
        "        Args:\n",
        "            spark_session: Active SparkSession\n",
        "            base_dir: Base directory for all checkpoints\n",
        "        \"\"\"\n",
        "        self.spark = spark_session\n",
        "        self.base_dir = Path(base_dir)\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Statistics tracking\n",
        "        self.stats = {\n",
        "            'loaded': 0,\n",
        "            'computed': 0,\n",
        "            'saved': 0\n",
        "        }\n",
        "    \n",
        "    def checkpoint_exists(self, checkpoint_path: str) -> bool:\n",
        "        \"\"\"Check if checkpoint file/directory exists.\"\"\"\n",
        "        return os.path.exists(checkpoint_path)\n",
        "    \n",
        "    def load_checkpoint(\n",
        "        self, \n",
        "        checkpoint_path: str, \n",
        "        required_columns: Optional[List[str]] = None,\n",
        "        cell_name: str = \"\"\n",
        "    ) -> Tuple[Optional[DataFrame], bool]:\n",
        "        \"\"\"\n",
        "        Load checkpoint if it exists and validate columns.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to Parquet checkpoint\n",
        "            required_columns: Columns that must exist (None = no validation)\n",
        "            cell_name: Name for logging\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (dataframe, loaded_successfully)\n",
        "        \"\"\"\n",
        "        if not self.checkpoint_exists(checkpoint_path):\n",
        "            if cell_name:\n",
        "                print(f\"âš ï¸  Checkpoint not found: {checkpoint_path}\")\n",
        "                print(f\"  Will compute {cell_name}...\")\n",
        "            return None, False\n",
        "        \n",
        "        try:\n",
        "            print(f\"âœ“ Checkpoint found for {cell_name}\")\n",
        "            print(f\"  Loading from: {checkpoint_path}\")\n",
        "            \n",
        "            df = self.spark.read.parquet(checkpoint_path)\n",
        "            row_count = df.count()\n",
        "            col_count = len(df.columns)\n",
        "            print(f\"  Loaded {row_count:,} rows, {col_count} columns\")\n",
        "            \n",
        "            # Validate required columns\n",
        "            if required_columns:\n",
        "                missing_cols = [\n",
        "                    col for col in required_columns \n",
        "                    if col not in df.columns\n",
        "                ]\n",
        "                if missing_cols:\n",
        "                    print(f\"  âš ï¸  Missing required columns: {missing_cols}\")\n",
        "                    print(f\"  Recomputing {cell_name}...\")\n",
        "                    return None, False\n",
        "                else:\n",
        "                    print(f\"  âœ“ All required columns present: {required_columns}\")\n",
        "            \n",
        "            self.stats['loaded'] += 1\n",
        "            print(f\"  â­ï¸  Skipping computation for {cell_name}\\n\")\n",
        "            return df, True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error loading checkpoint: {e}\")\n",
        "            print(f\"  Recomputing {cell_name}...\")\n",
        "            return None, False\n",
        "    \n",
        "    def save_checkpoint(\n",
        "        self, \n",
        "        df: DataFrame, \n",
        "        checkpoint_path: str, \n",
        "        cell_name: str = \"\"\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Save DataFrame to Parquet checkpoint.\n",
        "        \n",
        "        Args:\n",
        "            df: Spark DataFrame to save\n",
        "            checkpoint_path: Path to save checkpoint\n",
        "            cell_name: Name for logging\n",
        "        \n",
        "        Returns:\n",
        "            bool: True if saved successfully\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(f\"\\nðŸ’¾ Saving checkpoint for {cell_name}...\")\n",
        "            print(f\"  Path: {checkpoint_path}\")\n",
        "            \n",
        "            # Get statistics before saving\n",
        "            row_count = df.count()\n",
        "            col_count = len(df.columns)\n",
        "            \n",
        "            # Save to Parquet (overwrite mode)\n",
        "            df.write.mode(\"overwrite\").parquet(checkpoint_path)\n",
        "            \n",
        "            # Calculate file size\n",
        "            if os.path.exists(checkpoint_path):\n",
        "                total_size = sum(\n",
        "                    os.path.getsize(os.path.join(dirpath, filename))\n",
        "                    for dirpath, dirnames, filenames in os.walk(checkpoint_path)\n",
        "                    for filename in filenames\n",
        "                )\n",
        "                size_mb = total_size / (1024 * 1024)\n",
        "                print(f\"  âœ“ Saved {row_count:,} rows, {col_count} columns ({size_mb:.2f} MB)\")\n",
        "            else:\n",
        "                print(f\"  âœ“ Saved {row_count:,} rows, {col_count} columns\")\n",
        "            \n",
        "            self.stats['saved'] += 1\n",
        "            print(f\"âœ“ Checkpoint saved successfully\\n\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error saving checkpoint: {e}\")\n",
        "            return False\n",
        "    \n",
        "    def load_or_compute(\n",
        "        self,\n",
        "        checkpoint_path: str,\n",
        "        compute_func,\n",
        "        required_columns: Optional[List[str]] = None,\n",
        "        cell_name: str = \"\",\n",
        "        force_recompute: bool = False\n",
        "    ) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Load checkpoint if exists, otherwise compute and save.\n",
        "        \n",
        "        This is the main API method - use this in cells.\n",
        "        \n",
        "        Args:\n",
        "            checkpoint_path: Path to checkpoint\n",
        "            compute_func: Function that returns DataFrame (lambda or function)\n",
        "            required_columns: Columns that must exist after computation\n",
        "            cell_name: Name for logging\n",
        "            force_recompute: If True, ignore checkpoint and recompute\n",
        "        \n",
        "        Returns:\n",
        "            DataFrame: Either loaded from checkpoint or computed\n",
        "        \n",
        "        Example:\n",
        "            train_df = manager.load_or_compute(\n",
        "                checkpoint_path=f\"{CHECKPOINT_BASE_DIR}/cell_6_2_1_1.parquet\",\n",
        "                compute_func=lambda: add_customer_timezone(train_df),\n",
        "                required_columns=[\"customer_timezone\"],\n",
        "                cell_name=\"Section 6.2.1.1\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Try to load if not forcing recompute\n",
        "        if not force_recompute:\n",
        "            df_loaded, loaded = self.load_checkpoint(\n",
        "                checkpoint_path, \n",
        "                required_columns, \n",
        "                cell_name\n",
        "            )\n",
        "            if loaded:\n",
        "                return df_loaded\n",
        "        \n",
        "        # Need to compute\n",
        "        print(f\"ðŸ”„ Computing {cell_name}...\")\n",
        "        print(f\"  (Checkpoint will be saved to: {checkpoint_path})\")\n",
        "        \n",
        "        try:\n",
        "            # Execute computation function\n",
        "            df_computed = compute_func()\n",
        "            \n",
        "            # Validate required columns if specified\n",
        "            if required_columns:\n",
        "                missing_cols = [\n",
        "                    col for col in required_columns \n",
        "                    if col not in df_computed.columns\n",
        "                ]\n",
        "                if missing_cols:\n",
        "                    raise ValueError(\n",
        "                        f\"Computation failed: Missing required columns {missing_cols}\"\n",
        "                    )\n",
        "            \n",
        "            # Save checkpoint\n",
        "            self.save_checkpoint(df_computed, checkpoint_path, cell_name)\n",
        "            self.stats['computed'] += 1\n",
        "            \n",
        "            return df_computed\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error during computation: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def get_cell_checkpoint_path(self, section: str, subsection: str = \"\", cell_id: str = \"\") -> str:\n",
        "        \"\"\"\n",
        "        Generate standardized cell-level checkpoint path.\n",
        "        \n",
        "        Args:\n",
        "            section: Section number (e.g., \"6\", \"7\", \"8\")\n",
        "            subsection: Subsection identifier (e.g., \"2_1_1\")\n",
        "            cell_id: Optional cell identifier (e.g., \"customer_timezone\")\n",
        "        \n",
        "        Returns:\n",
        "            str: Full checkpoint path\n",
        "        \n",
        "        Examples:\n",
        "            manager.get_cell_checkpoint_path(\"6\", \"2_1_1\", \"customer_timezone\")\n",
        "            # Returns: \"data/checkpoints/cell_6_2_1_1_customer_timezone.parquet\"\n",
        "        \"\"\"\n",
        "        parts = [\"cell\"]\n",
        "        \n",
        "        if section:\n",
        "            parts.append(section)\n",
        "        if subsection:\n",
        "            parts.append(subsection)\n",
        "        if cell_id:\n",
        "            parts.append(cell_id)\n",
        "        \n",
        "        filename = \"_\".join(parts) + \".parquet\"\n",
        "        return str(self.base_dir / filename)\n",
        "    \n",
        "    def get_stats(self) -> dict:\n",
        "        \"\"\"Get checkpoint statistics.\"\"\"\n",
        "        return self.stats.copy()\n",
        "    \n",
        "    def print_stats(self):\n",
        "        \"\"\"Print checkpoint statistics summary.\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"CHECKPOINT STATISTICS\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"Checkpoints loaded:  {self.stats['loaded']}\")\n",
        "        print(f\"Checkpoints computed: {self.stats['computed']}\")\n",
        "        print(f\"Checkpoints saved:    {self.stats['saved']}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# Create global instance (singleton pattern for notebook)\n",
        "# NOTE: This must be AFTER Spark session is initialized (Section 2.1)\n",
        "checkpoint_manager = CheckpointManager(spark, CHECKPOINT_BASE_DIR)\n",
        "\n",
        "print(\"CheckpointManager class loaded\")\n",
        "print(f\"Checkpoint base directory: {CHECKPOINT_BASE_DIR}\")\n",
        "print(\"Ready to use: checkpoint_manager.load_or_compute(...)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# RESULTS MANAGER\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "class ResultsManager:\n",
        "    \"\"\"\n",
        "    Saves and loads EDA analysis results (DataFrames, statistics) to disk.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_dir: str) -> None:\n",
        "        self.base_dir = Path(base_dir) / \"results\"\n",
        "        self.base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def save_dataframe(self, df: \"pd.DataFrame\", section: str, result_name: str) -> str:\n",
        "        path = self.base_dir / f\"{section}_{result_name}.parquet\"\n",
        "        df.to_parquet(path, index=False)\n",
        "        return str(path)\n",
        "\n",
        "    def load_dataframe(self, section: str, result_name: str) -> Optional[\"pd.DataFrame\"]:\n",
        "        path = self.base_dir / f\"{section}_{result_name}.parquet\"\n",
        "        if path.exists():\n",
        "            return pd.read_parquet(path)\n",
        "        return None\n",
        "\n",
        "    def save_statistics(self, stats: dict, section: str, result_name: str) -> str:\n",
        "        path = self.base_dir / f\"{section}_{result_name}.json\"\n",
        "        with open(path, \"w\") as f:\n",
        "            json.dump(stats, f, indent=2, default=str)\n",
        "        return str(path)\n",
        "\n",
        "    def load_statistics(self, section: str, result_name: str) -> Optional[dict]:\n",
        "        path = self.base_dir / f\"{section}_{result_name}.json\"\n",
        "        if path.exists():\n",
        "            with open(path, \"r\") as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "\n",
        "    def result_exists(self, section: str, result_name: str, kind: str = \"parquet\") -> bool:\n",
        "        ext = \"json\" if kind == \"json\" else \"parquet\"\n",
        "        return (self.base_dir / f\"{section}_{result_name}.{ext}\").exists()\n",
        "\n",
        "\n",
        "results_manager = ResultsManager(CHECKPOINT_BASE_DIR)\n",
        "print(\"âœ“ ResultsManager class loaded\")\n",
        "print(f\"âœ“ Results directory: {results_manager.base_dir}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATA VALIDATION HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def validate_column_coverage(df, column_name, analysis_name):\n",
        "    \"\"\"\n",
        "    Validate column data coverage before analysis.\n",
        "\n",
        "    Args:\n",
        "        df: PySpark DataFrame\n",
        "        column_name: Name of column to validate\n",
        "        analysis_name: Description for logging\n",
        "\n",
        "    Returns:\n",
        "        tuple: (filtered_df, coverage_pct, valid_count, total_count)\n",
        "    \"\"\"\n",
        "    total_count = df.count()\n",
        "    filtered_df = df.filter(col(column_name).isNotNull())\n",
        "    valid_count = filtered_df.count()\n",
        "    coverage_pct = (valid_count / total_count) * 100\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"DATA VALIDATION: {analysis_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total rows:      {total_count:,}\")\n",
        "    print(f\"Valid rows:      {valid_count:,}\")\n",
        "    print(f\"Coverage:        {coverage_pct:.2f}%\")\n",
        "    print(f\"Excluded (NULL): {total_count - valid_count:,}\")\n",
        "\n",
        "    if coverage_pct < 95:\n",
        "        print(f\"\\nWARNING: Only {coverage_pct:.2f}% coverage - results may be biased\")\n",
        "    else:\n",
        "        print(f\"\\nCoverage acceptable ({coverage_pct:.2f}%)\")\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    return filtered_df, coverage_pct, valid_count, total_count\n",
        "\n",
        "\n",
        "def aggregate_fraud_by_dimension(df, dimension_col, dimension_name, cache_name=None,\n",
        "        save_result: bool = False, section: Optional[str] = None, result_name: Optional[str] = None):\n",
        "    \"\"\"\n",
        "    Aggregate fraud statistics by a single dimension.\n",
        "\n",
        "    Args:\n",
        "        df: PySpark DataFrame\n",
        "        dimension_col: Column to group by (e.g., \"hour\", \"day_of_week\")\n",
        "        dimension_name: Human-readable name for logging\n",
        "        cache_name: Optional cache key for idempotency\n",
        "        save_result: If True, save result via ResultsManager\n",
        "        section: Section id for save (e.g. \"7.5\")\n",
        "        result_name: Name for saved result file\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame with fraud statistics\n",
        "    \"\"\"\n",
        "\n",
        "    if cache_name and cache_name in globals() and globals()[cache_name] is not None:\n",
        "        print(f\"Using cached {dimension_name} analysis\")\n",
        "        return globals()[cache_name]\n",
        "\n",
        "    print(f\"Aggregating fraud by {dimension_name}...\")\n",
        "\n",
        "    agg_result = (\n",
        "        df\n",
        "        .groupBy(dimension_col)\n",
        "        .agg(\n",
        "            count(\"*\").alias(\"total_txns\"),\n",
        "            spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"fraud_rate_pct\",\n",
        "            spark_round((col(\"fraud_count\") / col(\"total_txns\")) * 100, 4)\n",
        "        )\n",
        "        .orderBy(dimension_col)\n",
        "    )\n",
        "\n",
        "    stats_df = agg_result.toPandas()\n",
        "    stats_df[\"legit_count\"] = stats_df[\"total_txns\"] - stats_df[\"fraud_count\"]\n",
        "\n",
        "    if cache_name:\n",
        "        globals()[cache_name] = stats_df\n",
        "\n",
        "    if save_result and section and result_name and \"results_manager\" in globals():\n",
        "        results_manager.save_dataframe(stats_df, section, result_name)\n",
        "        print(f\"Saved {dimension_name} results to disk\")\n",
        "\n",
        "    print(f\"{dimension_name} analysis complete ({len(stats_df)} groups)\")\n",
        "\n",
        "    return stats_df\n",
        "\n",
        "\n",
        "print(\"âœ“ Data validation and aggregation helper functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Data Loading & Structural Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.1 Load Training Dataset (Spark)\n",
        "\n",
        "### Purpose\n",
        "Load the training dataset into a **Spark DataFrame** and inspect its basic structure.\n",
        "**Includes checkpointing** - if checkpoint exists, loads from it; otherwise loads from CSV and saves checkpoint.\n",
        "\n",
        "### What We Validate\n",
        "- Number of rows\n",
        "- Number of columns\n",
        "- Column names and schema\n",
        "- Sample records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:26.623996Z",
          "iopub.status.busy": "2026-01-24T00:43:26.623721Z",
          "iopub.status.idle": "2026-01-24T00:43:27.267676Z",
          "shell.execute_reply": "2026-01-24T00:43:27.266872Z",
          "shell.execute_reply.started": "2026-01-24T00:43:26.623976Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD TRAINING DATASET WITH VALIDATION & CHECKPOINTING\n",
        "# ============================================================\n",
        "\n",
        "def load_training_dataset():\n",
        "    \"\"\"\n",
        "    Load training dataset from CSV.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame: Loaded and validated training dataset\n",
        "    \"\"\"\n",
        "    # Construct path\n",
        "    dataset_folders = os.listdir(INPUT_DIR)\n",
        "    dataset_path = sorted(os.listdir(INPUT_DIR))[0]\n",
        "    full_path = os.path.join(INPUT_DIR, 'fraud-detection', \"fraudTrain.csv\")\n",
        "    \n",
        "    # Validate path exists\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Training data not found: {full_path}\")\n",
        "    \n",
        "    print(f\"Dataset folder: {dataset_path}\")\n",
        "    print(f\"Loading from: {full_path}\")\n",
        "    \n",
        "    # Load with Spark\n",
        "    df = spark.read.csv(full_path, header=True, inferSchema=True)\n",
        "    \n",
        "    # Drop index column if exists\n",
        "    if \"_c0\" in df.columns:\n",
        "        df = df.drop(\"_c0\")\n",
        "    \n",
        "    # Basic validation\n",
        "    num_rows = df.count()\n",
        "    num_cols = len(df.columns)\n",
        "    \n",
        "    print(f\"\\nâœ“ Loaded successfully\")\n",
        "    print(f\"Shape: ({num_rows:,} rows, {num_cols} columns)\")\n",
        "    \n",
        "    # Show sample\n",
        "    print(\"\\nSample records:\")\n",
        "    df.show(3, truncate=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager to load or compute\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=CHECKPOINT_SECTION3,\n",
        "    compute_func=load_training_dataset,\n",
        "    required_columns=None,  # Base dataset - no specific column requirements\n",
        "    cell_name=\"Section 3.1 (Base Dataset Loading)\"\n",
        ")\n",
        "\n",
        "# Set global constant (always needed, regardless of checkpoint)\n",
        "TOTAL_DATASET_ROWS = train_df.count()\n",
        "print(f\"âœ“ Global constant TOTAL_DATASET_ROWS = {TOTAL_DATASET_ROWS:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Data Quality & Target Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1 Fraud Statistics\n",
        "\n",
        "### Purpose\n",
        "Compute basic fraud statistics from the training dataset using **Spark**:\n",
        "- Total transactions  \n",
        "- Fraudulent transactions count  \n",
        "- Legitimate transactions count  \n",
        "- Fraud rate (%)\n",
        "\n",
        "### Notes\n",
        "- `.count()` replaces `len()` in Spark\n",
        "- `sum()` can be applied with `agg` or `select` + `sum`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:27.268842Z",
          "iopub.status.busy": "2026-01-24T00:43:27.26858Z",
          "iopub.status.idle": "2026-01-24T00:43:27.701324Z",
          "shell.execute_reply": "2026-01-24T00:43:27.70047Z",
          "shell.execute_reply.started": "2026-01-24T00:43:27.268816Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Total transactions (use global constant to avoid recomputation)\n",
        "total = TOTAL_DATASET_ROWS\n",
        "\n",
        "# Fraudulent transactions count\n",
        "fraud_count = train_df.agg(spark_sum(col(\"is_fraud\"))).collect()[0][0]\n",
        "\n",
        "# Legitimate transactions\n",
        "legit_count = total - fraud_count\n",
        "\n",
        "# Fraud rate in percentage\n",
        "fraud_rate = (fraud_count / total) * 100\n",
        "\n",
        "# Print results\n",
        "print(f\"Total transactions: {total:,}\")\n",
        "print(f\"Fraud transactions: {fraud_count:,}\")\n",
        "print(f\"Legitimate transactions: {legit_count:,}\")\n",
        "print(f\"Fraud rate: {fraud_rate:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Missing Values Analysis\n",
        "\n",
        "### Purpose\n",
        "Check for missing values in each column of the Spark DataFrame and compute the total number of missing entries.\n",
        "\n",
        "### Notes\n",
        "- In Spark, use `isNull()` combined with `agg` and `sum` for each column.\n",
        "- `.count()` can also be used to compute non-missing rows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Numeric Summary Statistics (Optional)\n",
        "\n",
        "### Purpose\n",
        "Get summary statistics (count, mean, stddev, min, max) for numeric columns in the Spark DataFrame.\n",
        "\n",
        "### Notes\n",
        "- Only computes **numeric columns** by default\n",
        "- Returns a Spark DataFrame (use `.show()` to visualize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:27.70289Z",
          "iopub.status.busy": "2026-01-24T00:43:27.702606Z",
          "iopub.status.idle": "2026-01-24T00:43:31.995281Z",
          "shell.execute_reply": "2026-01-24T00:43:31.993957Z",
          "shell.execute_reply.started": "2026-01-24T00:43:27.702865Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Compute missing values per column\n",
        "missing_df = train_df.select([\n",
        "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
        "    for c in train_df.columns\n",
        "])\n",
        "\n",
        "# Show missing values per column\n",
        "missing_df.show(truncate=False)\n",
        "\n",
        "# Total missing values\n",
        "total_missing = sum([missing_df.collect()[0][c] for c in missing_df.columns])\n",
        "print(f\"\\nTotal missing values: {total_missing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:31.996642Z",
          "iopub.status.busy": "2026-01-24T00:43:31.996414Z",
          "iopub.status.idle": "2026-01-24T00:43:32.000402Z",
          "shell.execute_reply": "2026-01-24T00:43:31.999729Z",
          "shell.execute_reply.started": "2026-01-24T00:43:31.996622Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Spark equivalent of pandas describe()\n",
        "# train_df.describe().show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Key Findings, Gaps & Amount Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.1 Key Findings\n",
        "\n",
        "## 1. Dataset Size & Structure\n",
        "\n",
        "- 1,296,675 transactions (~1.3M)\n",
        "- 23 columns (features)\n",
        "- ~1GB memory footprint\n",
        "\n",
        "This size justifies using **PySpark** for distributed processing and scalable feature engineering.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Class Imbalance Analysis (CRITICAL)\n",
        "\n",
        "| Class      | Count     | Percentage |\n",
        "|-----------|-----------|------------|\n",
        "| Fraud     | 7,506     | 0.58%      |\n",
        "| Legitimate| 1,289,169 | 99.42%     |\n",
        "\n",
        "**Imbalance ratio:** 172:1\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "- Models can predict *â€œnot fraudâ€* for everything and still get ~99.4% accuracy â†’ **useless**.\n",
        "- This is why **SMOTE** or other imbalance-handling methods are mandatory.\n",
        "- This is the **key challenge** mentioned in class.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Missing Values\n",
        "\n",
        "- Zero missing values â†’ clean dataset.\n",
        "- No need for imputation strategies.\n",
        "- Can proceed directly to **feature engineering**.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Geographic Analysis (lat/long)\n",
        "\n",
        "### Customer Location\n",
        "\n",
        "- Latitude: 20.0Â° to 66.7Â°\n",
        "- Longitude: -165.7Â° to -67.9Â° (all negative)\n",
        "\n",
        "### What this tells us\n",
        "\n",
        "- Negative longitude â†’ **Western Hemisphere**.\n",
        "- This range covers the **entire United States**:\n",
        "  - Alaska: ~64Â°N, -165Â°W (northern/western extreme)\n",
        "  - Florida: ~25Â°N, -80Â°W (southern/eastern extreme)\n",
        "  - California: ~37Â°N, -120Â°W (west coast)\n",
        "  - Maine: ~45Â°N, -68Â°W (east coast)\n",
        "\n",
        "### Cruise/Water transactions?\n",
        "\n",
        "- To check this, compare lat/long to known land coordinates.\n",
        "- If coordinates fall in the ocean (Atlantic or Pacific), they could be **cruise ship** transactions.\n",
        "- This is a strong potential fraud indicator (class mentioned cruise transactions have higher fraud).\n",
        "\n",
        "### Not in Europe/Asia because\n",
        "\n",
        "- Europe longitude: about -10Â° to 40Â° (mostly positive).\n",
        "- Asia longitude: about 60Â° to 180Â° (positive).\n",
        "- Our data: all negative longitudes (-165 to -68) â†’ **North America only**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Transaction Amount Analysis (`amt` column)\n",
        "\n",
        "### Key statistics\n",
        "\n",
        "- Min: \\$1.00 â€” small purchases (coffee, snacks)\n",
        "- Median (50%): \\$47.52 â€” typical transaction\n",
        "- Mean: \\$70.35 â€” slightly higher than median (right-skewed)\n",
        "- 75th percentile: \\$83.14 â€” most transactions under \\$100\n",
        "- Max: \\$28,948.90 â€” very large purchase\n",
        "\n",
        "### What this tells us\n",
        "\n",
        "- Distribution is **right-skewed** â€” a few very large transactions pull the mean up.\n",
        "- Most transactions are **< \\$100** â€” normal retail purchases.\n",
        "- Large amounts (e.g., **\\$900+**) are rare â€” class mentioned using bins: `>200`, `>300`, `>900` as features.\n",
        "- Fraud likely correlates with **unusual amounts** â€” need to compare fraud vs non-fraud distributions.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Credit Card Numbers (`cc_num`)\n",
        "\n",
        "- Range: 60 billion to 4.9 quintillion.\n",
        "- This is effectively an **ID**, not a numeric magnitude feature.\n",
        "- Best used for **grouping**, e.g.:\n",
        "  - *How many transactions in the last hour for this card?*\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Time Analysis (`unix_time`)\n",
        "\n",
        "- Range: 1,325,376,018 to 1,371,817,018.\n",
        "- Converts to approximately: **January 1, 2012 â†’ June 21, 2013** (~18 months).\n",
        "\n",
        "### Implications\n",
        "\n",
        "- Over a year of data:\n",
        "  - Can detect **seasonal patterns**.\n",
        "  - Can analyze **holidays**.\n",
        "  - Can study **weekly/daily** patterns.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. City Population (`city_pop`)\n",
        "\n",
        "- Min: 23 â€” very small towns.\n",
        "- Median: 2,456 â€” small cities.\n",
        "- Max: 2,906,700 â€” major cities (e.g., Chicago ~2.7M).\n",
        "\n",
        "### Possible fraud signal\n",
        "\n",
        "- Fraud rates may differ between **small towns** and **large cities**.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Merchant Location (`merch_lat`, `merch_long`)\n",
        "\n",
        "- Similar range to customer location.\n",
        "\n",
        "### Critical feature\n",
        "\n",
        "- **Distance between customer and merchant**:\n",
        "  - Example: customer in NYC, merchant in LA â†’ potentially suspicious.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 What's Left (Need to Explore)\n",
        "\n",
        "### 1. Temporal Patterns\n",
        "\n",
        "- What hours have most fraud? (night vs day)\n",
        "- What days? (weekend vs weekday)\n",
        "- Holidays?\n",
        "\n",
        "### 2. Category Analysis\n",
        "\n",
        "- What are the categories? (e.g., `gas_transport`, `grocery_pos`, etc.)\n",
        "- Which categories have the **highest fraud rate**?\n",
        "\n",
        "### 3. Amount Distribution\n",
        "\n",
        "- How do **fraud amounts** compare to **legitimate** amounts?\n",
        "- Are frauds typically **small**, **medium**, or **large**?\n",
        "\n",
        "### 4. Geographic Patterns\n",
        "\n",
        "- Which **states** have the most fraud?\n",
        "- Are there **fraud hotspots**?\n",
        "- How does **distance between customer and merchant** differ for fraud vs legitimate?\n",
        "\n",
        "### 5. Merchant Analysis\n",
        "\n",
        "- How many **unique merchants**?\n",
        "- Do some merchants have **more fraud** than others?\n",
        "- Merchant name patterns? (e.g., `fraud_` prefix in sample data)\n",
        "\n",
        "### 6. Customer Demographics\n",
        "\n",
        "- Gender distribution in fraud.\n",
        "- Age (from DOB) correlation with fraud.\n",
        "- Job types with higher fraud incidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Amount Distribution Analysis\n",
        "\n",
        "### Overview\n",
        "\n",
        "This section compares transaction amounts between fraud and legitimate transactions to identify\n",
        "patterns in fraudulent transaction sizes.\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Fraud amounts may differ from legitimate amounts\n",
        "- **Reasoning:** Fraudsters may target specific amount ranges (small to avoid detection, or large for maximum gain)\n",
        "\n",
        "### What We'll Analyze\n",
        "1. Distribution statistics for fraud vs legitimate amounts\n",
        "2. Amount bins and fraud rates by bin\n",
        "3. Comparison of mean, median, and percentiles\n",
        "4. Visualizations of amount distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.1 Data Validation - Amount Column Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:32.001946Z",
          "iopub.status.busy": "2026-01-24T00:43:32.001541Z",
          "iopub.status.idle": "2026-01-24T00:43:32.023192Z",
          "shell.execute_reply": "2026-01-24T00:43:32.021785Z",
          "shell.execute_reply.started": "2026-01-24T00:43:32.001923Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA VALIDATION: AMOUNT COLUMN COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    raise ValueError(\"train_df not found. Please run previous sections first.\")\n",
        "\n",
        "# Check if amount column exists\n",
        "if \"amt\" not in train_df.columns:\n",
        "    raise ValueError(\"Column 'amt' not found in train_df. Please check the dataset.\")\n",
        "\n",
        "# Validate amount coverage\n",
        "amount_validation = validate_column_coverage(train_df, \"amt\", \"Amount Analysis\")\n",
        "amount_df, amount_coverage, amount_valid_count, amount_total = amount_validation\n",
        "\n",
        "print(\"\\nAmount column validation complete\")\n",
        "print(f\"  Coverage: {amount_coverage:.2f}%\")\n",
        "print(f\"  Valid rows: {amount_valid_count:,} / {amount_total:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.2 Amount Statistics: Fraud vs Legitimate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:32.024784Z",
          "iopub.status.busy": "2026-01-24T00:43:32.024527Z",
          "iopub.status.idle": "2026-01-24T00:43:33.267676Z",
          "shell.execute_reply": "2026-01-24T00:43:33.266872Z",
          "shell.execute_reply.started": "2026-01-24T00:43:32.024761Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AMOUNT STATISTICS: FRAUD VS LEGITIMATE\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import mean, stddev, min as spark_min, max as spark_max, percentile_approx\n",
        "\n",
        "# Calculate statistics for fraud transactions\n",
        "fraud_stats = amount_df.filter(col(\"is_fraud\") == 1).agg(\n",
        "    mean(\"amt\").alias(\"mean_amt\"),\n",
        "    stddev(\"amt\").alias(\"stddev_amt\"),\n",
        "    spark_min(\"amt\").alias(\"min_amt\"),\n",
        "    spark_max(\"amt\").alias(\"max_amt\"),\n",
        "    percentile_approx(\"amt\", 0.25).alias(\"p25_amt\"),\n",
        "    percentile_approx(\"amt\", 0.50).alias(\"p50_amt\"),\n",
        "    percentile_approx(\"amt\", 0.75).alias(\"p75_amt\"),\n",
        "    percentile_approx(\"amt\", 0.95).alias(\"p95_amt\")\n",
        ").collect()[0]\n",
        "\n",
        "# Calculate statistics for legitimate transactions\n",
        "legit_stats = amount_df.filter(col(\"is_fraud\") == 0).agg(\n",
        "    mean(\"amt\").alias(\"mean_amt\"),\n",
        "    stddev(\"amt\").alias(\"stddev_amt\"),\n",
        "    spark_min(\"amt\").alias(\"min_amt\"),\n",
        "    spark_max(\"amt\").alias(\"max_amt\"),\n",
        "    percentile_approx(\"amt\", 0.25).alias(\"p25_amt\"),\n",
        "    percentile_approx(\"amt\", 0.50).alias(\"p50_amt\"),\n",
        "    percentile_approx(\"amt\", 0.75).alias(\"p75_amt\"),\n",
        "    percentile_approx(\"amt\", 0.95).alias(\"p95_amt\")\n",
        ").collect()[0]\n",
        "\n",
        "# Create comparison DataFrame\n",
        "import pandas as pd\n",
        "comparison_data = {\n",
        "    'Metric': ['Mean', 'Median (P50)', 'Std Dev', 'Min', 'Max', 'P25', 'P75', 'P95'],\n",
        "    'Fraud': [\n",
        "        fraud_stats['mean_amt'],\n",
        "        fraud_stats['p50_amt'],\n",
        "        fraud_stats['stddev_amt'],\n",
        "        fraud_stats['min_amt'],\n",
        "        fraud_stats['max_amt'],\n",
        "        fraud_stats['p25_amt'],\n",
        "        fraud_stats['p75_amt'],\n",
        "        fraud_stats['p95_amt']\n",
        "    ],\n",
        "    'Legitimate': [\n",
        "        legit_stats['mean_amt'],\n",
        "        legit_stats['p50_amt'],\n",
        "        legit_stats['stddev_amt'],\n",
        "        legit_stats['min_amt'],\n",
        "        legit_stats['max_amt'],\n",
        "        legit_stats['p25_amt'],\n",
        "        legit_stats['p75_amt'],\n",
        "        legit_stats['p95_amt']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df['Difference'] = comparison_df['Fraud'] - comparison_df['Legitimate']\n",
        "comparison_df['Ratio'] = comparison_df['Fraud'] / comparison_df['Legitimate']\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AMOUNT STATISTICS: FRAUD VS LEGITIMATE\")\n",
        "print(\"=\" * 80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Key insights\n",
        "mean_ratio = comparison_df[comparison_df['Metric'] == 'Mean']['Ratio'].values[0]\n",
        "median_ratio = comparison_df[comparison_df['Metric'] == 'Median (P50)']['Ratio'].values[0]\n",
        "\n",
        "print(\"\\nKEY INSIGHTS:\")\n",
        "print(f\"Mean amount ratio (Fraud/Legitimate): {mean_ratio:.2f}x\")\n",
        "print(f\"Median amount ratio (Fraud/Legitimate): {median_ratio:.2f}x\")\n",
        "if mean_ratio > 1.1:\n",
        "    print(\"  â†’ Fraud transactions tend to be LARGER than legitimate\")\n",
        "elif mean_ratio < 0.9:\n",
        "    print(\"  â†’ Fraud transactions tend to be SMALLER than legitimate\")\n",
        "else:\n",
        "    print(\"  â†’ Fraud and legitimate amounts are similar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.3 Amount Bins and Fraud Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:33.268676Z",
          "iopub.status.busy": "2026-01-24T00:43:33.268443Z",
          "iopub.status.idle": "2026-01-24T00:43:34.523192Z",
          "shell.execute_reply": "2026-01-24T00:43:34.522785Z",
          "shell.execute_reply.started": "2026-01-24T00:43:33.268661Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AMOUNT BINS AND FRAUD RATES\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "amount_bin_stats = results_manager.load_dataframe(\"5.3\", \"amount_bin_stats\") if \"results_manager\" in globals() else None\n",
        "_computed_amount_bin = amount_bin_stats is None\n",
        "if amount_bin_stats is not None:\n",
        "    print(\"âœ“ Loaded amount_bin_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "\n",
        "# Create amount bins (needed for train_df.amount_bin and for aggregation)\n",
        "amount_bin_expr = (\n",
        "    when(col(\"amt\") < 50, \"<$50\")\n",
        "    .when(col(\"amt\") < 100, \"$50-$100\")\n",
        "    .when(col(\"amt\") < 200, \"$100-$200\")\n",
        "    .when(col(\"amt\") < 300, \"$200-$300\")\n",
        "    .when(col(\"amt\") < 500, \"$300-$500\")\n",
        "    .when(col(\"amt\") < 1000, \"$500-$1000\")\n",
        "    .otherwise(\">$1000\")\n",
        ")\n",
        "\n",
        "# Add amount_bin to train_df for downstream use and checkpointing\n",
        "if \"amount_bin\" not in train_df.columns:\n",
        "    train_df = train_df.withColumn(\"amount_bin\", amount_bin_expr)\n",
        "    print(\"Added amount_bin column to train_df\")\n",
        "\n",
        "if amount_bin_stats is None:\n",
        "    amount_binned = amount_df.withColumn(\"amount_bin\", amount_bin_expr)\n",
        "    amount_bin_stats = aggregate_fraud_by_dimension(\n",
        "        amount_binned,\n",
        "        \"amount_bin\",\n",
        "        \"amount_bin\",\n",
        "        cache_name=\"amount_bin_stats\"\n",
        "    )\n",
        "    bin_order = [\"<$50\", \"$50-$100\", \"$100-$200\", \"$200-$300\", \"$300-$500\", \"$500-$1000\", \">$1000\"]\n",
        "    amount_bin_stats[\"bin_order\"] = amount_bin_stats[\"amount_bin\"].apply(lambda x: bin_order.index(x) if x in bin_order else 999)\n",
        "    amount_bin_stats = amount_bin_stats.sort_values(\"bin_order\")\n",
        "else:\n",
        "    bin_order = [\"<$50\", \"$50-$100\", \"$100-$200\", \"$200-$300\", \"$300-$500\", \"$500-$1000\", \">$1000\"]\n",
        "    if \"bin_order\" not in amount_bin_stats.columns:\n",
        "        amount_bin_stats[\"bin_order\"] = amount_bin_stats[\"amount_bin\"].apply(lambda x: bin_order.index(x) if x in bin_order else 999)\n",
        "        amount_bin_stats = amount_bin_stats.sort_values(\"bin_order\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"FRAUD RATE BY AMOUNT BIN\")\n",
        "print(\"=\" * 80)\n",
        "print(amount_bin_stats[['amount_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Key insights\n",
        "riskiest_bin = amount_bin_stats.loc[amount_bin_stats['fraud_rate_pct'].idxmax()]\n",
        "safest_bin = amount_bin_stats.loc[amount_bin_stats['fraud_rate_pct'].idxmin()]\n",
        "\n",
        "print(\"\\nKEY INSIGHTS:\")\n",
        "print(f\"Riskiest amount bin:  {riskiest_bin['amount_bin']} ({riskiest_bin['fraud_rate_pct']:.4f}% fraud rate, {riskiest_bin['total_txns']:,} txns)\")\n",
        "print(f\"Safest amount bin:    {safest_bin['amount_bin']} ({safest_bin['fraud_rate_pct']:.4f}% fraud rate, {safest_bin['total_txns']:,} txns)\")\n",
        "\n",
        "if _computed_amount_bin and \"results_manager\" in globals():\n",
        "    results_manager.save_dataframe(amount_bin_stats, \"5.3\", \"amount_bin_stats\")\n",
        "    results_manager.save_statistics(\n",
        "        {\"riskiest_bin\": riskiest_bin[\"amount_bin\"], \"riskiest_rate\": float(riskiest_bin[\"fraud_rate_pct\"]),\n",
        "         \"safest_bin\": safest_bin[\"amount_bin\"], \"safest_rate\": float(safest_bin[\"fraud_rate_pct\"])},\n",
        "        \"5.3\", \"amount_key_stats\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.4 Amount Distribution Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:34.524192Z",
          "iopub.status.busy": "2026-01-24T00:43:34.523927Z",
          "iopub.status.idle": "2026-01-24T00:43:35.858492Z",
          "shell.execute_reply": "2026-01-24T00:43:35.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:43:34.524170Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AMOUNT DISTRIBUTION VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "# Sample data for visualization (to avoid memory issues)\n",
        "fraud_sample = amount_df.filter(col(\"is_fraud\") == 1).sample(False, 1.0).limit(7506).toPandas()\n",
        "legit_sample = amount_df.filter(col(\"is_fraud\") == 0).sample(False, 0.01).toPandas()\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Chart 1: Histogram comparison\n",
        "ax1 = axes[0, 0]\n",
        "ax1.hist(legit_sample['amt'], bins=50, alpha=0.6, label='Legitimate', color='blue', density=True)\n",
        "ax1.hist(fraud_sample['amt'], bins=50, alpha=0.6, label='Fraud', color='red', density=True)\n",
        "ax1.set_xlabel('Amount ($)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Density', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Amount Distribution: Fraud vs Legitimate', fontsize=14, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Chart 2: Box plot comparison\n",
        "ax2 = axes[0, 1]\n",
        "data_for_box = pd.concat([\n",
        "    pd.DataFrame({'Amount': legit_sample['amt'], 'Type': 'Legitimate'}),\n",
        "    pd.DataFrame({'Amount': fraud_sample['amt'], 'Type': 'Fraud'})\n",
        "])\n",
        "sns.boxplot(data=data_for_box, x='Type', y='Amount', ax=ax2)\n",
        "ax2.set_ylabel('Amount ($)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Amount Distribution: Box Plot Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Chart 3: Fraud rate by amount bin\n",
        "ax3 = axes[1, 0]\n",
        "ax3.bar(range(len(amount_bin_stats)), amount_bin_stats['fraud_rate_pct'], color='crimson', alpha=0.7)\n",
        "ax3.set_xticks(range(len(amount_bin_stats)))\n",
        "ax3.set_xticklabels(amount_bin_stats['amount_bin'], rotation=45, ha='right')\n",
        "ax3.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Fraud Rate by Amount Bin', fontsize=14, fontweight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 4: Transaction volume by amount bin\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar(range(len(amount_bin_stats)), amount_bin_stats['total_txns'], color='steelblue', alpha=0.7)\n",
        "ax4.set_xticks(range(len(amount_bin_stats)))\n",
        "ax4.set_xticklabels(amount_bin_stats['amount_bin'], rotation=45, ha='right')\n",
        "ax4.set_ylabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('Transaction Volume by Amount Bin', fontsize=14, fontweight='bold')\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('amount_distribution_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualizations saved as 'amount_distribution_analysis.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.5 Key Findings Summary\n",
        "\n",
        "**Data validation:** Amount column coverage 100% (1,296,675 valid rows, 0 excluded).\n",
        "\n",
        "**Amount statistics (fraud vs legitimate):**\n",
        "- Mean amount ratio (Fraud/Legitimate): **7.85x** (~$531 vs ~$68)\n",
        "- Median amount ratio (Fraud/Legitimate): **8.35x** (~$395 vs ~$47)\n",
        "- Fraud transactions tend to be **LARGER** than legitimate.\n",
        "\n",
        "**Amount bins and fraud rates:**\n",
        "- **Riskiest bin:** >$1000 (24.13% fraud rate, 3,937 txns)\n",
        "- **Safest bin:** $50-$100 (0.01% fraud rate, 389,514 txns)\n",
        "- Fraud rate rises sharply from $200-$300 (~2.5%) through $300-$500 (~8.6%) to $500-$1000 (~23.1%) and >$1000 (~24.1%).\n",
        "- Lowest fraud rates in <$50 (0.24%) and $50-$100 (0.01%).\n",
        "\n",
        "**Distribution and volume:**\n",
        "- Legitimate transactions are concentrated at low amounts; fraudulent ones are skewed toward higher values with a secondary peak roughly $500â€“$1000.\n",
        "- Most volume is in <$50 (~672k) and $50-$100 (~390k); bins with highest fraud ($500-$1000, >$1000) have much lower volume.\n",
        "\n",
        "**Implication:** Transaction amount is a strong fraud signal; higher amounts are associated with much higher fraud rates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3.6 Save Section 5 Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 5 (AMOUNT FEATURES)\n",
        "# ============================================================\n",
        "\n",
        "required_columns_section5 = [\"amt\", \"amount_bin\"]\n",
        "missing_cols = [c for c in required_columns_section5 if c not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"WARNING: Missing columns {missing_cols}; saving with available columns.\")\n",
        "else:\n",
        "    print(\"All Section 5 columns present; saving checkpoint.\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df,\n",
        "    CHECKPOINT_SECTION5,\n",
        "    \"Section 5 (Amount Features - Section-Level Checkpoint)\"\n",
        ")\n",
        "print(\"âœ“ Section 5 section-level checkpoint saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Timezone Resolution & Temporal Feature Engineering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.0 Load Checkpoint (Optional â€“ when restarting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If train_df not in memory, load from checkpoint (Section 5 preferred, else Section 3)\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    df_loaded, loaded = checkpoint_manager.load_checkpoint(\n",
        "        checkpoint_path=CHECKPOINT_SECTION5,\n",
        "        required_columns=[\"amt\", \"amount_bin\"],\n",
        "        cell_name=\"Section 5 (preferred for Section 6)\"\n",
        "    )\n",
        "    if not loaded:\n",
        "        df_loaded, loaded = checkpoint_manager.load_checkpoint(\n",
        "            checkpoint_path=CHECKPOINT_SECTION3,\n",
        "            required_columns=None,\n",
        "            cell_name=\"Section 3 (fallback for Section 6)\"\n",
        "        )\n",
        "    if not loaded:\n",
        "        raise ValueError(\n",
        "            \"train_df not found. Run Section 3 (Data Loading) first or ensure a checkpoint exists.\"\n",
        "        )\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "else:\n",
        "    print(\"âœ“ train_df already in memory - continuing with Section 6\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1 Constants and Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:32.001946Z",
          "iopub.status.busy": "2026-01-24T00:43:32.001541Z",
          "iopub.status.idle": "2026-01-24T00:43:32.023192Z",
          "shell.execute_reply": "2026-01-24T00:43:32.021785Z",
          "shell.execute_reply.started": "2026-01-24T00:43:32.001923Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "GRID_SIZE = 0.5  # ~50km grid resolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1.1 Helper Function for Timezone Resolution (DRY Pattern)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:32.024784Z",
          "iopub.status.busy": "2026-01-24T00:43:32.024527Z",
          "iopub.status.idle": "2026-01-24T00:43:32.04799Z",
          "shell.execute_reply": "2026-01-24T00:43:32.046683Z",
          "shell.execute_reply.started": "2026-01-24T00:43:32.024761Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TIMEZONE RESOLUTION HELPER (DRY PATTERN)\n",
        "# ============================================================\n",
        "\n",
        "def resolve_timezone_with_grid(df, lat_col, lon_col, grid_size, zip_ref_df, entity_name=\"locations\"):\n",
        "    \"\"\"\n",
        "    Resolve timezones using grid-based lookup with nearest neighbor fallback.\n",
        "    \n",
        "    Design: DRY pattern - reusable for both merchants and customers.\n",
        "    Performance: Broadcast, caching, vectorized neighbor search.\n",
        "    \n",
        "    Args:\n",
        "        df: Source DataFrame\n",
        "        lat_col: Latitude column name\n",
        "        lon_col: Longitude column name  \n",
        "        grid_size: Grid resolution in degrees\n",
        "        zip_ref_df: Reference table with (lat_grid, lng_grid, timezone)\n",
        "        entity_name: For logging\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (timezone_df, metrics_dict)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(f\"RESOLVING TIMEZONES: {entity_name.upper()}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Direct grid match\n",
        "    timezone_df = (\n",
        "        df.select(lat_col, lon_col).distinct()\n",
        "        .withColumn(\"lat_grid\", floor(col(lat_col) / grid_size))\n",
        "        .withColumn(\"lng_grid\", floor(col(lon_col) / grid_size))\n",
        "        .join(broadcast(zip_ref_df), on=[\"lat_grid\", \"lng_grid\"], how=\"left\")\n",
        "        .select(lat_col, lon_col, col(\"timezone\"))\n",
        "        .cache()\n",
        "    )\n",
        "    \n",
        "    total = timezone_df.count()\n",
        "    direct = timezone_df.filter(col(\"timezone\").isNotNull()).count()\n",
        "    direct_rate = (direct / total) * 100\n",
        "    \n",
        "    print(f\"Direct matches: {direct:,} / {total:,} ({direct_rate:.2f}%)\")\n",
        "    \n",
        "    # Nearest neighbor fallback for NULLs\n",
        "    null_count = total - direct\n",
        "    resolved_fallback = 0\n",
        "    \n",
        "    if null_count > 0:\n",
        "        print(f\"Applying nearest neighbor for {null_count:,} locations...\")\n",
        "        null_locs = timezone_df.filter(col(\"timezone\").isNull()).select(lat_col, lon_col)\n",
        "        \n",
        "        neighbors_found = None\n",
        "        for dx in [-1, 0, 1]:\n",
        "            for dy in [-1, 0, 1]:\n",
        "                if dx == 0 and dy == 0:\n",
        "                    continue\n",
        "                neighbor_tz = (\n",
        "                    null_locs\n",
        "                    .withColumn(\"lat_grid\", floor(col(lat_col) / grid_size) + dx)\n",
        "                    .withColumn(\"lng_grid\", floor(col(lon_col) / grid_size) + dy)\n",
        "                    .join(broadcast(zip_ref_df), on=[\"lat_grid\", \"lng_grid\"], how=\"inner\")\n",
        "                    .select(lat_col, lon_col, col(\"timezone\"))\n",
        "                    .dropDuplicates([lat_col, lon_col])\n",
        "                )\n",
        "                if neighbors_found is None:\n",
        "                    neighbors_found = neighbor_tz\n",
        "                else:\n",
        "                    neighbors_found = neighbors_found.union(neighbor_tz).dropDuplicates([lat_col, lon_col])\n",
        "        \n",
        "        if neighbors_found:\n",
        "            timezone_df = (\n",
        "                timezone_df\n",
        "                .join(neighbors_found.withColumnRenamed(\"timezone\", \"fallback_tz\"),\n",
        "                     on=[lat_col, lon_col], how=\"left\")\n",
        "                .withColumn(\"timezone\", when(col(\"timezone\").isNull(), col(\"fallback_tz\")).otherwise(col(\"timezone\")))\n",
        "                .drop(\"fallback_tz\")\n",
        "                .cache()\n",
        "            )\n",
        "            resolved_fallback = timezone_df.filter(col(\"timezone\").isNotNull()).count() - direct\n",
        "            print(f\"âœ“ Resolved {resolved_fallback:,} via nearest neighbor\")\n",
        "    \n",
        "    final_coverage = timezone_df.filter(col(\"timezone\").isNotNull()).count()\n",
        "    final_rate = (final_coverage / total) * 100\n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"Final coverage: {final_coverage:,} / {total:,} ({final_rate:.2f}%)\")\n",
        "    print(f\"Completed in {elapsed:.1f}s\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return timezone_df, {\n",
        "        \"entity\": entity_name,\n",
        "        \"total\": total,\n",
        "        \"direct_matches\": direct,\n",
        "        \"fallback_matches\": resolved_fallback,\n",
        "        \"final_coverage\": final_coverage,\n",
        "        \"direct_rate\": direct_rate,\n",
        "        \"final_rate\": final_rate,\n",
        "        \"elapsed_seconds\": elapsed\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Timezone resolution helper function loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.2 Load and Prepare ZIP Reference Table (Immutable)\n",
        "\n",
        "This table is never joined directly into train_df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:32.049422Z",
          "iopub.status.busy": "2026-01-24T00:43:32.049174Z",
          "iopub.status.idle": "2026-01-24T00:43:43.908063Z",
          "shell.execute_reply": "2026-01-24T00:43:43.90685Z",
          "shell.execute_reply.started": "2026-01-24T00:43:32.049399Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset_folders = os.listdir(INPUT_DIR)\n",
        "zipcode_path = sorted(os.listdir(INPUT_DIR))[1]\n",
        "full_zip_path = os.path.join(INPUT_DIR, zipcode_path, \"uszips.csv\")\n",
        "\n",
        "# ============================================================\n",
        "# CREATE ZIP REFERENCE TABLE (DEDUPLICATED BY GRID CELL)\n",
        "# ============================================================\n",
        "# CRITICAL: Ensure only ONE timezone per grid cell to prevent row explosion\n",
        "# Multiple ZIPs can fall into the same grid cell - we take the first one\n",
        "\n",
        "zip_ref_df = (\n",
        "    spark.read.csv(full_zip_path, header=True, inferSchema=True)\n",
        "    .withColumnRenamed(\"zip\", \"zip_ref\")\n",
        "    .withColumn(\"lat_grid\", floor(col(\"lat\") / GRID_SIZE))\n",
        "    .withColumn(\"lng_grid\", floor(col(\"lng\") / GRID_SIZE))\n",
        "    .select(\"lat_grid\", \"lng_grid\", \"timezone\")\n",
        "    .filter(\n",
        "        (trim(col(\"timezone\")) != \"\") &\n",
        "        (col(\"timezone\") != \"FALSE\") &\n",
        "        col(\"timezone\").rlike(\"^[A-Za-z_/]+$\")\n",
        "    )\n",
        "    .distinct()  # Remove duplicate grid+timezone combinations\n",
        "    .groupBy(\"lat_grid\", \"lng_grid\")\n",
        "    .agg(first(\"timezone\").alias(\"timezone\"))  # Ensure ONE timezone per grid cell\n",
        "    .cache()\n",
        ")\n",
        "\n",
        "print(f\"âœ“ ZIP reference table created: {zip_ref_df.count():,} unique grid cells\")\n",
        "print(f\"âœ“ Timezones available: {zip_ref_df.select('timezone').distinct().count()}\")\n",
        "zip_ref_df.select(\"timezone\").distinct().show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2.1 Customer Timezone (ZIP â†’ Timezone)\n",
        "\n",
        "#### Design\n",
        "\n",
        "- Join only on zip\n",
        "\n",
        "- Add exactly one column\n",
        "\n",
        "- Skip creation if it already exists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.1.1 Create Customer Timezone Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:43:43.909627Z",
          "iopub.status.busy": "2026-01-24T00:43:43.909344Z",
          "iopub.status.idle": "2026-01-24T00:44:53.405529Z",
          "shell.execute_reply": "2026-01-24T00:44:53.402849Z",
          "shell.execute_reply.started": "2026-01-24T00:43:43.909602Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CUSTOMER TIMEZONE RESOLUTION (GRID-BASED WITH FALLBACK)\n",
        "# ============================================================\n",
        "\n",
        "def compute_customer_timezone(df):\n",
        "    \"\"\"\n",
        "    Compute customer timezone using grid-based lookup with fallback.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with customer_timezone and customer_timezone_valid columns\n",
        "    \"\"\"\n",
        "    # Declare global variable at the top (before use)\n",
        "    global customer_metrics\n",
        "    \n",
        "    if \"customer_timezone\" in df.columns:\n",
        "        print(\"âš ï¸ customer_timezone already exists, skipping computation\")\n",
        "        return df\n",
        "    \n",
        "    # Use DRY helper function (includes nearest neighbor fallback)\n",
        "    customer_tz_df, customer_metrics = resolve_timezone_with_grid(\n",
        "        df=df,\n",
        "        lat_col=\"lat\",\n",
        "        lon_col=\"long\",\n",
        "        grid_size=GRID_SIZE,\n",
        "        zip_ref_df=zip_ref_df,\n",
        "        entity_name=\"customers\"\n",
        "    )\n",
        "    \n",
        "    # Join to main DataFrame\n",
        "    df = df.join(\n",
        "        customer_tz_df.withColumnRenamed(\"timezone\", \"customer_timezone\"),\n",
        "        on=[\"lat\", \"long\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    \n",
        "    # Validation column for data quality tracking\n",
        "    df = df.withColumn(\n",
        "        \"customer_timezone_valid\",\n",
        "        col(\"customer_timezone\").isNotNull()\n",
        "    )\n",
        "    \n",
        "    # Cleanup cached intermediate DataFrame\n",
        "    customer_tz_df.unpersist()\n",
        "    \n",
        "    print(f\"âœ“ Customer timezone column added to train_df\")\n",
        "    print(f\"  Coverage: {customer_metrics['final_rate']:.2f}% ({customer_metrics['final_coverage']:,} / {customer_metrics['total']:,})\")\n",
        "    \n",
        "    # Store metrics for summary (needed later) - already declared as global above\n",
        "    # customer_metrics is now assigned above, so this line is actually redundant\n",
        "    # But keeping it for clarity\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"6\", \"2_1_1\", \"customer_timezone\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: compute_customer_timezone(train_df),\n",
        "    required_columns=[\"customer_timezone\", \"customer_timezone_valid\"],\n",
        "    cell_name=\"Section 6.2.1.1 (Customer Timezone Resolution)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.1.2 Customer Timezone Validation\n",
        "\n",
        "**Note:** Validation column (`customer_timezone_valid`) is already created in Section 6.2.1.1 during timezone resolution. This section is kept for reference but the validation is performed automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:44:53.413834Z",
          "iopub.status.busy": "2026-01-24T00:44:53.411672Z",
          "iopub.status.idle": "2026-01-24T00:44:53.42333Z",
          "shell.execute_reply": "2026-01-24T00:44:53.422523Z",
          "shell.execute_reply.started": "2026-01-24T00:44:53.413765Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Validation column already created in Section 6.2.1.1\n",
        "# No additional action needed - customer_timezone_valid column exists\n",
        "print(\"âœ“ Customer timezone validation column already created in Section 6.2.1.1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2.2 Merchant Timezone (Lat/Lng â†’ Grid â†’ Timezone)\n",
        "\n",
        "#### Design\n",
        "\n",
        "- Convert merchant lat/lng into grid keys\n",
        "\n",
        "- Join using derived grid\n",
        "\n",
        "- Never leak grid columns into train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.2.1 Create Merchant Timezone Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:44:53.424526Z",
          "iopub.status.busy": "2026-01-24T00:44:53.424301Z",
          "iopub.status.idle": "2026-01-24T00:46:40.315288Z",
          "shell.execute_reply": "2026-01-24T00:46:40.312023Z",
          "shell.execute_reply.started": "2026-01-24T00:44:53.424504Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MERCHANT TIMEZONE RESOLUTION (GRID-BASED WITH FALLBACK)\n",
        "# ============================================================\n",
        "\n",
        "def compute_merchant_timezone(df):\n",
        "    \"\"\"\n",
        "    Compute merchant timezone using grid-based lookup with fallback.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with merchant_timezone and merchant_timezone_valid columns\n",
        "    \"\"\"\n",
        "    global merchant_metrics \n",
        "    \n",
        "    if \"merchant_timezone\" in df.columns:\n",
        "        print(\"âš ï¸ merchant_timezone already exists, skipping computation\")\n",
        "        return df\n",
        "        \n",
        "    # Use same DRY helper function (includes nearest neighbor fallback)\n",
        "    merchant_tz_df, merchant_metrics = resolve_timezone_with_grid(\n",
        "        df=df,\n",
        "        lat_col=\"merch_lat\",\n",
        "        lon_col=\"merch_long\",\n",
        "        grid_size=GRID_SIZE,\n",
        "        zip_ref_df=zip_ref_df,\n",
        "        entity_name=\"merchants\"\n",
        "    )\n",
        "    \n",
        "    # Join to main DataFrame\n",
        "    df = df.join(\n",
        "        merchant_tz_df.withColumnRenamed(\"timezone\", \"merchant_timezone\"),\n",
        "        on=[\"merch_lat\", \"merch_long\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    \n",
        "    # Validation column for data quality tracking\n",
        "    df = df.withColumn(\n",
        "        \"merchant_timezone_valid\",\n",
        "        col(\"merchant_timezone\").isNotNull()\n",
        "    )\n",
        "    \n",
        "    # Cleanup cached intermediate DataFrame\n",
        "    merchant_tz_df.unpersist()\n",
        "    \n",
        "    print(f\"âœ“ Merchant timezone column added to train_df\")\n",
        "    print(f\"  Coverage: {merchant_metrics['final_rate']:.2f}% ({merchant_metrics['final_coverage']:,} / {merchant_metrics['total']:,})\")\n",
        "    \n",
        "    merchant_metrics = merchant_metrics\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"6\", \"2_2_1\", \"merchant_timezone\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: compute_merchant_timezone(train_df),\n",
        "    required_columns=[\"merchant_timezone\", \"merchant_timezone_valid\"],\n",
        "    cell_name=\"Section 6.2.2.1 (Merchant Timezone Resolution)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.2.2 Merchant Timezone Validation\n",
        "\n",
        "**Note:** Validation column (`merchant_timezone_valid`) is already created in Section 6.2.2.1 during timezone resolution. This section is kept for reference but the validation is performed automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:46:40.318947Z",
          "iopub.status.busy": "2026-01-24T00:46:40.317081Z",
          "iopub.status.idle": "2026-01-24T00:46:40.325725Z",
          "shell.execute_reply": "2026-01-24T00:46:40.324951Z",
          "shell.execute_reply.started": "2026-01-24T00:46:40.318883Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Validation column already created in Section 6.2.2.1\n",
        "# No additional action needed - merchant_timezone_valid column exists\n",
        "print(\"âœ“ Merchant timezone validation column already created in Section 6.2.2.1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2.3 Final Schema Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:46:40.327239Z",
          "iopub.status.busy": "2026-01-24T00:46:40.327Z",
          "iopub.status.idle": "2026-01-24T00:46:40.358212Z",
          "shell.execute_reply": "2026-01-24T00:46:40.357333Z",
          "shell.execute_reply.started": "2026-01-24T00:46:40.327217Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2.3.3 Timezone Resolution Summary & Metrics\n",
        "\n",
        "**Purpose:** Display comprehensive metrics for both customer and merchant timezone resolution to validate data quality.\n",
        "\n",
        "**What We Track:**\n",
        "- Direct match rate (grid-based lookup)\n",
        "- Fallback match rate (nearest neighbor)\n",
        "- Final coverage percentage\n",
        "- Processing time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:46:40.361631Z",
          "iopub.status.busy": "2026-01-24T00:46:40.361303Z",
          "iopub.status.idle": "2026-01-24T00:46:40.4196Z",
          "shell.execute_reply": "2026-01-24T00:46:40.418873Z",
          "shell.execute_reply.started": "2026-01-24T00:46:40.361606Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TIMEZONE RESOLUTION SUMMARY & METRICS\n",
        "# ============================================================\n",
        "# Purpose: Display comprehensive metrics for both customer and merchant\n",
        "#          timezone resolution to validate data quality\n",
        "\n",
        "if 'customer_metrics' in locals() and 'merchant_metrics' in locals():\n",
        "    \n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame([customer_metrics, merchant_metrics])\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"TIMEZONE RESOLUTION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    print(summary_df[['entity', 'total', 'direct_matches', 'fallback_matches', \n",
        "                      'final_coverage', 'direct_rate', 'final_rate', 'elapsed_seconds']].to_string(index=False))\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Key insights\n",
        "    print(\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "    print(f\"  Customer coverage: {customer_metrics['final_rate']:.2f}% \"\n",
        "          f\"({customer_metrics['direct_rate']:.2f}% direct, \"\n",
        "          f\"{customer_metrics['fallback_matches']:,} via fallback)\")\n",
        "    print(f\"  Merchant coverage: {merchant_metrics['final_rate']:.2f}% \"\n",
        "          f\"({merchant_metrics['direct_rate']:.2f}% direct, \"\n",
        "          f\"{merchant_metrics['fallback_matches']:,} via fallback)\")\n",
        "    print(f\"  Total processing time: {customer_metrics['elapsed_seconds'] + merchant_metrics['elapsed_seconds']:.1f}s\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "else:\n",
        "    print(\"âš ï¸  Metrics not available - ensure both customer and merchant timezones were resolved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2.4 Data Quality Check "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:46:40.421317Z",
          "iopub.status.busy": "2026-01-24T00:46:40.421071Z",
          "iopub.status.idle": "2026-01-24T00:47:36.107634Z",
          "shell.execute_reply": "2026-01-24T00:47:36.106838Z",
          "shell.execute_reply.started": "2026-01-24T00:46:40.42129Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_df.select(\n",
        "    \"customer_timezone\",\n",
        "    \"merchant_timezone\",\n",
        "    \"customer_timezone_valid\",\n",
        "    \"merchant_timezone_valid\"\n",
        ").summary().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.3 Convert UTC Timestamps to Local Time\n",
        "\n",
        "### Purpose\n",
        "Now that we have timezone information for both merchants and customers, we can convert the raw transaction timestamps to **local time**.\n",
        "\n",
        "### Why This Matters\n",
        "- Raw timestamps in the data are in UTC (or mixed timezones)\n",
        "- \"10 PM\" in UTC could be 2 PM Pacific or 5 PM Eastern\n",
        "- We need to know \"What time was it AT THE MERCHANT when transaction occurred?\"\n",
        "- This gives us the TRUE local hour for fraud pattern analysis\n",
        "\n",
        "### What We'll Create\n",
        "Two new timestamp columns:\n",
        "1. **`merchant_local_time`**: Transaction time at merchant location (PRIMARY for analysis)\n",
        "2. **`customer_local_time`**: Transaction time at customer location (for behavioral analysis)\n",
        "\n",
        "### Method\n",
        "Use Spark's native `from_utc_timestamp()` function:\n",
        "- Input: UTC timestamp + timezone string\n",
        "- Output: Local timestamp\n",
        "- Fast, optimized, handles DST automatically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:47:36.109006Z",
          "iopub.status.busy": "2026-01-24T00:47:36.108753Z",
          "iopub.status.idle": "2026-01-24T00:48:59.564684Z",
          "shell.execute_reply": "2026-01-24T00:48:59.564173Z",
          "shell.execute_reply.started": "2026-01-24T00:47:36.108984Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONVERT UTC TO MERCHANT LOCAL TIME\n",
        "# ============================================================\n",
        "\n",
        "def compute_merchant_local_time(df):\n",
        "    \"\"\"\n",
        "    Convert UTC timestamps to merchant local time.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with merchant_local_time column\n",
        "    \"\"\"\n",
        "    if \"merchant_local_time\" in df.columns:\n",
        "        print(\"âš ï¸ merchant_local_time already exists, skipping\")\n",
        "        return df\n",
        "    \n",
        "    df = df.withColumn(\n",
        "        \"merchant_local_time\",\n",
        "        from_utc_timestamp(col(\"trans_date_trans_time\"), col(\"merchant_timezone\"))\n",
        "    )\n",
        "    \n",
        "    print(\"âœ“ Merchant local time created\")\n",
        "    print(\"\\nSample conversions (UTC â†’ Merchant Local):\")\n",
        "    df.select(\n",
        "        \"trans_date_trans_time\",\n",
        "        \"merchant_timezone\",\n",
        "        \"merchant_local_time\"\n",
        "    ).show(5, truncate=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"6\", \"3\", \"merchant_local_time\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: compute_merchant_local_time(train_df),\n",
        "    required_columns=[\"merchant_local_time\"],\n",
        "    cell_name=\"Section 6.3 (Merchant Local Time Conversion)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:48:59.565675Z",
          "iopub.status.busy": "2026-01-24T00:48:59.565458Z",
          "iopub.status.idle": "2026-01-24T00:50:14.19855Z",
          "shell.execute_reply": "2026-01-24T00:50:14.19785Z",
          "shell.execute_reply.started": "2026-01-24T00:48:59.565652Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONVERT UTC TO CUSTOMER LOCAL TIME\n",
        "# ============================================================\n",
        "\n",
        "def compute_customer_local_time(df):\n",
        "    \"\"\"\n",
        "    Convert UTC timestamps to customer local time.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with customer_local_time column\n",
        "    \"\"\"\n",
        "    if \"customer_local_time\" in df.columns:\n",
        "        print(\"âš ï¸ customer_local_time already exists, skipping\")\n",
        "        return df\n",
        "    \n",
        "    df = df.withColumn(\n",
        "        \"customer_local_time\",\n",
        "        from_utc_timestamp(col(\"trans_date_trans_time\"), col(\"customer_timezone\"))\n",
        "    )\n",
        "    \n",
        "    print(\"âœ“ Customer local time created\")\n",
        "    print(\"\\nSample conversions (UTC â†’ Customer Local):\")\n",
        "    df.select(\n",
        "        \"trans_date_trans_time\",\n",
        "        \"customer_timezone\",\n",
        "        \"customer_local_time\"\n",
        "    ).show(5, truncate=False)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"6\", \"3\", \"customer_local_time\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: compute_customer_local_time(train_df),\n",
        "    required_columns=[\"customer_local_time\"],\n",
        "    cell_name=\"Section 6.3 (Customer Local Time Conversion)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3.1 Local Time Conversion Summary\n",
        "\n",
        "Conversion applied above. Validation performed in Section 6.3.3 (Production Fallback).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:50:14.200172Z",
          "iopub.status.busy": "2026-01-24T00:50:14.199941Z",
          "iopub.status.idle": "2026-01-24T00:50:14.210247Z",
          "shell.execute_reply": "2026-01-24T00:50:14.208015Z",
          "shell.execute_reply.started": "2026-01-24T00:50:14.200153Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Validation happens in Section 6.3.3 with before/after comparison\n",
        "print(\"âœ“ Local time conversion complete - see Section 6.3.3 for validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3.2 Compare UTC vs Local Time Examples\n",
        "\n",
        "### Purpose\n",
        "Show concrete examples of how timestamps changed\n",
        "\n",
        "### What to Look For\n",
        "- Time difference between UTC and local\n",
        "- Different timezones showing different offsets\n",
        "- Verify conversions look correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:50:14.211466Z",
          "iopub.status.busy": "2026-01-24T00:50:14.211205Z",
          "iopub.status.idle": "2026-01-24T00:51:51.981458Z",
          "shell.execute_reply": "2026-01-24T00:51:51.980751Z",
          "shell.execute_reply.started": "2026-01-24T00:50:14.211442Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Show side-by-side comparison for different timezones\n",
        "print(\"UTC vs LOCAL TIME COMPARISON\\n\")\n",
        "print(\"Showing examples from different timezones:\")\n",
        "\n",
        "comparison_df = train_df.select(\n",
        "    \"trans_date_trans_time\",\n",
        "    \"merchant_timezone\",\n",
        "    \"merchant_local_time\",\n",
        "    hour(\"trans_date_trans_time\").alias(\"utc_hour\"),\n",
        "    hour(\"merchant_local_time\").alias(\"local_hour\")\n",
        ").filter(\n",
        "    col(\"merchant_timezone\").isNotNull()\n",
        ")\n",
        "\n",
        "# Show examples from each major timezone\n",
        "for tz in [\"America/New_York\", \"America/Chicago\", \"America/Denver\", \"America/Los_Angeles\"]:\n",
        "    print(f\"\\n{tz}:\")\n",
        "    comparison_df.filter(col(\"merchant_timezone\") == tz).show(2, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 6.3.3: Production Fallback "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:51:51.982408Z",
          "iopub.status.busy": "2026-01-24T00:51:51.982238Z",
          "iopub.status.idle": "2026-01-24T00:54:12.793279Z",
          "shell.execute_reply": "2026-01-24T00:54:12.792467Z",
          "shell.execute_reply.started": "2026-01-24T00:51:51.982392Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PRODUCTION FALLBACK: Single-Pass Validation & 100% Coverage\n",
        "# ============================================================\n",
        "\n",
        "def apply_production_fallback(df):\n",
        "    \"\"\"\n",
        "    Apply production fallback to ensure 100% coverage for local timestamps.\n",
        "    \n",
        "    Fallback strategy:\n",
        "    1. merchant_local_time NULL â†’ use customer_local_time\n",
        "    2. merchant_local_time still NULL â†’ use UTC (trans_date_trans_time)\n",
        "    3. customer_local_time NULL â†’ use UTC (trans_date_trans_time)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with 100% coverage for both local time columns\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Single-pass validation (before fallback)\n",
        "    counts_before = df.agg(\n",
        "        count(\"*\").alias(\"total\"),\n",
        "        spark_sum(when(col(\"merchant_local_time\").isNotNull(), 1).otherwise(0)).alias(\"merchant_valid\"),\n",
        "        spark_sum(when(col(\"customer_local_time\").isNotNull(), 1).otherwise(0)).alias(\"customer_valid\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    total = counts_before[\"total\"]\n",
        "    merchant_before = counts_before[\"merchant_valid\"]\n",
        "    customer_before = counts_before[\"customer_valid\"]\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"PRODUCTION FALLBACK: Ensuring 100% Coverage\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\nBefore fallback:\")\n",
        "    print(f\"Merchant: {merchant_before:,} / {total:,} ({merchant_before/total*100:.2f}%)\")\n",
        "    print(f\"Customer: {customer_before:,} / {total:,} ({customer_before/total*100:.2f}%)\")\n",
        "    \n",
        "    # Fallback Level 1: If merchant_local_time is NULL, use customer_local_time\n",
        "    df = df.withColumn(\n",
        "        \"merchant_local_time\",\n",
        "        when(col(\"merchant_local_time\").isNull(), col(\"customer_local_time\"))\n",
        "        .otherwise(col(\"merchant_local_time\"))\n",
        "    )\n",
        "    \n",
        "    # Fallback Level 2: If still NULL, use UTC (guaranteed to exist)\n",
        "    df = df.withColumn(\n",
        "        \"merchant_local_time\",\n",
        "        when(col(\"merchant_local_time\").isNull(), col(\"trans_date_trans_time\"))\n",
        "        .otherwise(col(\"merchant_local_time\"))\n",
        "    )\n",
        "    \n",
        "    df = df.withColumn(\n",
        "        \"customer_local_time\",\n",
        "        when(col(\"customer_local_time\").isNull(), col(\"trans_date_trans_time\"))\n",
        "        .otherwise(col(\"customer_local_time\"))\n",
        "    )\n",
        "    \n",
        "    # Single-pass validation (AFTER fallback)\n",
        "    counts_after = df.agg(\n",
        "        spark_sum(when(col(\"merchant_local_time\").isNotNull(), 1).otherwise(0)).alias(\"merchant_valid\"),\n",
        "        spark_sum(when(col(\"customer_local_time\").isNotNull(), 1).otherwise(0)).alias(\"customer_valid\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    merchant_after = counts_after[\"merchant_valid\"]\n",
        "    customer_after = counts_after[\"customer_valid\"]\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nAfter fallback:\")\n",
        "    print(f\"Merchant: {merchant_after:,} / {total:,} ({merchant_after/total*100:.2f}%)\")\n",
        "    print(f\"Customer: {customer_after:,} / {total:,} ({customer_after/total*100:.2f}%)\")\n",
        "    \n",
        "    if merchant_after == total and customer_after == total:\n",
        "        print(f\"\\nâœ“ SUCCESS: 100% coverage achieved in {elapsed:.1f}s - production ready!\")\n",
        "    else:\n",
        "        print(f\"\\nâš  WARNING: Coverage incomplete\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"6\", \"3_3\", \"production_fallback\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: apply_production_fallback(train_df),\n",
        "    required_columns=[\"merchant_local_time\", \"customer_local_time\"],\n",
        "    cell_name=\"Section 6.3.3 (Production Fallback - 100% Coverage)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3.4 Save Section-Level Checkpoint\n",
        "\n",
        "**Purpose:** Save the timezone-resolved DataFrame to section-level checkpoint for future use.\n",
        "This enables skipping Section 6 when running later sections (Section 7, 8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:54:12.794895Z",
          "iopub.status.busy": "2026-01-24T00:54:12.794655Z",
          "iopub.status.idle": "2026-01-24T00:55:12.128875Z",
          "shell.execute_reply": "2026-01-24T00:55:12.128281Z",
          "shell.execute_reply.started": "2026-01-24T00:54:12.794875Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 6 (TIMEZONE RESOLUTION COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 6 adds\n",
        "required_columns_section6 = [\n",
        "    \"merchant_timezone\",\n",
        "    \"customer_timezone\", \n",
        "    \"merchant_local_time\",\n",
        "    \"customer_local_time\"\n",
        "]\n",
        "\n",
        "# Verify all required columns exist\n",
        "missing_cols = [col for col in required_columns_section6 if col not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"âš ï¸  WARNING: Missing columns before section checkpoint save: {missing_cols}\")\n",
        "    print(\"  Section checkpoint not saved. Please ensure Section 6 completed successfully.\")\n",
        "else:\n",
        "    print(\"âœ“ All Section 6 columns present - saving section-level checkpoint...\")\n",
        "    checkpoint_manager.save_checkpoint(\n",
        "        train_df, \n",
        "        CHECKPOINT_SECTION6, \n",
        "        \"Section 6 (Timezone Resolution Complete - Section-Level Checkpoint)\"\n",
        "    )\n",
        "    print(\"âœ“ Section 6 section-level checkpoint saved.\")\n",
        "    print(\"  Note: Cell-level checkpoints also exist for granular control.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3.4 Section Summary\n",
        "\n",
        "**What We Accomplished:**\n",
        "- âœ… Converted all UTC timestamps to merchant local time\n",
        "- âœ… Converted all UTC timestamps to customer local time  \n",
        "- âœ… Validated conversion success rates\n",
        "- âœ… Verified timestamps look correct\n",
        "\n",
        "**Key Findings:**\n",
        "- Merchant conversion rate: [shown in validation above]\n",
        "- Customer conversion rate: [shown in validation above]\n",
        "\n",
        "**Next Steps (Section 6.4):**\n",
        "Now that we have LOCAL timestamps, we need to extract temporal features:\n",
        "- Local hour (0-23)\n",
        "- Local day of week (Mon-Sun)\n",
        "- Local time bins (Late Night, Morning, etc.)\n",
        "- Weekend flags\n",
        "\n",
        "These features will replace our previous UTC-based analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Temporal Fraud Pattern Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.0 Helper Functions for Temporal Analysis and Load Checkpoint\n",
        "\n",
        "### Design Principles\n",
        "- **DRY Pattern:** Reusable functions for all temporal analyses\n",
        "- **Validation First:** Check data quality before aggregation  \n",
        "- **Separation of Concerns:** Temporal patterns only (no amount analysis mixed in)\n",
        "- **Performance:** Minimize Spark operations, post-process in Pandas\n",
        "- **Idempotency:** Cache results to avoid recomputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:12.12992Z",
          "iopub.status.busy": "2026-01-24T00:55:12.129732Z",
          "iopub.status.idle": "2026-01-24T00:55:12.385598Z",
          "shell.execute_reply": "2026-01-24T00:55:12.384709Z",
          "shell.execute_reply.started": "2026-01-24T00:55:12.129903Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT: SECTION 6 (REQUIRED FOR SECTION 7)\n",
        "# ============================================================\n",
        "\n",
        "# Section 7 requires timezone-resolved data from Section 6\n",
        "required_columns_section7 = [\n",
        "    \"merchant_local_time\",\n",
        "    \"customer_local_time\"\n",
        "]\n",
        "\n",
        "df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_SECTION6,\n",
        "    required_columns=required_columns_section7,\n",
        "    cell_name=\"Section 6 (required for Section 7)\"\n",
        ")\n",
        "\n",
        "if loaded_from_checkpoint:\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint - ready for temporal analysis\")\n",
        "else:\n",
        "    if 'train_df' not in globals() or train_df is None:\n",
        "        raise ValueError(\n",
        "            \"train_df not found. Run Section 6 (Timezone Resolution) first or ensure checkpoint exists.\"\n",
        "        )\n",
        "    missing_cols = [c for c in required_columns_section7 if c not in train_df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Required columns missing: {missing_cols}. Run Section 6 first.\")\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(\"âœ“ train_df already in memory\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:12.387272Z",
          "iopub.status.busy": "2026-01-24T00:55:12.386755Z",
          "iopub.status.idle": "2026-01-24T00:55:12.396778Z",
          "shell.execute_reply": "2026-01-24T00:55:12.396261Z",
          "shell.execute_reply.started": "2026-01-24T00:55:12.387249Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS FOR TEMPORAL ANALYSIS (DRY PATTERN)\n",
        "# ============================================================\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "def validate_temporal_coverage(df, time_column, analysis_name):\n",
        "    \"\"\"\n",
        "    Validate temporal data coverage before analysis.\n",
        "    \n",
        "    Args:\n",
        "        df: PySpark DataFrame\n",
        "        time_column: Name of timestamp column to validate\n",
        "        analysis_name: Description for logging\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (filtered_df, coverage_pct, valid_count, total_count)\n",
        "    \"\"\"\n",
        "    total_count = df.count()\n",
        "    filtered_df = df.filter(col(time_column).isNotNull())\n",
        "    valid_count = filtered_df.count()\n",
        "    coverage_pct = (valid_count / total_count) * 100\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(f\"DATA VALIDATION: {analysis_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total rows:      {total_count:,}\")\n",
        "    print(f\"Valid rows:      {valid_count:,}\")\n",
        "    print(f\"Coverage:        {coverage_pct:.2f}%\")\n",
        "    print(f\"Excluded (NULL): {total_count - valid_count:,}\")\n",
        "    \n",
        "    if coverage_pct < 95:\n",
        "        print(f\"\\nâš ï¸  WARNING: Only {coverage_pct:.2f}% coverage - results may be biased\")\n",
        "    else:\n",
        "        print(f\"\\nâœ“ Coverage acceptable ({coverage_pct:.2f}%)\")\n",
        "    \n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    \n",
        "    return filtered_df, coverage_pct, valid_count, total_count\n",
        "\n",
        "\n",
        "# aggregate_fraud_by_dimension moved to Section 2.2 (available to Section 5.3.3 and later)\n",
        "\n",
        "print(\"Helper functions loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Fraud Patterns by Hour\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Fraud rates higher during late night/early morning hours (11 PM - 6 AM)\n",
        "- **Reasoning:** Victims asleep, delayed detection, less merchant monitoring\n",
        "\n",
        "### What We'll Analyze\n",
        "1. Fraud count and rate by hour (0-23)\n",
        "2. Peak fraud hours\n",
        "3. Safest transaction hours\n",
        "4. Visual patterns and anomalies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1.1 Data Validation - Merchant Local Time Coverageta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:12.397619Z",
          "iopub.status.busy": "2026-01-24T00:55:12.397451Z",
          "iopub.status.idle": "2026-01-24T00:55:12.713413Z",
          "shell.execute_reply": "2026-01-24T00:55:12.712832Z",
          "shell.execute_reply.started": "2026-01-24T00:55:12.397601Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE MERCHANT LOCAL TIME FOR HOURLY ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "analysis_df, coverage_pct, valid_count, total_count = validate_temporal_coverage(\n",
        "    df=train_df,\n",
        "    time_column=\"merchant_local_time\",\n",
        "    analysis_name=\"Hourly Fraud Analysis\"\n",
        ")\n",
        "\n",
        "# Extract hour if not already present\n",
        "if \"hour\" not in analysis_df.columns:\n",
        "    analysis_df = analysis_df.withColumn(\n",
        "        \"hour\",\n",
        "        hour(col(\"merchant_local_time\"))\n",
        "    )\n",
        "    print(\"âœ“ Extracted hour from merchant_local_time\")\n",
        "else:\n",
        "    print(\"âœ“ Hour column already exists\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1.2 Hourly Fraud Aggregation (Local Time)\n",
        "\n",
        "Using helper function for clean, reusable analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:12.714922Z",
          "iopub.status.busy": "2026-01-24T00:55:12.714491Z",
          "iopub.status.idle": "2026-01-24T00:55:13.803121Z",
          "shell.execute_reply": "2026-01-24T00:55:13.802473Z",
          "shell.execute_reply.started": "2026-01-24T00:55:12.714899Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HOURLY FRAUD AGGREGATION USING HELPER FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "hourly_fraud_stats = results_manager.load_dataframe(\"7.1\", \"hourly_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if hourly_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded hourly_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    hourly_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=analysis_df,\n",
        "        dimension_col=\"hour\",\n",
        "        dimension_name=\"Hour of Day\",\n",
        "        cache_name=\"cached_hourly_fraud_stats\",\n",
        "        save_result=True, section=\"7.1\", result_name=\"hourly_fraud_stats\"\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY HOUR (LOCAL TIME)\")\n",
        "print(\"=\" * 100)\n",
        "print(hourly_fraud_stats[['hour', 'total_txns', 'fraud_count', 'legit_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "peak_hour = hourly_fraud_stats.loc[hourly_fraud_stats['fraud_rate_pct'].idxmax()]\n",
        "safest_hour = hourly_fraud_stats.loc[hourly_fraud_stats['fraud_rate_pct'].idxmin()]\n",
        "risk_ratio = peak_hour['fraud_rate_pct'] / safest_hour['fraud_rate_pct']\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Peak fraud hour:    {int(peak_hour['hour'])}:00 ({peak_hour['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Safest hour:        {int(safest_hour['hour'])}:00 ({safest_hour['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:         {risk_ratio:.2f}x higher at peak\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1.3 Hourly Fraud Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:13.804465Z",
          "iopub.status.busy": "2026-01-24T00:55:13.804222Z",
          "iopub.status.idle": "2026-01-24T00:55:15.820065Z",
          "shell.execute_reply": "2026-01-24T00:55:15.819169Z",
          "shell.execute_reply.started": "2026-01-24T00:55:13.804442Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HOURLY FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Chart 1: Fraud Count by Hour\n",
        "ax1 = axes[0, 0]\n",
        "ax1.bar(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_count'], \n",
        "        color='crimson', alpha=0.7, edgecolor='darkred')\n",
        "ax1.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Fraud Count', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Fraud Transaction Count by Hour', fontsize=14, fontweight='bold')\n",
        "ax1.set_xticks(range(0, 24))\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 2: Fraud Rate by Hour\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_rate_pct'], \n",
        "         marker='o', linewidth=2.5, color='darkred', markersize=6)\n",
        "ax2.fill_between(hourly_fraud_stats['hour'], hourly_fraud_stats['fraud_rate_pct'], \n",
        "                  alpha=0.3, color='crimson')\n",
        "ax2.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Fraud Rate by Hour', fontsize=14, fontweight='bold')\n",
        "ax2.set_xticks(range(0, 24))\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axvline(x=int(peak_hour['hour']), color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
        "\n",
        "# Chart 3: Fraud vs Legitimate Counts\n",
        "ax3 = axes[1, 0]\n",
        "x = np.arange(len(hourly_fraud_stats))\n",
        "width = 0.35\n",
        "ax3.bar(x - width/2, hourly_fraud_stats['fraud_count'], width, \n",
        "        label='Fraud', color='crimson', alpha=0.7)\n",
        "ax3.bar(x + width/2, hourly_fraud_stats['legit_count'], width, \n",
        "        label='Legitimate', color='green', alpha=0.7)\n",
        "ax3.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Transaction Count (log scale)', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Fraud vs Legitimate Transactions by Hour', fontsize=14, fontweight='bold')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(hourly_fraud_stats['hour'])\n",
        "ax3.legend()\n",
        "ax3.set_yscale('log')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 4: Transaction Volume\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar(hourly_fraud_stats['hour'], hourly_fraud_stats['total_txns'], \n",
        "        color='steelblue', alpha=0.7, edgecolor='darkblue')\n",
        "ax4.set_xlabel('Hour of Day (Local Time)', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('Transaction Volume by Hour', fontsize=14, fontweight='bold')\n",
        "ax4.set_xticks(range(0, 24))\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('hourly_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Visualizations saved as 'hourly_fraud_patterns.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1.4 Key Findings Summary\n",
        "\n",
        "**Peak:** 6 PM (18:00) - 36.85x riskier than safest hour (6 AM)\n",
        "\n",
        "**Next:** Section 7.2 - Day of Week Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Fraud Patterns by Day of Week\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Weekend vs weekday patterns may differ\n",
        "- **Alternative:** Higher weekday volume = more fraud opportunities\n",
        "\n",
        "**Note:** PySpark `dayofweek()`: 1=Sunday, 2=Monday, ..., 7=Saturday\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.1 Data Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:15.821321Z",
          "iopub.status.busy": "2026-01-24T00:55:15.821062Z",
          "iopub.status.idle": "2026-01-24T00:55:15.989721Z",
          "shell.execute_reply": "2026-01-24T00:55:15.988768Z",
          "shell.execute_reply.started": "2026-01-24T00:55:15.821297Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Validate and prepare data for day-of-week analysis\n",
        "daily_analysis_df, coverage, valid, total = validate_temporal_coverage(\n",
        "    df=train_df,\n",
        "    time_column=\"merchant_local_time\",\n",
        "    analysis_name=\"Day of Week Analysis\"\n",
        ")\n",
        "\n",
        "# Extract day_of_week if needed\n",
        "if \"day_of_week\" not in daily_analysis_df.columns:\n",
        "    daily_analysis_df = daily_analysis_df.withColumn(\n",
        "        \"day_of_week\",\n",
        "        dayofweek(col(\"merchant_local_time\"))\n",
        "    )\n",
        "    print(\"âœ“ Extracted day_of_week from merchant_local_time\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.2 Day of Week Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:15.991189Z",
          "iopub.status.busy": "2026-01-24T00:55:15.990914Z",
          "iopub.status.idle": "2026-01-24T00:55:16.530033Z",
          "shell.execute_reply": "2026-01-24T00:55:16.529145Z",
          "shell.execute_reply.started": "2026-01-24T00:55:15.991166Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "daily_fraud_stats = results_manager.load_dataframe(\"7.2\", \"daily_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if daily_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded daily_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    daily_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=daily_analysis_df,\n",
        "        dimension_col=\"day_of_week\",\n",
        "        dimension_name=\"Day of Week\",\n",
        "        cache_name=\"cached_daily_fraud\",\n",
        "        save_result=True, section=\"7.2\", result_name=\"daily_fraud_stats\"\n",
        "    )\n",
        "\n",
        "# Add human-readable labels (needed for display whether loaded or computed)\n",
        "day_names = {1: \"Sunday\", 2: \"Monday\", 3: \"Tuesday\", 4: \"Wednesday\",\n",
        "             5: \"Thursday\", 6: \"Friday\", 7: \"Saturday\"}\n",
        "if \"day_name\" not in daily_fraud_stats.columns:\n",
        "    daily_fraud_stats[\"day_name\"] = daily_fraud_stats[\"day_of_week\"].map(day_names)\n",
        "    daily_fraud_stats[\"is_weekend\"] = daily_fraud_stats[\"day_of_week\"].isin([1, 7]).astype(int)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD BY DAY OF WEEK (LOCAL TIME)\")\n",
        "print(\"=\" * 100)\n",
        "print(daily_fraud_stats[['day_name', 'is_weekend', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "peak_day = daily_fraud_stats.loc[daily_fraud_stats['fraud_rate_pct'].idxmax()]\n",
        "print(f\"\\nðŸ“Š Peak day: {peak_day['day_name']} ({peak_day['fraud_rate_pct']:.4f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.3 Weekend vs Weekday Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:16.531181Z",
          "iopub.status.busy": "2026-01-24T00:55:16.530962Z",
          "iopub.status.idle": "2026-01-24T00:55:17.439876Z",
          "shell.execute_reply": "2026-01-24T00:55:17.43907Z",
          "shell.execute_reply.started": "2026-01-24T00:55:16.531158Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "weekend_stats = results_manager.load_dataframe(\"7.4\", \"weekend_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if weekend_stats is not None:\n",
        "    print(\"âœ“ Loaded weekend_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    if 'daily_analysis_df' not in locals():\n",
        "        print(\"âš ï¸  daily_analysis_df not found. Creating from train_df...\")\n",
        "        daily_analysis_df, coverage, valid, total = validate_temporal_coverage(\n",
        "            df=train_df,\n",
        "            time_column=\"merchant_local_time\",\n",
        "            analysis_name=\"Day of Week Analysis\"\n",
        "        )\n",
        "    if \"day_of_week\" not in daily_analysis_df.columns:\n",
        "        daily_analysis_df = daily_analysis_df.withColumn(\n",
        "            \"day_of_week\",\n",
        "            dayofweek(col(\"merchant_local_time\"))\n",
        "        )\n",
        "        print(\"âœ“ Extracted day_of_week from merchant_local_time\")\n",
        "    if \"is_weekend\" not in daily_analysis_df.columns:\n",
        "        daily_analysis_df = daily_analysis_df.withColumn(\n",
        "            \"is_weekend\",\n",
        "            when(col(\"day_of_week\").isin([1, 7]), 1).otherwise(0)\n",
        "        )\n",
        "        print(\"âœ“ Extracted is_weekend flag (1=Sunday, 7=Saturday)\")\n",
        "    weekend_stats = aggregate_fraud_by_dimension(\n",
        "        df=daily_analysis_df,\n",
        "        dimension_col=\"is_weekend\",\n",
        "        dimension_name=\"Weekend vs Weekday\",\n",
        "        cache_name=\"cached_weekend_stats\",\n",
        "        save_result=True, section=\"7.4\", result_name=\"weekend_fraud_stats\"\n",
        "    )\n",
        "\n",
        "if \"period\" not in weekend_stats.columns:\n",
        "    weekend_stats[\"period\"] = weekend_stats[\"is_weekend\"].map({0: \"Weekday\", 1: \"Weekend\"})\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"WEEKEND vs WEEKDAY\")\n",
        "print(\"=\" * 100)\n",
        "print(weekend_stats[['period', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "weekend_rate = weekend_stats[weekend_stats['period'] == 'Weekend']['fraud_rate_pct'].values[0]\n",
        "weekday_rate = weekend_stats[weekend_stats['period'] == 'Weekday']['fraud_rate_pct'].values[0]\n",
        "ratio = weekend_rate / weekday_rate if weekend_rate > weekday_rate else weekday_rate / weekend_rate\n",
        "period = 'Weekend' if weekend_rate > weekday_rate else 'Weekday'\n",
        "print(f\"\\n{period} transactions are {ratio:.2f}x riskier\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2.4 Key Findings Summary\n",
        "\n",
        "**Peak day:** Friday (0.7139% fraud rate)\n",
        "\n",
        "**Weekday vs Weekend:** Weekday transactions are 1.15x riskier than weekend\n",
        "\n",
        "**Next:** Section 7.3 - Month/Seasonal Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Fraud Patterns by Month (Seasonal Analysis)\n",
        "\n",
        "### Hypothesis\n",
        "- Seasonal patterns during holidays (December, tax season)\n",
        "- Shopping peaks (Black Friday, Christmas) = more fraud opportunities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:17.441296Z",
          "iopub.status.busy": "2026-01-24T00:55:17.441085Z",
          "iopub.status.idle": "2026-01-24T00:55:24.36153Z",
          "shell.execute_reply": "2026-01-24T00:55:24.360861Z",
          "shell.execute_reply.started": "2026-01-24T00:55:17.441278Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXTRACT MONTH FROM MERCHANT LOCAL TIME\n",
        "# ============================================================\n",
        "\n",
        "def extract_month_feature(df):\n",
        "    \"\"\"\n",
        "    Extract month from merchant_local_time.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with month column\n",
        "    \"\"\"\n",
        "    if \"month\" in df.columns:\n",
        "        print(\"âš ï¸ month column already exists, skipping\")\n",
        "        return df\n",
        "    \n",
        "    df = df.withColumn(\"month\", month(col(\"merchant_local_time\")))\n",
        "    print(\"âœ“ Month column extracted from merchant_local_time\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"7\", \"3\", \"month\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: extract_month_feature(train_df),\n",
        "    required_columns=[\"month\"],\n",
        "    cell_name=\"Section 7.3 (Month Feature Extraction)\"\n",
        ")\n",
        "\n",
        "# Continue with monthly aggregation (this doesn't modify train_df, so no checkpoint needed)\n",
        "monthly_fraud_stats = aggregate_fraud_by_dimension(\n",
        "    df=train_df,\n",
        "    dimension_col=\"month\",\n",
        "    dimension_name=\"Month\",\n",
        "    cache_name=\"cached_monthly_fraud\",\n",
        "    save_result=True, section=\"7.3\", result_name=\"monthly_fraud_stats\"\n",
        ")\n",
        "\n",
        "month_names = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\", 6: \"Jun\",\n",
        "               7: \"Jul\", 8: \"Aug\", 9: \"Sep\", 10: \"Oct\", 11: \"Nov\", 12: \"Dec\"}\n",
        "monthly_fraud_stats['month_name'] = monthly_fraud_stats['month'].map(month_names)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD BY MONTH (LOCAL TIME)\")\n",
        "print(\"=\" * 100)\n",
        "print(monthly_fraud_stats[['month_name', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "peak_month = monthly_fraud_stats.loc[monthly_fraud_stats[\"fraud_rate_pct\"].idxmax()]\n",
        "safest_month = monthly_fraud_stats.loc[monthly_fraud_stats[\"fraud_rate_pct\"].idxmin()]\n",
        "risk_ratio = peak_month[\"fraud_rate_pct\"] / safest_month[\"fraud_rate_pct\"]\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Peak fraud month:    {peak_month[\"month_name\"]} ({peak_month[\"fraud_rate_pct\"]:.4f}% fraud rate)\")\n",
        "print(f\"Safest month:        {safest_month[\"month_name\"]} ({safest_month[\"fraud_rate_pct\"]:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:          {risk_ratio:.2f}x higher at peak\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3.1 Key Findings Summary\n",
        "\n",
        "**Peak month:** February (0.8658% fraud rate)\n",
        "\n",
        "**Safest month:** July (0.3834% fraud rate)\n",
        "\n",
        "**Risk ratio:** 2.26x higher in peak month vs. safest month\n",
        "\n",
        "**Seasonal pattern:** Winter (Jan-Feb) shows highest fraud rates; Summer (Jul-Aug) shows lowest.\n",
        "\n",
        "**Next:** Section 7.4 - Weekend vs Weekday Deep Dive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Weekend vs Weekday Deep Dive\n",
        "\n",
        "Detailed comparison of weekend and weekday fraud behaviors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:24.362629Z",
          "iopub.status.busy": "2026-01-24T00:55:24.362435Z",
          "iopub.status.idle": "2026-01-24T00:55:24.36753Z",
          "shell.execute_reply": "2026-01-24T00:55:24.366859Z",
          "shell.execute_reply.started": "2026-01-24T00:55:24.362612Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Already computed in Section 7.2.3\n",
        "# Display detailed statistics\n",
        "\n",
        "if 'weekend_stats' in locals():\n",
        "    print(\"Weekend vs Weekday detailed analysis completed in Section 7.2.3\")\n",
        "    print(\"See above for statistical breakdown\")\n",
        "else:\n",
        "    print(\"Run Section 7.2.3 first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Time Bin Analysis\n",
        "\n",
        "Analyzing fraud by time periods: Night, Morning, Afternoon, Evening\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:24.369394Z",
          "iopub.status.busy": "2026-01-24T00:55:24.36851Z",
          "iopub.status.idle": "2026-01-24T00:55:32.023543Z",
          "shell.execute_reply": "2026-01-24T00:55:32.022286Z",
          "shell.execute_reply.started": "2026-01-24T00:55:24.369363Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXTRACT HOUR AND CREATE TIME BIN FEATURES\n",
        "# ============================================================\n",
        "\n",
        "def extract_time_bin_features(df):\n",
        "    \"\"\"\n",
        "    Extract hour and create time_bin categories.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with hour and time_bin columns\n",
        "    \"\"\"\n",
        "    # Ensure hour column exists\n",
        "    if \"hour\" not in df.columns:\n",
        "        if \"merchant_local_time\" not in df.columns:\n",
        "            raise ValueError(\n",
        "                \"merchant_local_time column is required to extract hour. \"\n",
        "                \"Run timezone conversion first.\"\n",
        "            )\n",
        "        df = df.withColumn(\n",
        "            \"hour\",\n",
        "            hour(col(\"merchant_local_time\"))\n",
        "        )\n",
        "        print(\"âœ“ Extracted hour from merchant_local_time\")\n",
        "    \n",
        "    # Extract time_bin if needed\n",
        "    if \"time_bin\" not in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"time_bin\",\n",
        "            when((col(\"hour\") >= 23) | (col(\"hour\") < 6), \"Night\")\n",
        "            .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"Morning\")\n",
        "            .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"Afternoon\")\n",
        "            .otherwise(\"Evening\")\n",
        "        )\n",
        "        print(\"âœ“ Created time_bin column\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"7\", \"5\", \"time_bin\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: extract_time_bin_features(train_df),\n",
        "    required_columns=[\"hour\", \"time_bin\"],\n",
        "    cell_name=\"Section 7.5 (Time Bin Feature Extraction)\"\n",
        ")\n",
        "\n",
        "# Continue with time bin aggregation (this doesn't modify train_df, so no checkpoint needed)\n",
        "timebin_fraud_stats = aggregate_fraud_by_dimension(\n",
        "    df=train_df,\n",
        "    dimension_col=\"time_bin\",\n",
        "    dimension_name=\"Time Bin\",\n",
        "    cache_name=\"cached_timebin_fraud\",\n",
        "    save_result=True, section=\"7.5\", result_name=\"timebin_fraud_stats\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD BY TIME BIN (LOCAL TIME)\")\n",
        "print(\"=\" * 100)\n",
        "print(timebin_fraud_stats[['time_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "peak_bin = timebin_fraud_stats.loc[timebin_fraud_stats['fraud_rate_pct'].idxmax()]\n",
        "print(f\"\\nðŸ“Š Highest risk period: {peak_bin['time_bin']} ({peak_bin['fraud_rate_pct']:.4f}%)\")\n",
        "safest_bin = timebin_fraud_stats.loc[timebin_fraud_stats[\"fraud_rate_pct\"].idxmin()]\n",
        "risk_ratio = peak_bin[\"fraud_rate_pct\"] / safest_bin[\"fraud_rate_pct\"]\n",
        "\n",
        "print(f\"Safest period:        {safest_bin[\"time_bin\"]} ({safest_bin[\"fraud_rate_pct\"]:.4f}%)\")\n",
        "print(f\"Risk ratio:          {risk_ratio:.2f}x higher at peak\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.5.1 Key Findings Summary\n",
        "\n",
        "**Peak period:** Evening (18-23 PM) - 1.7871% fraud rate\n",
        "\n",
        "**Safest period:** Morning (6-12 PM) - 0.1179% fraud rate\n",
        "\n",
        "**Risk ratio:** 15.16x higher in evening vs. morning\n",
        "\n",
        "**Next:** Section 7.6 - Temporal Analysis Summary & Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Temporal Analysis Summary & Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "Temporal patterns reveal **strong, actionable fraud signals** with 36.85x risk variation across hours and clear seasonal trends. All temporal dimensions (hour, day, month, time bins) show statistically significant fraud rate differences, making them **high-priority features** for modeling.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Findings by Dimension\n",
        "\n",
        "#### 1. Hour of Day Analysis (Section 7.1)\n",
        "\n",
        "**Critical Discovery:**\n",
        "- **Peak Risk:** 18:00 (6 PM) - **2.7155% fraud rate**\n",
        "- **Safest Hour:** 06:00 (6 AM) - **0.0737% fraud rate**\n",
        "- **Risk Ratio:** **36.85x higher** at peak vs. safest hour\n",
        "\n",
        "**Pattern Analysis:**\n",
        "- **Morning (6-11 AM):** Low risk (0.07-0.14%) - Safest transaction window\n",
        "- **Afternoon (12-15 PM):** Moderate risk (0.11-0.44%) - Gradual increase\n",
        "- **Evening Peak (16-19 PM):** **Critical risk zone**\n",
        "  - 16:00: 0.94% (10x morning baseline)\n",
        "  - 17:00: 1.77% (24x morning baseline)\n",
        "  - 18:00: 2.72% (36.85x morning baseline) âš ï¸ **HIGHEST RISK**\n",
        "  - 19:00: 2.10% (28x morning baseline)\n",
        "- **Night (20-23 PM):** Elevated risk (0.52-1.40%) - Remains above baseline\n",
        "- **Late Night (0-5 AM):** Low-moderate risk (0.08-0.13%) - Returns to baseline\n",
        "\n",
        "**Business Insight:** Fraudsters target **evening rush hours** (4-7 PM) when:\n",
        "- Transaction volume is high (merchant distraction)\n",
        "- Customer monitoring is lower (end of workday)\n",
        "- Detection delays are more likely\n",
        "\n",
        "#### 2. Day of Week Analysis (Section 7.2)\n",
        "\n",
        "**Key Findings:**\n",
        "- **Peak Risk Day:** Friday (0.7139%) - End of workweek\n",
        "- **Safest Day:** Monday (0.4575%) - Start of workweek\n",
        "- **Weekday vs Weekend:** **Weekdays are 1.15x riskier** (0.6071% vs 0.5276%)\n",
        "\n",
        "**Day-by-Day Breakdown:**\n",
        "- **Sunday:** 0.48% (Weekend - Moderate)\n",
        "- **Monday:** 0.46% (Safest weekday)\n",
        "- **Tuesday:** 0.59% (Mid-week increase)\n",
        "- **Wednesday:** 0.68% (Peak weekday)\n",
        "- **Thursday:** 0.68% (Peak weekday)\n",
        "- **Friday:** 0.71% (Highest risk - end of week)\n",
        "- **Saturday:** 0.59% (Weekend - Moderate)\n",
        "\n",
        "**Business Insight:** Contrary to common assumptions, **weekdays are riskier** than weekends. This suggests:\n",
        "- Fraudsters exploit higher transaction volumes during business days\n",
        "- Weekend transactions may have better monitoring/verification\n",
        "- Friday's peak aligns with end-of-week payment processing\n",
        "\n",
        "#### 3. Monthly/Seasonal Analysis (Section 7.3)\n",
        "\n",
        "**Seasonal Patterns:**\n",
        "- **Peak Risk Months:** \n",
        "  - February: 0.8658% (Highest)\n",
        "  - January: 0.8268% (Second highest)\n",
        "- **Lowest Risk Months:**\n",
        "  - July: 0.3834% (Safest)\n",
        "  - December: 0.4169% (Holiday season - surprisingly low)\n",
        "  - August: 0.4460%\n",
        "\n",
        "**Seasonal Trends:**\n",
        "- **Winter (Jan-Feb):** Highest fraud rates (0.83-0.87%)\n",
        "- **Spring (Mar-Jun):** Moderate risk (0.48-0.65%)\n",
        "- **Summer (Jul-Aug):** Lowest risk (0.38-0.45%)\n",
        "- **Fall (Sep-Nov):** Moderate risk (0.54-0.67%)\n",
        "\n",
        "**Business Insight:** \n",
        "- **Winter peak** may correlate with:\n",
        "  - Post-holiday financial stress\n",
        "  - Tax season preparation (Jan-Feb)\n",
        "  - Reduced merchant staffing\n",
        "- **Summer low** suggests:\n",
        "  - Lower transaction volumes\n",
        "  - Better merchant monitoring\n",
        "  - Different consumer behavior patterns\n",
        "\n",
        "#### 4. Time Bin Analysis (Section 7.5)\n",
        "\n",
        "**Risk Ranking by Time Period:**\n",
        "1. **Evening (18-23 PM):** 1.7871% âš ï¸ **HIGHEST RISK**\n",
        "2. **Afternoon (12-18 PM):** 0.5921% (Moderate)\n",
        "3. **Night (23-6 AM):** 0.1711% (Low)\n",
        "4. **Morning (6-12 PM):** 0.1179% (Safest)\n",
        "\n",
        "**Business Insight:** Evening transactions represent **15x higher risk** than morning transactions, confirming the hourly analysis findings.\n",
        "\n",
        "---\n",
        "\n",
        "### Statistical Significance Assessment\n",
        "\n",
        "**Strong Temporal Signals (Ready for Modeling):**\n",
        "- âœ… **Hour of Day:** 36.85x variation - **CRITICAL FEATURE**\n",
        "- âœ… **Time Bin:** 15x variation - **HIGH PRIORITY**\n",
        "- âœ… **Day of Week:** 1.55x variation (Friday vs Monday) - **MODERATE PRIORITY**\n",
        "- âœ… **Month/Season:** 2.26x variation (Feb vs Jul) - **MODERATE PRIORITY**\n",
        "- âœ… **Weekend Flag:** 1.15x variation - **LOW-MODERATE PRIORITY**\n",
        "\n",
        "**Feature Engineering Readiness:**\n",
        "- All temporal features show **statistically significant differences**\n",
        "- No missing data (100% coverage after timezone conversion)\n",
        "- Features are **mutually independent** and can be combined\n",
        "- **Ready for immediate use** in modeling pipeline\n",
        "\n",
        "---\n",
        "\n",
        "### Feature Enhancement Decision Framework\n",
        "\n",
        "#### Current Temporal Features (Sufficient for Initial Modeling)\n",
        "\n",
        "**Basic Features (Already Extracted):**\n",
        "- `hour` (0-23)\n",
        "- `day_of_week` (1-7)\n",
        "- `month` (1-12)\n",
        "- `time_bin` (Night/Morning/Afternoon/Evening)\n",
        "- `is_weekend` (0/1)\n",
        "\n",
        "**Assessment:** These features capture **primary temporal patterns** and are sufficient for initial model development.\n",
        "\n",
        "#### Potential Enhancements (Defer Until After Other Feature Analysis)\n",
        "\n",
        "**Recommended Approach:** **WAIT** to enhance temporal features until after analyzing:\n",
        "1. **Geographical features** (location-based patterns)\n",
        "2. **Population features** (city size, density)\n",
        "3. **Credit card features** (card age, transaction history)\n",
        "4. **Merchant features** (category, location)\n",
        "\n",
        "**Reasoning:**\n",
        "- Current temporal features already show **strong predictive power** (36.85x variation)\n",
        "- **Interaction effects** with other features may reveal more nuanced patterns:\n",
        "  - \"Evening + High-risk merchant category\" might be more predictive than evening alone\n",
        "  - \"Friday + Large city\" might show different patterns than Friday alone\n",
        "  - \"Winter + High transaction amount\" might reveal seasonal fraud strategies\n",
        "- **Feature engineering efficiency:** Create interaction features after understanding all dimensions\n",
        "- **Avoid over-engineering:** Current features are sufficient; enrich based on model performance\n",
        "\n",
        "**Future Enhancement Candidates (Post-Other-Features):**\n",
        "- `is_peak_hour` (16-19 PM flag)\n",
        "- `is_high_risk_day` (Wednesday-Friday flag)\n",
        "- `is_peak_season` (January-February flag)\n",
        "- `hour_sin/cos` (Cyclical encoding for hour)\n",
        "- `day_sin/cos` (Cyclical encoding for day of week)\n",
        "- `month_sin/cos` (Cyclical encoding for month)\n",
        "- Interaction features: `hour Ã— day_of_week`, `time_bin Ã— month`, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "#### 1. Dynamic Risk Scoring\n",
        "- **Time-based multipliers:**\n",
        "  - Evening (18-23 PM): 15x base risk\n",
        "  - Afternoon (12-18 PM): 5x base risk\n",
        "  - Morning (6-12 PM): 1x base risk (baseline)\n",
        "  - Night (23-6 AM): 1.5x base risk\n",
        "\n",
        "#### 2. Threshold Adjustments\n",
        "- **Hour-specific amount limits:**\n",
        "  - Evening hours: Lower approval thresholds\n",
        "  - Morning hours: Standard thresholds\n",
        "- **Day-specific monitoring:**\n",
        "  - Friday: Enhanced monitoring\n",
        "  - Wednesday-Thursday: Moderate monitoring\n",
        "  - Monday: Standard monitoring\n",
        "\n",
        "#### 3. Resource Allocation\n",
        "- **Focus monitoring resources on:**\n",
        "  - Evening hours (4-7 PM) - **Highest priority**\n",
        "  - Weekdays (especially Wednesday-Friday)\n",
        "  - Winter months (January-February)\n",
        "\n",
        "#### 4. Real-Time Alerts\n",
        "- **Immediate flagging for:**\n",
        "  - Transactions during peak risk hours (16-19 PM)\n",
        "  - Friday evening transactions\n",
        "  - January-February evening transactions\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**Immediate Actions:**\n",
        "1. âœ… **Temporal features are ready** - Proceed to other feature analysis\n",
        "2. âœ… **Document patterns** - Use findings for business rules\n",
        "3. â¸ï¸ **Defer temporal enhancements** - Wait for other feature analysis\n",
        "\n",
        "**Recommended Analysis Sequence:**\n",
        "1. **Geographical Analysis** (Section 8) - Location-based patterns\n",
        "2. **Population Analysis** (Section 9) - City size, density effects\n",
        "3. **Credit Card Analysis** (Section 10) - Card age, transaction history\n",
        "4. **Merchant Analysis** (Section 11) - Category, location patterns\n",
        "5. **Feature Interactions** (Section 12) - Cross-dimensional patterns\n",
        "6. **Temporal Enhancements** (If needed) - Based on interaction findings\n",
        "\n",
        "**Modeling Readiness:**\n",
        "- Temporal features: **âœ… READY**\n",
        "- Enhancement decision: **â¸ï¸ DEFER** (after other features)\n",
        "- Production deployment: **âœ… READY** (with current features)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.6.1: Save Section-Level Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:32.024653Z",
          "iopub.status.busy": "2026-01-24T00:55:32.024443Z",
          "iopub.status.idle": "2026-01-24T00:55:37.732716Z",
          "shell.execute_reply": "2026-01-24T00:55:37.73183Z",
          "shell.execute_reply.started": "2026-01-24T00:55:32.024634Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 7 (TEMPORAL FEATURES COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 7 adds\n",
        "required_columns_section7 = [\n",
        "    \"hour\",           # Created in Section 7.5\n",
        "    \"month\",          # Created in Section 7.3\n",
        "    \"time_bin\"        # Created in Section 7.5\n",
        "]\n",
        "\n",
        "# Verify all required columns exist\n",
        "missing_cols = [col for col in required_columns_section7 if col not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"âš ï¸  WARNING: Missing temporal columns: {missing_cols}\")\n",
        "    print(\"  Some temporal features may not have been created.\")\n",
        "    print(\"  Checkpoint will be saved with available columns.\")\n",
        "else:\n",
        "    print(\"âœ“ All Section 7 temporal columns present - saving section-level checkpoint...\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df, \n",
        "    CHECKPOINT_SECTION7, \n",
        "    \"Section 7 (Temporal Features Complete - Section-Level Checkpoint)\"\n",
        ")\n",
        "print(\"âœ“ Section 7 section-level checkpoint saved.\")\n",
        "print(\"  Note: Cell-level checkpoints also exist for granular control.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7.7 Category Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This section analyzes fraud patterns by transaction category (e.g., `gas_transport`, `grocery_pos`, etc.)\n",
        "to identify which categories have the highest fraud rates.\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Some categories may have higher fraud rates due to transaction characteristics\n",
        "- **Reasoning:** Different categories have different risk profiles (e.g., online vs. in-person)\n",
        "\n",
        "### What We'll Analyze\n",
        "1. Unique categories in the dataset\n",
        "2. Fraud count and rate by category\n",
        "3. Transaction volume by category\n",
        "4. Riskiest and safest categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7.1 Data Validation - Category Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:37.733716Z",
          "iopub.status.busy": "2026-01-24T00:55:37.733443Z",
          "iopub.status.idle": "2026-01-24T00:55:37.755492Z",
          "shell.execute_reply": "2026-01-24T00:55:37.754507Z",
          "shell.execute_reply.started": "2026-01-24T00:55:37.733696Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA VALIDATION: CATEGORY COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    raise ValueError(\"train_df not found. Please run previous sections first.\")\n",
        "\n",
        "# Check if category column exists\n",
        "if \"category\" not in train_df.columns:\n",
        "    raise ValueError(\"Column 'category' not found in train_df. Please check the dataset.\")\n",
        "\n",
        "# Validate category coverage\n",
        "category_validation = validate_column_coverage(train_df, \"category\", \"Category Analysis\")\n",
        "category_df, category_coverage, category_valid_count, category_total = category_validation\n",
        "\n",
        "# Show unique categories\n",
        "unique_categories = category_df.select(\"category\").distinct().orderBy(\"category\")\n",
        "category_count = unique_categories.count()\n",
        "print(f\"\\nâœ“ Unique categories: {category_count}\")\n",
        "print(\"\\nCategory list:\")\n",
        "unique_categories.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7.2 Category-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:37.756492Z",
          "iopub.status.busy": "2026-01-24T00:55:37.756234Z",
          "iopub.status.idle": "2026-01-24T00:55:38.858492Z",
          "shell.execute_reply": "2026-01-24T00:55:38.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:55:37.756470Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CATEGORY-BASED FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "category_fraud_stats = results_manager.load_dataframe(\"7.7\", \"category_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if category_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded category_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    category_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        category_df,\n",
        "        \"category\",\n",
        "        \"category\",\n",
        "        cache_name=\"category_fraud_stats\",\n",
        "        save_result=True, section=\"7.7\", result_name=\"category_fraud_stats\"\n",
        "    )\n",
        "\n",
        "category_fraud_stats = category_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CATEGORY FRAUD STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(category_fraud_stats.to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Key insights\n",
        "riskiest_category = category_fraud_stats.iloc[0]\n",
        "safest_category = category_fraud_stats.iloc[-1]\n",
        "overall_fraud_rate = (category_fraud_stats['fraud_count'].sum() / category_fraud_stats['total_txns'].sum()) * 100\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Riskiest category:  {riskiest_category['category']} ({riskiest_category['fraud_rate_pct']:.4f}% fraud rate, {riskiest_category['total_txns']:,} txns)\")\n",
        "print(f\"Safest category:    {safest_category['category']} ({safest_category['fraud_rate_pct']:.4f}% fraud rate, {safest_category['total_txns']:,} txns)\")\n",
        "print(f\"Overall fraud rate: {overall_fraud_rate:.4f}%\")\n",
        "print(f\"Risk ratio:         {riskiest_category['fraud_rate_pct'] / safest_category['fraud_rate_pct']:.2f}x\" if safest_category['fraud_rate_pct'] > 0 else \"Risk ratio:         N/A (safest has 0% fraud)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7.3 Category-Based Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:38.859492Z",
          "iopub.status.busy": "2026-01-24T00:55:38.859234Z",
          "iopub.status.idle": "2026-01-24T00:55:39.858492Z",
          "shell.execute_reply": "2026-01-24T00:55:39.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:55:38.859470Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CATEGORY-BASED FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        "\n",
        "# Chart 1: Fraud Rate by Category (Bar Chart)\n",
        "ax1 = axes[0]\n",
        "ax1.barh(range(len(category_fraud_stats)), category_fraud_stats['fraud_rate_pct'], \n",
        "        color='crimson', alpha=0.7)\n",
        "ax1.set_yticks(range(len(category_fraud_stats)))\n",
        "ax1.set_yticklabels(category_fraud_stats['category'])\n",
        "ax1.set_xlabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Fraud Rate by Category', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# Chart 2: Transaction Volume vs Fraud Rate (Scatter)\n",
        "ax2 = axes[1]\n",
        "scatter = ax2.scatter(category_fraud_stats['total_txns'], category_fraud_stats['fraud_rate_pct'], \n",
        "                     s=100, alpha=0.6, c=category_fraud_stats['fraud_rate_pct'], \n",
        "                     cmap='Reds', edgecolors='black', linewidth=1)\n",
        "ax2.set_xlabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Transaction Volume vs Fraud Rate by Category', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add category labels\n",
        "for idx, row in category_fraud_stats.iterrows():\n",
        "    ax2.annotate(row['category'], (row['total_txns'], row['fraud_rate_pct']), \n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.colorbar(scatter, ax=ax2, label='Fraud Rate (%)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('category_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Visualizations saved as 'category_fraud_patterns.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.7.4 Key Findings Summary\n",
        "\n",
        "**Riskiest category:** shopping_net (1.7561% fraud rate, 97,543 transactions)\n",
        "\n",
        "**Safest category:** health_fitness (0.1549% fraud rate, 85,879 transactions)\n",
        "\n",
        "**Risk ratio:** 11.34x higher for shopping_net vs. health_fitness\n",
        "\n",
        "**Overall fraud rate:** 0.5789%\n",
        "\n",
        "**Key patterns:**\n",
        "- Online shopping categories (shopping_net, misc_net) show highest fraud rates\n",
        "- In-person categories (health_fitness, home, food_dining) show lowest fraud rates\n",
        "- grocery_pos shows elevated risk (1.41%) despite being in-person\n",
        "\n",
        "**Next:** Section 8 - Geographical Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Geographical Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.0 Helper Functions for Geographical Analysis and Load Checkpoint\n",
        "\n",
        "### Design Principles\n",
        "\n",
        "- **DRY Pattern:** Reusable functions for all geographical analyses\n",
        "- **Distance Calculation:** Haversine formula for accurate lat/long distance\n",
        "- **Categorization:** Consistent binning for city size and distance\n",
        "- **Validation First:** Check data quality before aggregation\n",
        "- **Separation of Concerns:** Geographic patterns only (no temporal mixing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:37.734012Z",
          "iopub.status.busy": "2026-01-24T00:55:37.733656Z",
          "iopub.status.idle": "2026-01-24T00:55:37.878856Z",
          "shell.execute_reply": "2026-01-24T00:55:37.878111Z",
          "shell.execute_reply.started": "2026-01-24T00:55:37.733987Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT: SECTION 8 (REQUIRED FOR SECTION 9)\n",
        "# ============================================================\n",
        "\n",
        "# Section 8 can work with base data, but temporal features are nice to have\n",
        "# We'll check for base columns (always required) and temporal columns (optional)\n",
        "\n",
        "required_columns_section8_base = [\n",
        "    \"lat\",\n",
        "    \"long\", \n",
        "    \"merch_lat\",\n",
        "    \"merch_long\",\n",
        "    \"state\",\n",
        "    \"city\",\n",
        "    \"city_pop\"\n",
        "]\n",
        "\n",
        "# Try to load from Section 7 checkpoint (includes temporal features)\n",
        "df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_SECTION7,\n",
        "    required_columns=required_columns_section8_base,\n",
        "    cell_name=\"Section 7 (preferred for Section 8)\"\n",
        ")\n",
        "\n",
        "if loaded_from_checkpoint:\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint - ready for geographical analysis\")\n",
        "else:\n",
        "    # Fallback: Try Section 6 checkpoint\n",
        "    df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "        checkpoint_path=CHECKPOINT_SECTION6,\n",
        "        required_columns=required_columns_section8_base,\n",
        "        cell_name=\"Section 6 (fallback for Section 8)\"\n",
        "    )\n",
        "    \n",
        "    if loaded_from_checkpoint:\n",
        "        train_df = df_loaded\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint - ready for geographical analysis\")\n",
        "    else:\n",
        "        # Final fallback: Check if train_df exists in memory\n",
        "        if 'train_df' not in globals() or train_df is None:\n",
        "            raise ValueError(\n",
        "                \"train_df not found. Please run Section 6 (Timezone Resolution) first, \"\n",
        "                \"or ensure a checkpoint exists.\"\n",
        "            )\n",
        "        \n",
        "        # Validate required columns exist\n",
        "        missing_cols = [col for col in required_columns_section8_base if col not in train_df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(\n",
        "                f\"Required columns missing: {missing_cols}. \"\n",
        "                \"Please run Section 6 (Timezone Resolution) first.\"\n",
        "            )\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(\"âœ“ train_df already in memory - ready for geographical analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:37.879904Z",
          "iopub.status.busy": "2026-01-24T00:55:37.879698Z",
          "iopub.status.idle": "2026-01-24T00:55:37.891641Z",
          "shell.execute_reply": "2026-01-24T00:55:37.890314Z",
          "shell.execute_reply.started": "2026-01-24T00:55:37.879873Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS FOR GEOGRAPHICAL ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# Note: sqrt, sin, cos, radians, atan2, col, when are already imported globally\n",
        "# Re-importing here for clarity and cell self-containment\n",
        "\n",
        "def calculate_haversine_distance(lat1_col, lon1_col, lat2_col, lon2_col):\n",
        "    \"\"\"\n",
        "    Calculate Haversine distance between two lat/long points in kilometers.\n",
        "    \n",
        "    Formula: a = sinÂ²(Î”lat/2) + cos(lat1) * cos(lat2) * sinÂ²(Î”lon/2)\n",
        "             c = 2 * atan2(âˆša, âˆš(1âˆ’a))\n",
        "             d = R * c (where R = 6371 km)\n",
        "    \n",
        "    Args:\n",
        "        lat1_col: Column name or Column for first point latitude\n",
        "        lon1_col: Column name or Column for first point longitude\n",
        "        lat2_col: Column name or Column for second point latitude\n",
        "        lon2_col: Column name or Column for second point longitude\n",
        "    \n",
        "    Returns:\n",
        "        PySpark Column with distance in kilometers\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert to radians\n",
        "    lat1_rad = radians(col(lat1_col) if isinstance(lat1_col, str) else lat1_col)\n",
        "    lon1_rad = radians(col(lon1_col) if isinstance(lon1_col, str) else lon1_col)\n",
        "    lat2_rad = radians(col(lat2_col) if isinstance(lat2_col, str) else lat2_col)\n",
        "    lon2_rad = radians(col(lon2_col) if isinstance(lon2_col, str) else lon2_col)\n",
        "    \n",
        "    # Haversine formula\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    \n",
        "    a = (sin(dlat / 2) ** 2 + \n",
        "         cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2)\n",
        "    \n",
        "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "    \n",
        "    # Earth radius in kilometers\n",
        "    R = 6371.0\n",
        "    distance_km = R * c\n",
        "    \n",
        "    return distance_km\n",
        "\n",
        "\n",
        "def categorize_city_size(city_pop_col):\n",
        "    \"\"\"\n",
        "    Categorize city population into size bins.\n",
        "    \n",
        "    Args:\n",
        "        city_pop_col: Column name or Column for city population\n",
        "    \n",
        "    Returns:\n",
        "        PySpark Column with category string\n",
        "    \"\"\"\n",
        "    city_pop = col(city_pop_col) if isinstance(city_pop_col, str) else city_pop_col\n",
        "    \n",
        "    return when(city_pop < 10000, \"Small Town\") \\\n",
        "        .when((city_pop >= 10000) & (city_pop < 100000), \"Small City\") \\\n",
        "        .when((city_pop >= 100000) & (city_pop < 500000), \"Medium City\") \\\n",
        "        .when((city_pop >= 500000) & (city_pop < 1000000), \"Large City\") \\\n",
        "        .otherwise(\"Metropolitan\")\n",
        "\n",
        "\n",
        "def categorize_distance(distance_km_col):\n",
        "    \"\"\"\n",
        "    Categorize distance into bins for analysis.\n",
        "    \n",
        "    Args:\n",
        "        distance_km_col: Column name or Column for distance in kilometers\n",
        "    \n",
        "    Returns:\n",
        "        PySpark Column with category string\n",
        "    \"\"\"\n",
        "    distance = col(distance_km_col) if isinstance(distance_km_col, str) else distance_km_col\n",
        "    \n",
        "    return when(distance < 10, \"Local (<10km)\") \\\n",
        "        .when((distance >= 10) & (distance < 50), \"Regional (10-50km)\") \\\n",
        "        .when((distance >= 50) & (distance < 200), \"Statewide (50-200km)\") \\\n",
        "        .when((distance >= 200) & (distance < 1000), \"Long Distance (200-1000km)\") \\\n",
        "        .otherwise(\"Cross-Country (>1000km)\")\n",
        "\n",
        "\n",
        "def validate_geographic_coverage(df, lat_col, lon_col, analysis_name):\n",
        "    \"\"\"\n",
        "    Validate geographic data coverage before analysis.\n",
        "    \n",
        "    Args:\n",
        "        df: PySpark DataFrame\n",
        "        lat_col: Name of latitude column\n",
        "        lon_col: Name of longitude column\n",
        "        analysis_name: Description for logging\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (filtered_df, coverage_pct, valid_count, total_count)\n",
        "    \"\"\"\n",
        "    total_count = df.count()\n",
        "    filtered_df = df.filter(\n",
        "        col(lat_col).isNotNull() & \n",
        "        col(lon_col).isNotNull() &\n",
        "        (col(lat_col).between(-90, 90)) &\n",
        "        (col(lon_col).between(-180, 180))\n",
        "    )\n",
        "    valid_count = filtered_df.count()\n",
        "    coverage_pct = (valid_count / total_count) * 100\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(f\"DATA VALIDATION: {analysis_name}\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Total rows:      {total_count:,}\")\n",
        "    print(f\"Valid rows:      {valid_count:,}\")\n",
        "    print(f\"Coverage:        {coverage_pct:.2f}%\")\n",
        "    print(f\"Excluded (NULL/Invalid): {total_count - valid_count:,}\")\n",
        "    \n",
        "    if coverage_pct < 95:\n",
        "        print(f\"\\nâš ï¸  WARNING: Only {coverage_pct:.2f}% coverage - results may be biased\")\n",
        "    else:\n",
        "        print(f\"\\nâœ“ Coverage acceptable ({coverage_pct:.2f}%)\")\n",
        "    \n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    \n",
        "    return filtered_df, coverage_pct, valid_count, total_count\n",
        "\n",
        "\n",
        "print(\"âœ“ Helper functions loaded for geographical analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.1 Fraud Patterns by State\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Some states may have higher fraud rates due to:\n",
        "  - Population density\n",
        "  - Economic factors\n",
        "  - Regional fraud networks\n",
        "  - Merchant concentration\n",
        "- **Alternative:** Fraud is evenly distributed across states\n",
        "\n",
        "### What We'll Analyze\n",
        "- Fraud count and rate by US state\n",
        "- Transaction volume by state\n",
        "- Top and bottom states by fraud rate\n",
        "- State-level risk ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1.1 Data Validation - State Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:37.893123Z",
          "iopub.status.busy": "2026-01-24T00:55:37.892894Z",
          "iopub.status.idle": "2026-01-24T00:55:38.88027Z",
          "shell.execute_reply": "2026-01-24T00:55:38.87913Z",
          "shell.execute_reply.started": "2026-01-24T00:55:37.893104Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE STATE DATA COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "# Note: col, count, spark_sum, when are imported globally\n",
        "\n",
        "# Check state column coverage (use global constant to avoid recomputation)\n",
        "total_rows = TOTAL_DATASET_ROWS\n",
        "state_valid = train_df.filter(col(\"state\").isNotNull() & (trim(col(\"state\")) != \"\")).count()\n",
        "state_coverage = (state_valid / total_rows) * 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VALIDATION: State-Level Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows:      {total_rows:,}\")\n",
        "print(f\"Valid states:   {state_valid:,}\")\n",
        "print(f\"Coverage:       {state_coverage:.2f}%\")\n",
        "print(f\"Excluded:       {total_rows - state_valid:,}\")\n",
        "\n",
        "if state_coverage < 95:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {state_coverage:.2f}% coverage - results may be biased\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Coverage acceptable ({state_coverage:.2f}%)\")\n",
        "\n",
        "# Show unique states count\n",
        "unique_states = train_df.select(\"state\").distinct().filter(col(\"state\").isNotNull()).count()\n",
        "print(f\"Unique states:  {unique_states}\")\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1.2 State-Level Fraud Aggregation\n",
        "\n",
        "Using the same helper function pattern from Section 7 for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:38.885128Z",
          "iopub.status.busy": "2026-01-24T00:55:38.88489Z",
          "iopub.status.idle": "2026-01-24T00:55:39.341835Z",
          "shell.execute_reply": "2026-01-24T00:55:39.340859Z",
          "shell.execute_reply.started": "2026-01-24T00:55:38.885111Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STATE-LEVEL FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "state_fraud_stats = results_manager.load_dataframe(\"8.1\", \"state_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if state_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded state_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    state_analysis_df = train_df.filter(\n",
        "        col(\"state\").isNotNull() & (trim(col(\"state\")) != \"\")\n",
        "    )\n",
        "    state_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=state_analysis_df,\n",
        "        dimension_col=\"state\",\n",
        "        dimension_name=\"State\",\n",
        "        cache_name=\"cached_state_fraud_stats\",\n",
        "        save_result=True, section=\"8.1\", result_name=\"state_fraud_stats\"\n",
        "    )\n",
        "\n",
        "state_fraud_stats = state_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY STATE\")\n",
        "print(\"=\" * 100)\n",
        "print(state_fraud_stats[['state', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "top_state = state_fraud_stats.iloc[0]\n",
        "bottom_state = state_fraud_stats.iloc[-1]\n",
        "risk_ratio = top_state['fraud_rate_pct'] / bottom_state['fraud_rate_pct']\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Highest risk state: {top_state['state']} ({top_state['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Lowest risk state:  {bottom_state['state']} ({bottom_state['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:         {risk_ratio:.2f}x difference\")\n",
        "print(f\"Total states:       {len(state_fraud_stats)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1.3 State-Level Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:39.342988Z",
          "iopub.status.busy": "2026-01-24T00:55:39.342734Z",
          "iopub.status.idle": "2026-01-24T00:55:40.237059Z",
          "shell.execute_reply": "2026-01-24T00:55:40.236166Z",
          "shell.execute_reply.started": "2026-01-24T00:55:39.342962Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STATE-LEVEL FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Prepare data for visualization (top 15 states by fraud rate)\n",
        "top_states = state_fraud_stats.head(15)\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Chart 1: Fraud Rate by State (Top 15)\n",
        "ax1 = axes[0]\n",
        "ax1.barh(range(len(top_states)), top_states['fraud_rate_pct'], color='crimson', alpha=0.7)\n",
        "ax1.set_yticks(range(len(top_states)))\n",
        "ax1.set_yticklabels(top_states['state'])\n",
        "ax1.set_xlabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Top 15 States by Fraud Rate', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='x', alpha=0.3)\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# Chart 2: Transaction Volume vs Fraud Rate\n",
        "ax2 = axes[1]\n",
        "scatter = ax2.scatter(top_states['total_txns'], top_states['fraud_rate_pct'], \n",
        "                     s=100, alpha=0.6, c=top_states['fraud_rate_pct'], \n",
        "                     cmap='Reds', edgecolors='black', linewidth=1)\n",
        "ax2.set_xlabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Transaction Volume vs Fraud Rate (Top 15 States)', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Add state labels\n",
        "for idx, row in top_states.iterrows():\n",
        "    ax2.annotate(row['state'], (row['total_txns'], row['fraud_rate_pct']), \n",
        "                fontsize=8, alpha=0.7)\n",
        "\n",
        "plt.colorbar(scatter, ax=ax2, label='Fraud Rate (%)')\n",
        "plt.tight_layout()\n",
        "plt.savefig('state_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Visualizations saved as 'state_fraud_patterns.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1.4 Key Findings Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2 City Population Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Larger cities may have higher fraud rates due to:\n",
        "  - Higher transaction volumes\n",
        "  - More anonymity\n",
        "  - Diverse merchant base\n",
        "- **Alternative:** Smaller cities may be riskier due to less monitoring\n",
        "\n",
        "### What We'll Analyze\n",
        "- Fraud rate by city population bins (Small Town, Small City, Medium City, Large City, Metropolitan)\n",
        "- Transaction volume by city size\n",
        "- Urban vs rural fraud patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2.1 Data Validation - City Population Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:40.238204Z",
          "iopub.status.busy": "2026-01-24T00:55:40.237964Z",
          "iopub.status.idle": "2026-01-24T00:55:40.61345Z",
          "shell.execute_reply": "2026-01-24T00:55:40.612393Z",
          "shell.execute_reply.started": "2026-01-24T00:55:40.238177Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE CITY POPULATION DATA COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "# Note: col, count are imported globally\n",
        "\n",
        "# Check city_pop column coverage (use global constant to avoid recomputation)\n",
        "total_rows = TOTAL_DATASET_ROWS\n",
        "city_pop_valid = train_df.filter(col(\"city_pop\").isNotNull() & (col(\"city_pop\") > 0)).count()\n",
        "city_pop_coverage = (city_pop_valid / total_rows) * 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VALIDATION: City Population Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows:      {total_rows:,}\")\n",
        "print(f\"Valid city_pop: {city_pop_valid:,}\")\n",
        "print(f\"Coverage:       {city_pop_coverage:.2f}%\")\n",
        "print(f\"Excluded:       {total_rows - city_pop_valid:,}\")\n",
        "\n",
        "if city_pop_coverage < 95:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {city_pop_coverage:.2f}% coverage - results may be biased\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Coverage acceptable ({city_pop_coverage:.2f}%)\")\n",
        "\n",
        "# Show city_pop statistics\n",
        "city_pop_stats = train_df.select(\"city_pop\").filter(col(\"city_pop\").isNotNull()).describe()\n",
        "print(\"\\nCity Population Statistics:\")\n",
        "city_pop_stats.show()\n",
        "print(\"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2.2 City Size Categorization & Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:40.615064Z",
          "iopub.status.busy": "2026-01-24T00:55:40.614715Z",
          "iopub.status.idle": "2026-01-24T00:55:48.170499Z",
          "shell.execute_reply": "2026-01-24T00:55:48.169664Z",
          "shell.execute_reply.started": "2026-01-24T00:55:40.615045Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CITY SIZE CATEGORIZATION & FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "def add_city_size_feature(df):\n",
        "    \"\"\"\n",
        "    Add city_size category based on city_pop.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with city_size column\n",
        "    \"\"\"\n",
        "    if \"city_size\" in df.columns:\n",
        "        print(\"âš ï¸ city_size column already exists, skipping\")\n",
        "        return df\n",
        "    \n",
        "    # Filter valid city_pop\n",
        "    df = df.withColumn(\n",
        "        \"city_size\",\n",
        "        categorize_city_size(\"city_pop\")\n",
        "    )\n",
        "    print(\"âœ“ Created city_size category column\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"8\", \"2_2\", \"city_size\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: add_city_size_feature(train_df),\n",
        "    required_columns=[\"city_size\"],\n",
        "    cell_name=\"Section 8.2.2 (City Size Categorization)\"\n",
        ")\n",
        "\n",
        "# Aggregation: try load first, else compute\n",
        "city_size_fraud_stats = results_manager.load_dataframe(\"8.2\", \"city_size_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if city_size_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded city_size_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    city_analysis_df = train_df.filter(col(\"city_pop\").isNotNull() & (col(\"city_pop\") > 0))\n",
        "    city_size_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=city_analysis_df,\n",
        "        dimension_col=\"city_size\",\n",
        "        dimension_name=\"City Size\",\n",
        "        cache_name=\"cached_city_size_fraud_stats\",\n",
        "        save_result=True, section=\"8.2\", result_name=\"city_size_fraud_stats\"\n",
        "    )\n",
        "\n",
        "city_size_fraud_stats = city_size_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY CITY SIZE\")\n",
        "print(\"=\" * 100)\n",
        "print(city_size_fraud_stats[['city_size', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "top_size = city_size_fraud_stats.iloc[0]\n",
        "bottom_size = city_size_fraud_stats.iloc[-1]\n",
        "risk_ratio = top_size['fraud_rate_pct'] / bottom_size['fraud_rate_pct'] if bottom_size['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Highest risk: {top_size['city_size']} ({top_size['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Lowest risk:  {bottom_size['city_size']} ({bottom_size['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "if risk_ratio != float('inf'):\n",
        "    print(f\"Risk ratio:   {risk_ratio:.2f}x difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2.3 Key Findings Summary\n",
        "\n",
        "**Next:** Section 8.3 - Customer-Merchant Distance Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 Customer-Merchant Distance Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Long-distance transactions are riskier because:\n",
        "  - Unusual for customer location\n",
        "  - May indicate stolen card usage\n",
        "  - Geographic anomalies are fraud signals\n",
        "- **Alternative:** Distance doesn't correlate with fraud\n",
        "\n",
        "### What We'll Analyze\n",
        "- Distance between customer and merchant locations (Haversine formula)\n",
        "- Fraud rate by distance bins\n",
        "- Long-distance transaction patterns\n",
        "- Distance as fraud predictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3.1 Data Validation - Geographic Coordinates Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:48.171755Z",
          "iopub.status.busy": "2026-01-24T00:55:48.171508Z",
          "iopub.status.idle": "2026-01-24T00:55:48.8471Z",
          "shell.execute_reply": "2026-01-24T00:55:48.846362Z",
          "shell.execute_reply.started": "2026-01-24T00:55:48.17173Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE GEOGRAPHIC COORDINATES FOR DISTANCE CALCULATION\n",
        "# ============================================================\n",
        "\n",
        "# Validate both customer and merchant coordinates\n",
        "distance_analysis_df, coverage_pct, valid_count, total_count = validate_geographic_coverage(\n",
        "    df=train_df,\n",
        "    lat_col=\"lat\",\n",
        "    lon_col=\"long\",\n",
        "    analysis_name=\"Customer Coordinates for Distance Analysis\"\n",
        ")\n",
        "\n",
        "# Also validate merchant coordinates\n",
        "merchant_valid = distance_analysis_df.filter(\n",
        "    col(\"merch_lat\").isNotNull() & \n",
        "    col(\"merch_long\").isNotNull() &\n",
        "    (col(\"merch_lat\").between(-90, 90)) &\n",
        "    (col(\"merch_long\").between(-180, 180))\n",
        ").count()\n",
        "\n",
        "merchant_coverage = (merchant_valid / valid_count) * 100 if valid_count > 0 else 0\n",
        "\n",
        "print(f\"Merchant coordinates valid: {merchant_valid:,} / {valid_count:,} ({merchant_coverage:.2f}%)\")\n",
        "print()\n",
        "\n",
        "# Filter to rows with both valid customer and merchant coordinates\n",
        "distance_analysis_df = distance_analysis_df.filter(\n",
        "    col(\"merch_lat\").isNotNull() & \n",
        "    col(\"merch_long\").isNotNull() &\n",
        "    (col(\"merch_lat\").between(-90, 90)) &\n",
        "    (col(\"merch_long\").between(-180, 180))\n",
        ")\n",
        "\n",
        "final_valid = distance_analysis_df.count()\n",
        "final_coverage = (final_valid / total_count) * 100\n",
        "\n",
        "print(f\"Final coverage (both coordinates valid): {final_valid:,} / {total_count:,} ({final_coverage:.2f}%)\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3.2 Distance Calculation & Categorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:48.84836Z",
          "iopub.status.busy": "2026-01-24T00:55:48.848133Z",
          "iopub.status.idle": "2026-01-24T00:55:58.10635Z",
          "shell.execute_reply": "2026-01-24T00:55:58.105606Z",
          "shell.execute_reply.started": "2026-01-24T00:55:48.848339Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CALCULATE CUSTOMER-MERCHANT DISTANCE & CATEGORIZATION\n",
        "# ============================================================\n",
        "\n",
        "def add_distance_features(df):\n",
        "    \"\"\"\n",
        "    Calculate customer-merchant distance and create distance categories.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with customer_merchant_distance_km and distance_category columns\n",
        "    \"\"\"\n",
        "    # Calculate distance if not exists\n",
        "    if \"customer_merchant_distance_km\" not in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"customer_merchant_distance_km\",\n",
        "            calculate_haversine_distance(\"lat\", \"long\", \"merch_lat\", \"merch_long\")\n",
        "        )\n",
        "        print(\"âœ“ Calculated customer-merchant distance using Haversine formula\")\n",
        "    else:\n",
        "        print(\"âœ“ customer_merchant_distance_km column already exists\")\n",
        "    \n",
        "    # Create distance category if not exists\n",
        "    if \"distance_category\" not in df.columns:\n",
        "        df = df.withColumn(\n",
        "            \"distance_category\",\n",
        "            categorize_distance(\"customer_merchant_distance_km\")\n",
        "        )\n",
        "        print(\"âœ“ Created distance_category column\")\n",
        "    else:\n",
        "        print(\"âœ“ distance_category column already exists\")\n",
        "    \n",
        "    # IMPORTANT: Filter AFTER columns are added, and ensure columns are in the filter\n",
        "    # Create df_valid from the UPDATED df (with new columns)\n",
        "    df_valid = df.filter(\n",
        "        col(\"lat\").isNotNull() & \n",
        "        col(\"long\").isNotNull() &\n",
        "        col(\"merch_lat\").isNotNull() & \n",
        "        col(\"merch_long\").isNotNull() &\n",
        "        (col(\"lat\").between(-90, 90)) &\n",
        "        (col(\"long\").between(-180, 180)) &\n",
        "        (col(\"merch_lat\").between(-90, 90)) &\n",
        "        (col(\"merch_long\").between(-180, 180)) &\n",
        "        col(\"customer_merchant_distance_km\").isNotNull()  # This ensures column exists\n",
        "    )\n",
        "    \n",
        "    # Show sample distances (now df_valid has the columns)\n",
        "    print(\"\\nSample distance calculations:\")\n",
        "    df_valid.select(\n",
        "        \"lat\", \"long\", \"merch_lat\", \"merch_long\", \n",
        "        \"customer_merchant_distance_km\", \"distance_category\"\n",
        "    ).show(5, truncate=False)\n",
        "    \n",
        "    # Distance statistics (using single-pass aggregation for efficiency)\n",
        "    distance_stats = df_valid.agg(\n",
        "        spark_min(\"customer_merchant_distance_km\").alias(\"min_km\"),\n",
        "        spark_max(\"customer_merchant_distance_km\").alias(\"max_km\"),\n",
        "        avg(\"customer_merchant_distance_km\").alias(\"avg_km\"),\n",
        "        stddev(\"customer_merchant_distance_km\").alias(\"stddev_km\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(\"\\nDistance Statistics (km):\")\n",
        "    print(f\"  Min:    {distance_stats['min_km']:.2f}\")\n",
        "    print(f\"  Max:    {distance_stats['max_km']:.2f}\")\n",
        "    print(f\"  Mean:   {distance_stats['avg_km']:.2f}\")\n",
        "    print(f\"  StdDev: {distance_stats['stddev_km']:.2f}\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Use CheckpointManager\n",
        "cell_path = checkpoint_manager.get_cell_checkpoint_path(\"8\", \"3_2\", \"distance_features\")\n",
        "train_df = checkpoint_manager.load_or_compute(\n",
        "    checkpoint_path=cell_path,\n",
        "    compute_func=lambda: add_distance_features(train_df),\n",
        "    required_columns=[\"customer_merchant_distance_km\", \"distance_category\"],\n",
        "    cell_name=\"Section 8.3.2 (Distance Calculation & Categorization)\"\n",
        ")\n",
        "\n",
        "# Continue with distance-based aggregation (this doesn't modify train_df, so no checkpoint needed)\n",
        "# Filter to rows with valid distances\n",
        "distance_analysis_df = train_df.filter(\n",
        "    col(\"customer_merchant_distance_km\").isNotNull()\n",
        ")\n",
        "\n",
        "distance_fraud_stats = aggregate_fraud_by_dimension(\n",
        "    df=distance_analysis_df,\n",
        "    dimension_col=\"distance_category\",\n",
        "    dimension_name=\"Distance Category\",\n",
        "    cache_name=\"cached_distance_fraud_stats\",\n",
        "    save_result=True, section=\"8.3\", result_name=\"distance_fraud_stats\"\n",
        ")\n",
        "\n",
        "# Sort by fraud rate descending\n",
        "distance_fraud_stats = distance_fraud_stats.sort_values('fraud_rate_pct', ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY DISTANCE CATEGORY\")\n",
        "print(\"=\" * 100)\n",
        "print(distance_fraud_stats[['distance_category', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "top_distance = distance_fraud_stats.iloc[0]\n",
        "bottom_distance = distance_fraud_stats.iloc[-1]\n",
        "risk_ratio = top_distance['fraud_rate_pct'] / bottom_distance['fraud_rate_pct'] if bottom_distance['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Highest risk: {top_distance['distance_category']} ({top_distance['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Lowest risk:  {bottom_distance['distance_category']} ({bottom_distance['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "if risk_ratio != float('inf'):\n",
        "    print(f\"Risk ratio:   {risk_ratio:.2f}x difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3.3 Distance-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:58.107314Z",
          "iopub.status.busy": "2026-01-24T00:55:58.107122Z",
          "iopub.status.idle": "2026-01-24T00:55:58.116628Z",
          "shell.execute_reply": "2026-01-24T00:55:58.115934Z",
          "shell.execute_reply.started": "2026-01-24T00:55:58.107297Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FRAUD AGGREGATION BY DISTANCE CATEGORY\n",
        "# ============================================================\n",
        "\n",
        "distance_fraud_stats = results_manager.load_dataframe(\"8.3\", \"distance_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if distance_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded distance_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    distance_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=distance_analysis_df,\n",
        "        dimension_col=\"distance_category\",\n",
        "        dimension_name=\"Distance Category\",\n",
        "        cache_name=\"cached_distance_fraud_stats\",\n",
        "        save_result=True, section=\"8.3\", result_name=\"distance_fraud_stats\"\n",
        "    )\n",
        "\n",
        "distance_fraud_stats = distance_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY DISTANCE CATEGORY\")\n",
        "print(\"=\" * 100)\n",
        "print(distance_fraud_stats[['distance_category', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "top_distance = distance_fraud_stats.iloc[0]\n",
        "bottom_distance = distance_fraud_stats.iloc[-1]\n",
        "risk_ratio = top_distance['fraud_rate_pct'] / bottom_distance['fraud_rate_pct'] if bottom_distance['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Highest risk: {top_distance['distance_category']} ({top_distance['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Lowest risk:  {bottom_distance['distance_category']} ({bottom_distance['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "if risk_ratio != float('inf'):\n",
        "    print(f\"Risk ratio:   {risk_ratio:.2f}x difference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3.4 Key Findings Summary\n",
        "\n",
        "**Next:** Section 8.4 - Geographic Hotspots Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.4 Geographic Hotspots Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Certain geographic locations (cities, states, merchant locations) may be fraud hotspots\n",
        "- **Purpose:** Identify high-risk locations for targeted monitoring\n",
        "\n",
        "### What We'll Analyze\n",
        "- Top fraud cities (by fraud rate and count)\n",
        "- High-risk merchant locations\n",
        "- Geographic clusters of fraud\n",
        "- Cross-dimensional patterns (state + city size, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4.1 Top Fraud Cities Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# TOP FRAUD CITIES ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# Filter valid cities (with minimum transaction threshold for statistical significance)\n",
        "# Note: trim is imported globally\n",
        "\n",
        "city_analysis_df = train_df.filter(\n",
        "    col(\"city\").isNotNull() & \n",
        "    (trim(col(\"city\")) != \"\") &\n",
        "    col(\"state\").isNotNull() &\n",
        "    (trim(col(\"state\")) != \"\")\n",
        ")\n",
        "\n",
        "# Aggregate by city (with state for disambiguation)\n",
        "city_fraud_stats = aggregate_fraud_by_dimension(\n",
        "    df=city_analysis_df,\n",
        "    dimension_col=\"city\",\n",
        "    dimension_name=\"City\",\n",
        "    cache_name=\"cached_city_fraud_stats\",\n",
        "    save_result=True, section=\"8.4\", result_name=\"city_fraud_stats\"\n",
        ")\n",
        "\n",
        "# Filter cities with minimum transaction volume for statistical significance (e.g., >100 transactions)\n",
        "min_txns = 100\n",
        "city_fraud_stats_filtered = city_fraud_stats[city_fraud_stats['total_txns'] >= min_txns]\n",
        "\n",
        "# Sort by fraud rate descending\n",
        "city_fraud_stats_filtered = city_fraud_stats_filtered.sort_values('fraud_rate_pct', ascending=False)\n",
        "\n",
        "# Display top 20 cities\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(f\"TOP 20 CITIES BY FRAUD RATE (Minimum {min_txns} transactions)\")\n",
        "print(\"=\" * 100)\n",
        "print(city_fraud_stats_filtered.head(20)[['city', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "if len(city_fraud_stats_filtered) > 0:\n",
        "    top_city = city_fraud_stats_filtered.iloc[0]\n",
        "    print(f\"\\nðŸ“Š Highest risk city: {top_city['city']} ({top_city['fraud_rate_pct']:.4f}% fraud rate, {int(top_city['total_txns']):,} transactions)\")\n",
        "    print(f\"Total cities analyzed: {len(city_fraud_stats_filtered)} (with â‰¥{min_txns} transactions)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4.2 High-Risk Merchant Locations\n",
        "\n",
        "Analyzing merchant locations (merch_lat, merch_long) to identify geographic hotspots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HIGH-RISK MERCHANT LOCATIONS ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "# Note: round (as spark_round), concat, lit are imported globally\n",
        "\n",
        "# Validate merchant coordinates\n",
        "merchant_geo_df, merchant_coverage, merchant_valid, merchant_total = validate_geographic_coverage(\n",
        "    df=train_df,\n",
        "    lat_col=\"merch_lat\",\n",
        "    lon_col=\"merch_long\",\n",
        "    analysis_name=\"Merchant Location Analysis\"\n",
        ")\n",
        "\n",
        "# Aggregate by merchant location (rounded to reduce granularity)\n",
        "# Round to 2 decimal places (~1km precision) to group nearby merchants\n",
        "merchant_geo_df = merchant_geo_df.withColumn(\n",
        "    \"merch_lat_rounded\",\n",
        "    round(col(\"merch_lat\"), 2)\n",
        ").withColumn(\n",
        "    \"merch_long_rounded\",\n",
        "    round(col(\"merch_long\"), 2)\n",
        ").withColumn(\n",
        "    \"merch_location_key\",\n",
        "    concat(col(\"merch_lat_rounded\"), lit(\"_\"), col(\"merch_long_rounded\"))\n",
        ")\n",
        "\n",
        "# Aggregate by merchant location key (both lat and long)\n",
        "merchant_location_stats = aggregate_fraud_by_dimension(\n",
        "    df=merchant_geo_df,\n",
        "    dimension_col=\"merch_location_key\",\n",
        "    dimension_name=\"Merchant Location\",\n",
        "    cache_name=\"cached_merchant_location_stats\",\n",
        "    save_result=True, section=\"8.4\", result_name=\"merchant_location_stats\"\n",
        ")\n",
        "\n",
        "# Sort by fraud rate\n",
        "merchant_location_stats = merchant_location_stats.sort_values('fraud_rate_pct', ascending=False)\n",
        "\n",
        "# Display top 20 merchant locations\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"TOP 20 MERCHANT LOCATIONS BY FRAUD RATE\")\n",
        "print(\"=\" * 100)\n",
        "print(merchant_location_stats.head(20)[['merch_location_key', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "if len(merchant_location_stats) > 0:\n",
        "    top_location = merchant_location_stats.iloc[0]\n",
        "    print(f\"\\nðŸ“Š Highest risk merchant location: {top_location['merch_location_key']} ({top_location['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "    print(f\"Total location groups: {len(merchant_location_stats)}\")\n",
        "    print(f\"Coverage: {merchant_coverage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4.3 Key Findings Summary\n",
        "\n",
        "#### What we learned from hotspots\n",
        "\n",
        "**1) City-level hotspots are real and actionable (with a minimum-volume threshold).**\n",
        "- We analyzed **836 cities** with **â‰¥100 transactions** to reduce small-sample noise.\n",
        "- Top cities show **materially elevated fraud rates** (e.g., **Aurora ~4.49%**, **Clearwater ~4.34%**) vs. baseline (~0.5â€“0.7% range in most aggregates).\n",
        "- **Interpretation:** Location risk is not uniform; there are pockets of higher fraud exposure that are worth encoding as features.\n",
        "\n",
        "**2) Merchant â€œlat/long hotspotsâ€ at ~1km resolution are dominated by statistical noise.**\n",
        "- Grouping merchants by rounded coordinates produced **~1.12M location groups** from **~1.30M transactions**, meaning most groups have **Nâ‰ˆ1**.\n",
        "- The â€œtop 20â€ merchant locations show **100% fraud**, but each is based on **1 transaction** â†’ not meaningful.\n",
        "- **Conclusion:** Merchant location granularity is too fine for hotspot detection without additional aggregation and minimum support.\n",
        "\n",
        "#### Quality / significance guardrails (senior-level checks)\n",
        "\n",
        "- **Small-N filter is mandatory** for any â€œtop risk locationâ€ list.\n",
        "  - City analysis already applies **min_txns = 100** âœ…\n",
        "  - Merchant-location analysis should apply something like **min_txns â‰¥ 50â€“200** (and/or coarser spatial binning) before interpreting results.\n",
        "- **Outlier awareness:** Extremely high rates with tiny counts should be treated as noise (good candidate for smoothing or shrinkage).\n",
        "\n",
        "#### Modeling implications (what to keep vs. defer)\n",
        "\n",
        "**High-value features to keep:**\n",
        "- **State** (but handle tiny-count states as outliers)\n",
        "- **City** (with frequency encoding / target encoding using out-of-fold strategy)\n",
        "- Potentially **location risk score** computed with smoothing (e.g., Bayesian / empirical Bayes)\n",
        "\n",
        "**Defer / redesign:**\n",
        "- Merchant coordinate hotspots at 1km precision â†’ redesign with:\n",
        "  - coarser bins (e.g., 0.05â€“0.1 degrees or geohash),\n",
        "  - minimum support threshold,\n",
        "  - smoothed fraud-rate estimate to reduce variance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.5 Geographical Analysis Summary & Conclusions\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "Geographical analysis reveals **significant state-level variation** (504x risk ratio, though driven by outlier) and **actionable city-level hotspots** (Aurora: 4.49% vs 0.58% baseline). However, **city size** and **distance** show minimal predictive power (1.22x and 1.16x variation respectively), suggesting these features may have limited standalone value for modeling.\n",
        "\n",
        "**Key Decision:** Prioritize **state** and **city** features for immediate feature engineering. Defer or redesign **city_size**, **distance_category**, and **merchant location** features pending further validation or alternative approaches.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Findings by Dimension\n",
        "\n",
        "#### 1. State-Level Patterns (Section 8.1)\n",
        "\n",
        "**Statistical Significance:** âœ… **STRONG** (with outlier handling)\n",
        "\n",
        "**Results:**\n",
        "- **Highest Risk:** Delaware (DE) - 100.00% fraud rate âš ï¸ **OUTLIER** (only 9 transactions)\n",
        "- **Highest Risk (Valid):** Rhode Island (RI) - 2.73% fraud rate (550 transactions)\n",
        "- **Lowest Risk:** Idaho (ID) - 0.20% fraud rate (5,545 transactions)\n",
        "- **Risk Ratio:** 504x (with DE outlier) / ~8x (excluding DE)\n",
        "- **Coverage:** 100% (all 51 states/territories represented)\n",
        "\n",
        "**Business Insights:**\n",
        "- **Regional fraud networks** may exist (RI, AK, NV show elevated rates)\n",
        "- **Small states** (DE, RI) show high variance due to low transaction volume\n",
        "- **State-level risk scoring** is viable but requires **outlier handling** for low-volume states\n",
        "\n",
        "**Feature Engineering Recommendation:**\n",
        "- âœ… **Include:** State as categorical feature (with frequency/target encoding)\n",
        "- âš ï¸ **Handle:** Low-volume states (<100 transactions) as special category or apply smoothing\n",
        "- ðŸ“Š **Priority:** **HIGH** - Strong signal, actionable for risk scoring\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. City Population Patterns (Section 8.2)\n",
        "\n",
        "**Statistical Significance:** âš ï¸ **WEAK** (1.22x variation)\n",
        "\n",
        "**Results:**\n",
        "- **Highest Risk:** Medium City (100K-500K) - 0.68% fraud rate\n",
        "- **Lowest Risk:** Small City (10K-100K) - 0.55% fraud rate\n",
        "- **Risk Ratio:** 1.22x difference\n",
        "- **Coverage:** 100% (all transactions have city_pop)\n",
        "\n",
        "**Business Insights:**\n",
        "- **Minimal variation** across city size categories\n",
        "- Urban vs rural distinction shows **no strong fraud pattern**\n",
        "- City size alone is **not a strong predictor**\n",
        "\n",
        "**Feature Engineering Recommendation:**\n",
        "- âš ï¸ **Defer:** City size as standalone feature (too weak)\n",
        "- ðŸ’¡ **Consider:** Interaction features (city_size Ã— state, city_size Ã— distance)\n",
        "- ðŸ“Š **Priority:** **LOW** - Weak signal, may be useful in interactions only\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Distance Patterns (Section 8.3)\n",
        "\n",
        "**Statistical Significance:** âš ï¸ **WEAK** (1.16x variation)\n",
        "\n",
        "**Results:**\n",
        "- **Highest Risk:** Statewide (50-200km) - 0.58% fraud rate\n",
        "- **Lowest Risk:** Local (<10km) - 0.50% fraud rate\n",
        "- **Risk Ratio:** 1.16x difference\n",
        "- **Coverage:** 100% (all transactions have valid coordinates)\n",
        "\n",
        "**Business Insights:**\n",
        "- **Distance shows minimal predictive power** as standalone feature\n",
        "- Long-distance transactions are **not significantly riskier** than local\n",
        "- Hypothesis (long-distance = riskier) **not strongly supported**\n",
        "\n",
        "**Feature Engineering Recommendation:**\n",
        "- âš ï¸ **Defer:** Distance category as standalone feature (too weak)\n",
        "- ðŸ’¡ **Consider:** \n",
        "  - Continuous distance feature (km) instead of bins\n",
        "  - Interaction features (distance Ã— state, distance Ã— time_of_day)\n",
        "  - Anomaly detection (distance > customer's typical range)\n",
        "- ðŸ“Š **Priority:** **LOW-MODERATE** - Weak standalone, may be useful in interactions or anomaly detection\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. Geographic Hotspots (Section 8.4)\n",
        "\n",
        "**Statistical Significance:** âœ… **STRONG** (city-level) / âŒ **NOISE** (merchant-level)\n",
        "\n",
        "**City-Level Results:**\n",
        "- **Highest Risk City:** Aurora - 4.49% fraud rate (512 transactions)\n",
        "- **Top 20 Cities:** Fraud rates range from 2.61% to 4.49%\n",
        "- **Statistical Threshold:** â‰¥100 transactions (836 cities analyzed)\n",
        "- **Coverage:** 100% (all transactions have city/state)\n",
        "\n",
        "**Merchant Location Results:**\n",
        "- **Analysis Method:** Rounded coordinates (~1km precision)\n",
        "- **Location Groups:** 1.12M groups from 1.30M transactions\n",
        "- **Problem:** Most locations have N=1 transaction (statistical noise)\n",
        "- **Top \"hotspots\":** 100% fraud rate but based on single transactions\n",
        "\n",
        "**Business Insights:**\n",
        "- **City-level hotspots are real and actionable** (Aurora, Clearwater show 7-8x baseline risk)\n",
        "- **Merchant location granularity too fine** for meaningful pattern detection\n",
        "- **Location risk is not uniform** - specific cities warrant targeted monitoring\n",
        "\n",
        "**Feature Engineering Recommendation:**\n",
        "- âœ… **Include:** City name as categorical feature (with target encoding, out-of-fold)\n",
        "- âœ… **Include:** High-risk city flag (binary: fraud_rate > 2x baseline)\n",
        "- âš ï¸ **Defer:** Merchant coordinate hotspots (redesign needed)\n",
        "- ðŸ’¡ **Future Enhancement:** \n",
        "  - Coarser merchant location bins (0.05-0.1 degrees or geohash)\n",
        "  - Minimum support threshold (â‰¥50-200 transactions per location)\n",
        "  - Smoothed fraud rate estimates (Bayesian/empirical Bayes)\n",
        "- ðŸ“Š **Priority:** **HIGH** (city-level) / **DEFER** (merchant-level)\n",
        "\n",
        "---\n",
        "\n",
        "### Statistical Significance Assessment\n",
        "\n",
        "**Strong Geographical Signals (Ready for Modeling):**\n",
        "- âœ… **State:** 8x variation (excluding outlier) - **CRITICAL FEATURE**\n",
        "- âœ… **City:** 7-8x variation (top cities vs baseline) - **HIGH PRIORITY**\n",
        "- âš ï¸ **City Size:** 1.22x variation - **LOW PRIORITY** (interactions only)\n",
        "- âš ï¸ **Distance:** 1.16x variation - **LOW PRIORITY** (interactions/anomaly only)\n",
        "\n",
        "**Feature Engineering Readiness:**\n",
        "- All geographical features show **statistically significant differences**\n",
        "- However, **city_size** and **distance** signals are **too weak** for standalone use\n",
        "- **State** and **city** features are **production-ready** with proper encoding\n",
        "\n",
        "---\n",
        "\n",
        "### Feature Engineering Decision Framework\n",
        "\n",
        "#### âœ… **Implement Now (High Value)**\n",
        "\n",
        "1. **State Feature**\n",
        "   - **Encoding:** Frequency encoding or target encoding (out-of-fold)\n",
        "   - **Handling:** Low-volume states (<100 transactions) as \"Other\" category\n",
        "   - **Rationale:** Strong signal, actionable, production-ready\n",
        "\n",
        "2. **City Feature**\n",
        "   - **Encoding:** Target encoding (out-of-fold) or high-risk city flag\n",
        "   - **Handling:** Cities with <100 transactions grouped by state\n",
        "   - **Rationale:** Strong signal (7-8x variation), identifies specific hotspots\n",
        "\n",
        "#### âš ï¸ **Defer or Redesign (Low Value / Needs Work)**\n",
        "\n",
        "1. **City Size Feature**\n",
        "   - **Status:** Defer as standalone feature\n",
        "   - **Alternative:** Use in interaction features (city_size Ã— state)\n",
        "   - **Rationale:** 1.22x variation too weak for standalone use\n",
        "\n",
        "2. **Distance Category Feature**\n",
        "   - **Status:** Defer current binning approach\n",
        "   - **Alternatives:**\n",
        "     - Continuous distance (km) feature\n",
        "     - Anomaly detection (distance > customer's typical range)\n",
        "     - Interaction features (distance Ã— state, distance Ã— time)\n",
        "   - **Rationale:** 1.16x variation too weak, but may be useful in interactions\n",
        "\n",
        "3. **Merchant Location Hotspots**\n",
        "   - **Status:** Redesign required\n",
        "   - **Requirements:**\n",
        "     - Coarser spatial bins (0.05-0.1 degrees or geohash)\n",
        "     - Minimum support threshold (â‰¥50-200 transactions)\n",
        "     - Smoothed fraud rate estimates\n",
        "   - **Rationale:** Current granularity produces statistical noise\n",
        "\n",
        "---\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "#### Immediate Actions\n",
        "1. **Feature Engineering:**\n",
        "   - Add **state** and **city** features to model pipeline\n",
        "   - Use **target encoding** with out-of-fold strategy to prevent leakage\n",
        "   - Handle **low-volume categories** (states/cities with <100 transactions)\n",
        "\n",
        "2. **Risk Scoring:**\n",
        "   - Implement **state-level risk scores** (with outlier handling)\n",
        "   - Flag **high-risk cities** (Aurora, Clearwater, etc.) for enhanced monitoring\n",
        "   - Create **location risk tiers** (High/Medium/Low) based on fraud rates\n",
        "\n",
        "3. **Monitoring:**\n",
        "   - Track fraud rates by state and city in production\n",
        "   - Set up alerts for **new high-risk locations** (emerging hotspots)\n",
        "   - Monitor **location feature drift** (changes in state/city distribution)\n",
        "\n",
        "#### Future Enhancements\n",
        "1. **Merchant Location Analysis:**\n",
        "   - Redesign with coarser bins and minimum support\n",
        "   - Implement smoothed fraud rate estimates\n",
        "   - Consider **merchant category Ã— location** interactions\n",
        "\n",
        "2. **Interaction Features:**\n",
        "   - Test **state Ã— city_size** interactions\n",
        "   - Test **distance Ã— state** interactions\n",
        "   - Test **location Ã— temporal** interactions (e.g., state Ã— hour)\n",
        "\n",
        "3. **Advanced Techniques:**\n",
        "   - **Geospatial clustering** (DBSCAN, K-means) for merchant locations\n",
        "   - **Location embeddings** (learned representations) instead of categorical\n",
        "   - **Anomaly detection** based on customer's typical transaction locations\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Statistics\n",
        "\n",
        "| Dimension | Highest Risk | Lowest Risk | Risk Ratio | Signal Strength | Priority |\n",
        "|----------|--------------|------------|------------|----------------|----------|\n",
        "| **State** | RI (2.73%) | ID (0.20%) | 8x* | âœ… Strong | **HIGH** |\n",
        "| **City** | Aurora (4.49%) | Baseline (0.58%) | 7.7x | âœ… Strong | **HIGH** |\n",
        "| **City Size** | Medium (0.68%) | Small (0.55%) | 1.22x | âš ï¸ Weak | **LOW** |\n",
        "| **Distance** | Statewide (0.58%) | Local (0.50%) | 1.16x | âš ï¸ Weak | **LOW** |\n",
        "| **Merchant Location** | N/A (noise) | N/A | N/A | âŒ Noise | **DEFER** |\n",
        "\n",
        "*Excluding DE outlier (100% with 9 transactions)\n",
        "\n",
        "---\n",
        "\n",
        "### Final Recommendations\n",
        "\n",
        "**For Modeling:**\n",
        "- âœ… **Include:** State, City (with proper encoding and outlier handling)\n",
        "- âš ï¸ **Defer:** City Size, Distance Category (too weak standalone)\n",
        "- ðŸ”„ **Redesign:** Merchant Location (needs coarser bins and minimum support)\n",
        "\n",
        "**For Production:**\n",
        "- Implement **location-based risk scoring** using state and city features\n",
        "- Monitor **geographical fraud trends** and emerging hotspots\n",
        "- Set up **alerts** for new high-risk locations\n",
        "\n",
        "**For Future Analysis:**\n",
        "- Explore **interaction features** (state Ã— city_size, distance Ã— state)\n",
        "- Redesign **merchant location** analysis with proper statistical rigor\n",
        "- Consider **advanced geospatial techniques** (clustering, embeddings)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5.1 Data Validation - Merchant Name Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:50.024653Z",
          "iopub.status.busy": "2026-01-24T00:55:50.024443Z",
          "iopub.status.idle": "2026-01-24T00:55:50.047716Z",
          "shell.execute_reply": "2026-01-24T00:55:50.04683Z",
          "shell.execute_reply.started": "2026-01-24T00:55:50.024634Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA VALIDATION: MERCHANT NAME COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "if 'train_df' not in globals() or train_df is None:\n",
        "    raise ValueError(\"train_df not found. Please run previous sections first.\")\n",
        "\n",
        "# Check for merchant name column (could be 'merchant', 'merch_name', 'merchant_name', etc.)\n",
        "merchant_col = None\n",
        "for col_name in [\"merchant\", \"merch_name\", \"merchant_name\", \"first\", \"last\"]:\n",
        "    if col_name in train_df.columns:\n",
        "        merchant_col = col_name\n",
        "        break\n",
        "\n",
        "if merchant_col is None:\n",
        "    print(\"âš ï¸  WARNING: No merchant name column found. Checking available columns:\")\n",
        "    print(train_df.columns)\n",
        "    raise ValueError(\"Merchant name column not found. Please check the dataset.\")\n",
        "\n",
        "print(f\"âœ“ Using merchant column: '{merchant_col}'\")\n",
        "\n",
        "# Validate merchant name coverage\n",
        "merchant_validation = validate_column_coverage(train_df, merchant_col, \"Merchant Name Analysis\")\n",
        "merchant_df, merchant_coverage, merchant_valid_count, merchant_total = merchant_validation\n",
        "\n",
        "# Count unique merchants\n",
        "unique_merchants = merchant_df.select(merchant_col).distinct()\n",
        "merchant_count = unique_merchants.count()\n",
        "print(f\"\\nâœ“ Unique merchants: {merchant_count:,}\")\n",
        "print(f\"  Coverage: {merchant_coverage:.2f}%\")\n",
        "print(f\"  Valid rows: {merchant_valid_count:,} / {merchant_total:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5.2 Merchant Name Pattern Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:50.048716Z",
          "iopub.status.busy": "2026-01-24T00:55:50.048443Z",
          "iopub.status.idle": "2026-01-24T00:55:51.247716Z",
          "shell.execute_reply": "2026-01-24T00:55:51.24683Z",
          "shell.execute_reply.started": "2026-01-24T00:55:50.048696Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MERCHANT NAME PATTERN DETECTION\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import lower, col as spark_col, when\n",
        "\n",
        "# Ensure merchant_df and merchant_col are available (if Section 8.5.1 wasn't run)\n",
        "if 'merchant_df' not in globals() or merchant_df is None:\n",
        "    if 'train_df' not in globals() or train_df is None:\n",
        "        raise ValueError(\"train_df not found. Please run previous sections first.\")\n",
        "    \n",
        "    # Find merchant column\n",
        "    merchant_col = None\n",
        "    for col_name in [\"merchant\", \"merch_name\", \"merchant_name\", \"first\", \"last\"]:\n",
        "        if col_name in train_df.columns:\n",
        "            merchant_col = col_name\n",
        "            break\n",
        "    \n",
        "    if merchant_col is None:\n",
        "        raise ValueError(\"Merchant name column not found. Please check the dataset.\")\n",
        "    \n",
        "    # Create merchant_df from validation\n",
        "    merchant_validation = validate_column_coverage(train_df, merchant_col, \"Merchant Name Analysis\")\n",
        "    merchant_df, merchant_coverage, merchant_valid_count, merchant_total = merchant_validation\n",
        "    print(f\"âœ“ Created merchant_df from train_df (coverage: {merchant_coverage:.2f}%)\")\n",
        "else:\n",
        "    print(\"âœ“ Using merchant_df from Section 8.5.1\")\n",
        "\n",
        "# Ensure merchant_col is defined\n",
        "if 'merchant_col' not in globals() or merchant_col is None:\n",
        "    # Find merchant column\n",
        "    for col_name in [\"merchant\", \"merch_name\", \"merchant_name\", \"first\", \"last\"]:\n",
        "        if col_name in train_df.columns:\n",
        "            merchant_col = col_name\n",
        "            break\n",
        "    if merchant_col is None:\n",
        "        raise ValueError(\"merchant_col not found. Please run Section 8.5.1 first.\")\n",
        "\n",
        "# Detect common patterns in merchant names\n",
        "merchant_with_patterns = merchant_df.withColumn(\n",
        "    \"has_fraud_prefix\",\n",
        "    lower(spark_col(merchant_col)).like(\"fraud_%\")\n",
        ").withColumn(\n",
        "    \"has_online_keyword\",\n",
        "    (lower(spark_col(merchant_col)).like(\"%online%\") |\n",
        "     lower(spark_col(merchant_col)).like(\"%web%\"))\n",
        ").withColumn(\n",
        "    \"has_pos_keyword\",\n",
        "    lower(spark_col(merchant_col)).like(\"%pos%\")\n",
        ")\n",
        "\n",
        "# Count merchants with patterns\n",
        "pattern_counts = merchant_with_patterns.agg(\n",
        "    spark_sum(when(spark_col(\"has_fraud_prefix\"), 1).otherwise(0)).alias(\"fraud_prefix_count\"),\n",
        "    spark_sum(when(spark_col(\"has_online_keyword\"), 1).otherwise(0)).alias(\"online_keyword_count\"),\n",
        "    spark_sum(when(spark_col(\"has_pos_keyword\"), 1).otherwise(0)).alias(\"pos_keyword_count\")\n",
        ").collect()[0]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MERCHANT NAME PATTERN DETECTION\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Merchants with 'fraud_' prefix: {pattern_counts['fraud_prefix_count']:,}\")\n",
        "print(f\"Merchants with 'online'/'web' keyword: {pattern_counts['online_keyword_count']:,}\")\n",
        "print(f\"Merchants with 'pos' keyword: {pattern_counts['pos_keyword_count']:,}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Analyze fraud rate for merchants with 'fraud_' prefix\n",
        "if pattern_counts['fraud_prefix_count'] > 0:\n",
        "    fraud_prefix_stats = merchant_with_patterns.filter(spark_col(\"has_fraud_prefix\")).agg(\n",
        "        count(\"*\").alias(\"total_txns\"),\n",
        "        spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
        "    ).collect()[0]\n",
        "    fraud_prefix_rate = (fraud_prefix_stats['fraud_count'] / fraud_prefix_stats['total_txns']) * 100\n",
        "    print(f\"\\nðŸ“Š Merchants with 'fraud_' prefix:\")\n",
        "    print(f\"  Total transactions: {fraud_prefix_stats['total_txns']:,}\")\n",
        "    print(f\"  Fraud count: {fraud_prefix_stats['fraud_count']:,}\")\n",
        "    print(f\"  Fraud rate: {fraud_prefix_rate:.4f}%\")\n",
        "else:\n",
        "    print(\"\\nðŸ“Š No merchants found with 'fraud_' prefix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5.3 Merchant-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:51.248716Z",
          "iopub.status.busy": "2026-01-24T00:55:51.248443Z",
          "iopub.status.idle": "2026-01-24T00:55:52.858492Z",
          "shell.execute_reply": "2026-01-24T00:55:52.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:55:51.248696Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MERCHANT-BASED FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "merchant_fraud_stats = results_manager.load_dataframe(\"8.5\", \"merchant_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if merchant_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded merchant_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    merchant_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        merchant_df,\n",
        "        merchant_col,\n",
        "        \"merchant\",\n",
        "        cache_name=\"merchant_fraud_stats\",\n",
        "        save_result=True, section=\"8.5\", result_name=\"merchant_fraud_stats\"\n",
        "    )\n",
        "\n",
        "merchant_fraud_stats = merchant_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "# Apply minimum transaction threshold for reliable statistics\n",
        "min_txns = 50\n",
        "merchant_fraud_stats_filtered = merchant_fraud_stats[merchant_fraud_stats['total_txns'] >= min_txns]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"MERCHANT FRAUD STATISTICS (â‰¥{min_txns} transactions)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total merchants analyzed: {len(merchant_fraud_stats_filtered):,}\")\n",
        "print(f\"Merchants with <{min_txns} transactions excluded: {len(merchant_fraud_stats) - len(merchant_fraud_stats_filtered):,}\")\n",
        "print(\"\\nTop 20 Riskiest Merchants:\")\n",
        "print(merchant_fraud_stats_filtered.head(20)[[merchant_col, 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Key insights\n",
        "if len(merchant_fraud_stats_filtered) > 0:\n",
        "    riskiest_merchant = merchant_fraud_stats_filtered.iloc[0]\n",
        "    safest_merchant = merchant_fraud_stats_filtered.iloc[-1]\n",
        "    \n",
        "    print(f\"\\nðŸ“Š KEY INSIGHTS (Merchants with â‰¥{min_txns} transactions):\")\n",
        "    print(f\"Riskiest merchant:  {riskiest_merchant[merchant_col]} ({riskiest_merchant['fraud_rate_pct']:.4f}% fraud rate, {riskiest_merchant['total_txns']:,} txns)\")\n",
        "    print(f\"Safest merchant:    {safest_merchant[merchant_col]} ({safest_merchant['fraud_rate_pct']:.4f}% fraud rate, {safest_merchant['total_txns']:,} txns)\")\n",
        "    if safest_merchant['fraud_rate_pct'] > 0:\n",
        "        print(f\"Risk ratio:         {riskiest_merchant['fraud_rate_pct'] / safest_merchant['fraud_rate_pct']:.2f}x\")\n",
        "    else:\n",
        "        print(\"Risk ratio:         N/A (safest has 0% fraud)\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  No merchants with â‰¥{min_txns} transactions found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5.4 Merchant-Based Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:55:52.859492Z",
          "iopub.status.busy": "2026-01-24T00:55:52.859234Z",
          "iopub.status.idle": "2026-01-24T00:55:53.858492Z",
          "shell.execute_reply": "2026-01-24T00:55:53.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:55:52.859470Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# MERCHANT-BASED FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "# Focus on top 15 riskiest merchants (by fraud rate) with minimum transaction threshold\n",
        "\n",
        "min_txns_viz = 50\n",
        "merchant_fraud_viz = merchant_fraud_stats_filtered.head(15)\n",
        "\n",
        "if len(merchant_fraud_viz) > 0:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Chart 1: Fraud Rate by Merchant (Top 15 Riskiest)\n",
        "    ax1 = axes[0]\n",
        "    ax1.barh(range(len(merchant_fraud_viz)), merchant_fraud_viz['fraud_rate_pct'], \n",
        "            color='crimson', alpha=0.7)\n",
        "    ax1.set_yticks(range(len(merchant_fraud_viz)))\n",
        "    ax1.set_yticklabels(merchant_fraud_viz[merchant_col], fontsize=8)\n",
        "    ax1.set_xlabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title(f'Top 15 Riskiest Merchants by Fraud Rate (â‰¥{min_txns_viz} transactions)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "    ax1.invert_yaxis()\n",
        "    \n",
        "    # Chart 2: Transaction Volume vs Fraud Rate (Scatter)\n",
        "    ax2 = axes[1]\n",
        "    scatter = ax2.scatter(merchant_fraud_viz['total_txns'], merchant_fraud_viz['fraud_rate_pct'], \n",
        "                         s=100, alpha=0.6, c=merchant_fraud_viz['fraud_rate_pct'], \n",
        "                         cmap='Reds', edgecolors='black', linewidth=1)\n",
        "    ax2.set_xlabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title(f'Transaction Volume vs Fraud Rate (Top 15 Riskiest Merchants, â‰¥{min_txns_viz} transactions)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add merchant labels\n",
        "    for idx, row in merchant_fraud_viz.iterrows():\n",
        "        ax2.annotate(row[merchant_col][:20], (row['total_txns'], row['fraud_rate_pct']), \n",
        "                    fontsize=7, alpha=0.7)\n",
        "    \n",
        "    plt.colorbar(scatter, ax=ax2, label='Fraud Rate (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('merchant_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"âœ“ Visualizations saved as 'merchant_fraud_patterns.png'\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Not enough merchants with â‰¥{min_txns_viz} transactions for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5.5 Key Findings Summary\n",
        "\n",
        "**Data validation:** Merchant name coverage analyzed.\n",
        "\n",
        "**Key patterns:**\n",
        "- Unique merchants identified\n",
        "- Fraud rates vary significantly by merchant\n",
        "- Merchant name patterns (e.g., 'fraud_' prefix) detected and analyzed\n",
        "- Minimum transaction threshold applied for reliable statistics\n",
        "\n",
        "**Next:** Section 8 checkpoint save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.6 Save Section-Level Checkpoint\n",
        "\n",
        "**Purpose:** Save the geographical-features-enriched DataFrame to checkpoint for future use.\n",
        "This enables skipping Section 8 when running later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:08.109626Z",
          "iopub.status.busy": "2026-01-24T00:56:08.109335Z",
          "iopub.status.idle": "2026-01-24T00:56:14.670471Z",
          "shell.execute_reply": "2026-01-24T00:56:14.669247Z",
          "shell.execute_reply.started": "2026-01-24T00:56:08.109598Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 8 (GEOGRAPHICAL FEATURES COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 8 adds\n",
        "required_columns_section8 = [\"customer_merchant_distance_km\", \"distance_category\", \"city_size\"]\n",
        "\n",
        "missing_cols = [col for col in required_columns_section8 if col not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"WARNING: Missing geographical columns: {missing_cols}\")\n",
        "    print(\"  Some geographical features may not have been created.\")\n",
        "    print(\"  Checkpoint will be saved with available columns.\")\n",
        "else:\n",
        "    print(\"All Section 8 geographical columns present - saving section-level checkpoint...\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df,\n",
        "    CHECKPOINT_SECTION8,\n",
        "    \"Section 8 (Geographical Features Complete - Section-Level Checkpoint)\"\n",
        ")\n",
        "print(\"Section 8 section-level checkpoint saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Customer Demographics Analysis\n",
        "\n",
        "## Overview\n",
        "\n",
        "This section analyzes customer demographic features to identify fraud patterns based on:\n",
        "- **Age** (calculated from date of birth)\n",
        "- **Gender** \n",
        "- **Job/Occupation**\n",
        "\n",
        "### Hypothesis\n",
        "- **Age:** Younger customers may have higher fraud rates (less established credit history, higher risk-taking)\n",
        "- **Gender:** Gender-based patterns may exist (though we'll be careful about bias)\n",
        "- **Job:** Certain occupations may correlate with fraud risk (e.g., high-income vs. low-income jobs)\n",
        "\n",
        "### What We'll Analyze\n",
        "1. Age distribution and fraud rates by age bins\n",
        "2. Gender-based fraud patterns\n",
        "3. Job/occupation fraud patterns\n",
        "4. Cross-dimensional insights\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** Section 9.0 - Load Checkpoint & Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.0 Load Checkpoint & Helper Functions\n",
        "\n",
        "### Purpose\n",
        "Load the required DataFrame from Section 8 checkpoint (or use existing `train_df` if available).\n",
        "\n",
        "### Helper Functions\n",
        "We'll reuse the `aggregate_fraud_by_dimension` helper from Section 7 for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:14.672609Z",
          "iopub.status.busy": "2026-01-24T00:56:14.672387Z",
          "iopub.status.idle": "2026-01-24T00:56:14.679528Z",
          "shell.execute_reply": "2026-01-24T00:56:14.678316Z",
          "shell.execute_reply.started": "2026-01-24T00:56:14.67259Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT: SECTION 8 (REQUIRED FOR SECTION 9)\n",
        "# ============================================================\n",
        "\n",
        "required_columns_section9 = [\"dob\", \"gender\", \"job\", \"is_fraud\"]\n",
        "\n",
        "df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_SECTION8,\n",
        "    required_columns=required_columns_section9,\n",
        "    cell_name=\"Section 8 (preferred for Section 9)\"\n",
        ")\n",
        "\n",
        "if loaded_from_checkpoint:\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "else:\n",
        "    if 'train_df' not in globals() or train_df is None:\n",
        "        raise ValueError(\n",
        "            f\"train_df not found and checkpoint not available: {CHECKPOINT_SECTION8}. \"\n",
        "            \"Run previous sections first or ensure checkpoint exists.\"\n",
        "        )\n",
        "    missing_cols = [c for c in required_columns_section9 if c not in train_df.columns]\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns for Section 9: {missing_cols}\")\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(\"âœ“ train_df already in memory\")\n",
        "\n",
        "print(f\"train_df ready with {TOTAL_DATASET_ROWS:,} rows\")\n",
        "print(\"=\" * 80)\n",
        "print(\"SECTION 9: CUSTOMER DEMOGRAPHICS ANALYSIS\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.1 Age Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Younger customers (18-30) may have higher fraud rates\n",
        "  - Less established credit history\n",
        "  - Higher risk-taking behavior\n",
        "  - More likely to be victims of identity theft\n",
        "- **Alternative:** Middle-aged customers (30-50) may be riskier due to higher transaction volumes\n",
        "\n",
        "### What We'll Analyze\n",
        "- Calculate age from date of birth (`dob`)\n",
        "- Create age bins (e.g., 18-25, 26-35, 36-50, 51-65, 65+)\n",
        "- Fraud rate by age bin\n",
        "- Transaction volume by age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.1 Data Validation - Date of Birth Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:14.681059Z",
          "iopub.status.busy": "2026-01-24T00:56:14.680731Z",
          "iopub.status.idle": "2026-01-24T00:56:15.159621Z",
          "shell.execute_reply": "2026-01-24T00:56:15.159067Z",
          "shell.execute_reply.started": "2026-01-24T00:56:14.68103Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE DATE OF BIRTH DATA COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "# Check dob column coverage\n",
        "total_rows = TOTAL_DATASET_ROWS\n",
        "dob_valid = train_df.filter(col(\"dob\").isNotNull()).count()\n",
        "dob_coverage = (dob_valid / total_rows) * 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VALIDATION: Age Analysis (Date of Birth)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows:      {total_rows:,}\")\n",
        "print(f\"Valid dob:       {dob_valid:,}\")\n",
        "print(f\"Coverage:        {dob_coverage:.2f}%\")\n",
        "print(f\"Excluded (NULL): {total_rows - dob_valid:,}\")\n",
        "\n",
        "if dob_coverage < 95:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {dob_coverage:.2f}% coverage - results may be biased\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Coverage acceptable ({dob_coverage:.2f}%)\")\n",
        "\n",
        "# Show sample dob values\n",
        "print(\"\\nSample date of birth values:\")\n",
        "train_df.select(\"dob\").show(5, truncate=False)\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.2 Calculate Age from Date of Birth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:15.161178Z",
          "iopub.status.busy": "2026-01-24T00:56:15.160586Z",
          "iopub.status.idle": "2026-01-24T00:56:17.700076Z",
          "shell.execute_reply": "2026-01-24T00:56:17.699328Z",
          "shell.execute_reply.started": "2026-01-24T00:56:15.161155Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CALCULATE AGE FROM DATE OF BIRTH\n",
        "# ============================================================\n",
        "# Design: Calculate age at transaction time (using trans_date_trans_time)\n",
        "# Senior pattern: Defensive check, idempotent column creation\n",
        "# Note: year, month, dayofmonth are imported globally\n",
        "\n",
        "# Get transaction date (use merchant_local_time if available, else trans_date_trans_time)\n",
        "transaction_date_col = \"merchant_local_time\" if \"merchant_local_time\" in train_df.columns else \"trans_date_trans_time\"\n",
        "\n",
        "# Calculate age in years (more accurate than days/365.25)\n",
        "if \"age\" not in train_df.columns:\n",
        "    # Extract year from transaction date and dob, then calculate difference\n",
        "    train_df = train_df.withColumn(\n",
        "        \"age\",\n",
        "        when(\n",
        "            col(\"dob\").isNotNull(),\n",
        "            year(col(transaction_date_col)) - year(col(\"dob\"))\n",
        "            - when(\n",
        "                (month(col(transaction_date_col)) < month(col(\"dob\"))) |\n",
        "                ((month(col(transaction_date_col)) == month(col(\"dob\"))) & \n",
        "                 (dayofmonth(col(transaction_date_col)) < dayofmonth(col(\"dob\")))),\n",
        "                1\n",
        "            ).otherwise(0)\n",
        "        ).otherwise(None)\n",
        "    )\n",
        "    print(\"âœ“ Calculated age from date of birth\")\n",
        "else:\n",
        "    print(\"âœ“ Age column already exists\")\n",
        "\n",
        "# Validate age range (reasonable: 18-100)\n",
        "age_analysis_df = train_df.filter(\n",
        "    col(\"age\").isNotNull() & \n",
        "    (col(\"age\") >= 18) & \n",
        "    (col(\"age\") <= 100)\n",
        ")\n",
        "\n",
        "valid_age_count = age_analysis_df.count()\n",
        "age_coverage = (valid_age_count / dob_valid) * 100 if dob_valid > 0 else 0\n",
        "\n",
        "print(f\"\\nAge validation:\")\n",
        "print(f\"Valid ages (18-100): {valid_age_count:,} / {dob_valid:,} ({age_coverage:.2f}%)\")\n",
        "\n",
        "# Show age statistics\n",
        "age_stats = age_analysis_df.agg(\n",
        "    spark_min(\"age\").alias(\"min_age\"),\n",
        "    spark_max(\"age\").alias(\"max_age\"),\n",
        "    avg(\"age\").alias(\"avg_age\"),\n",
        "    stddev(\"age\").alias(\"stddev_age\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nAge Statistics:\")\n",
        "print(f\"  Min:    {age_stats['min_age']:.0f}\")\n",
        "print(f\"  Max:    {age_stats['max_age']:.0f}\")\n",
        "print(f\"  Mean:   {age_stats['avg_age']:.2f}\")\n",
        "print(f\"  StdDev: {age_stats['stddev_age']:.2f}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nSample age calculations:\")\n",
        "age_analysis_df.select(\"dob\", transaction_date_col, \"age\").show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.3 Create Age Bins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:17.701495Z",
          "iopub.status.busy": "2026-01-24T00:56:17.701236Z",
          "iopub.status.idle": "2026-01-24T00:56:19.646161Z",
          "shell.execute_reply": "2026-01-24T00:56:19.645454Z",
          "shell.execute_reply.started": "2026-01-24T00:56:17.701467Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREATE AGE BINS FOR ANALYSIS\n",
        "# ============================================================\n",
        "# Design: Categorize age into meaningful bins for fraud pattern analysis\n",
        "\n",
        "if \"age_bin\" not in age_analysis_df.columns:\n",
        "    age_analysis_df = age_analysis_df.withColumn(\n",
        "        \"age_bin\",\n",
        "        when(col(\"age\").between(18, 25), \"18-25\")\n",
        "        .when(col(\"age\").between(26, 35), \"26-35\")\n",
        "        .when(col(\"age\").between(36, 50), \"36-50\")\n",
        "        .when(col(\"age\").between(51, 65), \"51-65\")\n",
        "        .when(col(\"age\") > 65, \"65+\")\n",
        "        .otherwise(\"Unknown\")\n",
        "    )\n",
        "    print(\"âœ“ Created age_bin column\")\n",
        "else:\n",
        "    print(\"âœ“ age_bin column already exists\")\n",
        "\n",
        "# Show distribution\n",
        "print(\"\\nAge bin distribution:\")\n",
        "age_analysis_df.groupBy(\"age_bin\").agg(\n",
        "    count(\"*\").alias(\"count\")\n",
        ").orderBy(\"age_bin\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.4 Age-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:19.647066Z",
          "iopub.status.busy": "2026-01-24T00:56:19.646869Z",
          "iopub.status.idle": "2026-01-24T00:56:21.208278Z",
          "shell.execute_reply": "2026-01-24T00:56:21.207579Z",
          "shell.execute_reply.started": "2026-01-24T00:56:19.647047Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AGGREGATE FRAUD BY AGE BIN\n",
        "# ============================================================\n",
        "\n",
        "age_fraud_stats = results_manager.load_dataframe(\"9.1\", \"age_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if age_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded age_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    age_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=age_analysis_df,\n",
        "        dimension_col=\"age_bin\",\n",
        "        dimension_name=\"Age Bin\",\n",
        "        cache_name=\"cached_age_fraud_stats\",\n",
        "        save_result=True, section=\"9.1\", result_name=\"age_fraud_stats\"\n",
        "    )\n",
        "\n",
        "age_order = [\"18-25\", \"26-35\", \"36-50\", \"51-65\", \"65+\", \"Unknown\"]\n",
        "age_fraud_stats[\"age_bin_ordered\"] = pd.Categorical(age_fraud_stats[\"age_bin\"], categories=age_order, ordered=True)\n",
        "age_fraud_stats = age_fraud_stats.sort_values(\"age_bin_ordered\").drop(\"age_bin_ordered\", axis=1)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY AGE BIN\")\n",
        "print(\"=\" * 100)\n",
        "print(age_fraud_stats[['age_bin', 'total_txns', 'fraud_count', 'legit_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "peak_age = age_fraud_stats.loc[age_fraud_stats['fraud_rate_pct'].idxmax()]\n",
        "safest_age = age_fraud_stats.loc[age_fraud_stats['fraud_rate_pct'].idxmin()]\n",
        "risk_ratio = peak_age['fraud_rate_pct'] / safest_age['fraud_rate_pct'] if safest_age['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Highest risk age group: {peak_age['age_bin']} ({peak_age['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Lowest risk age group:  {safest_age['age_bin']} ({safest_age['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:             {risk_ratio:.2f}x higher at peak\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.5 Age-Based Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:21.209407Z",
          "iopub.status.busy": "2026-01-24T00:56:21.209188Z",
          "iopub.status.idle": "2026-01-24T00:56:22.501685Z",
          "shell.execute_reply": "2026-01-24T00:56:22.500721Z",
          "shell.execute_reply.started": "2026-01-24T00:56:21.209385Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AGE-BASED FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Chart 1: Fraud Count by Age Bin\n",
        "ax1 = axes[0, 0]\n",
        "age_order_viz = [x for x in age_order if x in age_fraud_stats['age_bin'].values]\n",
        "age_fraud_ordered = age_fraud_stats.set_index('age_bin').reindex(age_order_viz).reset_index()\n",
        "ax1.bar(age_fraud_ordered['age_bin'], age_fraud_ordered['fraud_count'], \n",
        "        color='crimson', alpha=0.7, edgecolor='darkred')\n",
        "ax1.set_xlabel('Age Bin', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Fraud Count', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Fraud Transaction Count by Age Bin', fontsize=14, fontweight='bold')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 2: Fraud Rate by Age Bin\n",
        "ax2 = axes[0, 1]\n",
        "ax2.plot(age_fraud_ordered['age_bin'], age_fraud_ordered['fraud_rate_pct'], \n",
        "         marker='o', linewidth=2.5, color='darkred', markersize=8)\n",
        "ax2.fill_between(age_fraud_ordered['age_bin'], age_fraud_ordered['fraud_rate_pct'], \n",
        "                  alpha=0.3, color='crimson')\n",
        "ax2.set_xlabel('Age Bin', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Fraud Rate by Age Bin', fontsize=14, fontweight='bold')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.axhline(y=age_fraud_stats['fraud_rate_pct'].mean(), color='red', linestyle='--', \n",
        "            linewidth=2, alpha=0.7, label='Overall Average')\n",
        "ax2.legend()\n",
        "\n",
        "# Chart 3: Fraud vs Legitimate Counts\n",
        "ax3 = axes[1, 0]\n",
        "x = np.arange(len(age_fraud_ordered))\n",
        "width = 0.35\n",
        "ax3.bar(x - width/2, age_fraud_ordered['fraud_count'], width, \n",
        "        label='Fraud', color='crimson', alpha=0.7)\n",
        "ax3.bar(x + width/2, age_fraud_ordered['legit_count'], width, \n",
        "        label='Legitimate', color='green', alpha=0.7)\n",
        "ax3.set_xlabel('Age Bin', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Transaction Count (log scale)', fontsize=12, fontweight='bold')\n",
        "ax3.set_title('Fraud vs Legitimate Transactions by Age Bin', fontsize=14, fontweight='bold')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(age_fraud_ordered['age_bin'], rotation=45)\n",
        "ax3.legend()\n",
        "ax3.set_yscale('log')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 4: Transaction Volume by Age Bin\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar(age_fraud_ordered['age_bin'], age_fraud_ordered['total_txns'], \n",
        "        color='steelblue', alpha=0.7, edgecolor='darkblue')\n",
        "ax4.set_xlabel('Age Bin', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('Transaction Volume by Age Bin', fontsize=14, fontweight='bold')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('age_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Visualizations saved as 'age_fraud_patterns.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1.6 Key Findings Summary\n",
        "\n",
        "**Data validation:** DOB coverage 100%; valid ages (18â€“100) 98.80% of rows. Age range 18â€“95, mean 45.86.\n",
        "\n",
        "**Fraud by age bin:**\n",
        "\n",
        "| Age Bin | Total Txns | Fraud Count | Fraud Rate (%) |\n",
        "|---------|------------|-------------|----------------|\n",
        "| 18-25   | 129,507    | 777         | 0.60           |\n",
        "| 26-35   | 299,980    | 1,417       | 0.47           |\n",
        "| 36-50   | 412,228    | 1,927       | 0.47           |\n",
        "| 51-65   | 260,912    | 1,901       | 0.73           |\n",
        "| 65+     | 178,493    | 1,366       | 0.77           |\n",
        "\n",
        "**Key patterns:**\n",
        "- **Highest risk:** 65+ (0.77% fraud rate)\n",
        "- **Lowest risk:** 36-50 (0.47% fraud rate)\n",
        "- **Risk ratio:** 1.64x higher at peak vs. safest\n",
        "- 51-65 and 65+ are above overall average; 26-35 and 36-50 below. Despite highest transaction volume and fraud counts, 36-50 has the lowest fraud rate; older groups show a higher share of fraud relative to volume."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.2 Gender Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Gender may show different fraud patterns\n",
        "  - Note: We'll analyze patterns but be cautious about bias implications\n",
        "  - Focus on statistical patterns, not causal claims\n",
        "- **Alternative:** No significant gender-based differences\n",
        "\n",
        "### What We'll Analyze\n",
        "- Gender distribution in dataset\n",
        "- Fraud rate by gender\n",
        "- Transaction volume by gender"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2.1 Data Validation - Gender Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:22.503028Z",
          "iopub.status.busy": "2026-01-24T00:56:22.502747Z",
          "iopub.status.idle": "2026-01-24T00:56:23.107745Z",
          "shell.execute_reply": "2026-01-24T00:56:23.107156Z",
          "shell.execute_reply.started": "2026-01-24T00:56:22.503009Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE GENDER DATA COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "total_rows = TOTAL_DATASET_ROWS\n",
        "gender_valid = train_df.filter(col(\"gender\").isNotNull() & (trim(col(\"gender\")) != \"\")).count()\n",
        "gender_coverage = (gender_valid / total_rows) * 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VALIDATION: Gender Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows:      {total_rows:,}\")\n",
        "print(f\"Valid gender:    {gender_valid:,}\")\n",
        "print(f\"Coverage:        {gender_coverage:.2f}%\")\n",
        "print(f\"Excluded (NULL): {total_rows - gender_valid:,}\")\n",
        "\n",
        "if gender_coverage < 95:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {gender_coverage:.2f}% coverage - results may be biased\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Coverage acceptable ({gender_coverage:.2f}%)\")\n",
        "\n",
        "# Show gender distribution\n",
        "print(\"\\nGender distribution:\")\n",
        "train_df.filter(col(\"gender\").isNotNull() & (trim(col(\"gender\")) != \"\")) \\\n",
        "    .groupBy(\"gender\").agg(count(\"*\").alias(\"count\")) \\\n",
        "    .orderBy(\"gender\").show(truncate=False)\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2.2 Gender-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:23.108838Z",
          "iopub.status.busy": "2026-01-24T00:56:23.108574Z",
          "iopub.status.idle": "2026-01-24T00:56:23.604267Z",
          "shell.execute_reply": "2026-01-24T00:56:23.603287Z",
          "shell.execute_reply.started": "2026-01-24T00:56:23.108811Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AGGREGATE FRAUD BY GENDER\n",
        "# ============================================================\n",
        "\n",
        "gender_fraud_stats = results_manager.load_dataframe(\"9.2\", \"gender_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if gender_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded gender_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    gender_analysis_df = train_df.filter(\n",
        "        col(\"gender\").isNotNull() & (trim(col(\"gender\")) != \"\")\n",
        "    )\n",
        "    gender_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=gender_analysis_df,\n",
        "        dimension_col=\"gender\",\n",
        "        dimension_name=\"Gender\",\n",
        "        cache_name=\"cached_gender_fraud_stats\",\n",
        "        save_result=True, section=\"9.2\", result_name=\"gender_fraud_stats\"\n",
        "    )\n",
        "\n",
        "gender_fraud_stats = gender_fraud_stats.sort_values(\"gender\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY GENDER\")\n",
        "print(\"=\" * 100)\n",
        "print(gender_fraud_stats[['gender', 'total_txns', 'fraud_count', 'legit_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "if len(gender_fraud_stats) >= 2:\n",
        "    peak_gender = gender_fraud_stats.loc[gender_fraud_stats['fraud_rate_pct'].idxmax()]\n",
        "    safest_gender = gender_fraud_stats.loc[gender_fraud_stats['fraud_rate_pct'].idxmin()]\n",
        "    risk_ratio = peak_gender['fraud_rate_pct'] / safest_gender['fraud_rate_pct'] if safest_gender['fraud_rate_pct'] > 0 else float('inf')\n",
        "    \n",
        "    print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "    print(f\"Highest risk gender: {peak_gender['gender']} ({peak_gender['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "    print(f\"Lowest risk gender: {safest_gender['gender']} ({safest_gender['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "    print(f\"Risk ratio:         {risk_ratio:.2f}x higher at peak\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Only {len(gender_fraud_stats)} gender category found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2.3 Gender-Based Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:23.605979Z",
          "iopub.status.busy": "2026-01-24T00:56:23.605647Z",
          "iopub.status.idle": "2026-01-24T00:56:24.50863Z",
          "shell.execute_reply": "2026-01-24T00:56:24.507458Z",
          "shell.execute_reply.started": "2026-01-24T00:56:23.605939Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# GENDER-BASED FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Chart 1: Fraud Count by Gender\n",
        "ax1 = axes[0, 0]\n",
        "ax1.bar(gender_fraud_stats['gender'], gender_fraud_stats['fraud_count'], \n",
        "        color='crimson', alpha=0.7, edgecolor='darkred')\n",
        "ax1.set_xlabel('Gender', fontsize=12, fontweight='bold')\n",
        "ax1.set_ylabel('Fraud Count', fontsize=12, fontweight='bold')\n",
        "ax1.set_title('Fraud Transaction Count by Gender', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim(0, max(gender_fraud_stats['fraud_count']) * 1.15)  # Set y-axis to show full range with padding\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 2: Fraud Rate by Gender\n",
        "ax2 = axes[0, 1]\n",
        "ax2.bar(gender_fraud_stats['gender'], gender_fraud_stats['fraud_rate_pct'], \n",
        "        color='darkred', alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax2.set_xlabel('Gender', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "ax2.set_title('Fraud Rate by Gender', fontsize=14, fontweight='bold')\n",
        "ax2.axhline(y=gender_fraud_stats['fraud_rate_pct'].mean(), color='red', linestyle='--', \n",
        "            linewidth=2, alpha=0.7, label='Overall Average')\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Chart 3: Fraud vs Legitimate Counts (with dual y-axes)\n",
        "ax3 = axes[1, 0]\n",
        "ax3_twin = ax3.twinx()  # Create second y-axis for legitimate transactions\n",
        "\n",
        "x = np.arange(len(gender_fraud_stats))\n",
        "width = 0.35\n",
        "\n",
        "# Fraud bars on left axis\n",
        "bars1 = ax3.bar(x - width/2, gender_fraud_stats['fraud_count'], width, \n",
        "        label='Fraud', color='crimson', alpha=0.7)\n",
        "ax3.set_xlabel('Gender', fontsize=12, fontweight='bold')\n",
        "ax3.set_ylabel('Fraud Count', fontsize=12, fontweight='bold', color='crimson')\n",
        "ax3.tick_params(axis='y', labelcolor='crimson')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(gender_fraud_stats['gender'])\n",
        "\n",
        "# Legitimate bars on right axis\n",
        "bars2 = ax3_twin.bar(x + width/2, gender_fraud_stats['legit_count'], width, \n",
        "        label='Legitimate', color='green', alpha=0.7)\n",
        "ax3_twin.set_ylabel('Legitimate Count', fontsize=12, fontweight='bold', color='green')\n",
        "ax3_twin.tick_params(axis='y', labelcolor='green')\n",
        "\n",
        "ax3.set_title('Fraud vs Legitimate Transactions by Gender', fontsize=14, fontweight='bold')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Combined legend\n",
        "lines1, labels1 = ax3.get_legend_handles_labels()\n",
        "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
        "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
        "\n",
        "# Chart 4: Transaction Volume by Gender\n",
        "ax4 = axes[1, 1]\n",
        "ax4.bar(gender_fraud_stats['gender'], gender_fraud_stats['total_txns'], \n",
        "        color='steelblue', alpha=0.7, edgecolor='darkblue')\n",
        "ax4.set_xlabel('Gender', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "ax4.set_title('Transaction Volume by Gender', fontsize=14, fontweight='bold')\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('gender_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Visualizations saved as 'gender_fraud_patterns.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2.4 Key Findings Summary\n",
        "\n",
        "**Data validation:** Gender coverage 100%. F: 709,863 txns; M: 586,812 txns.\n",
        "\n",
        "**Fraud by gender:**\n",
        "\n",
        "| Gender | Total Txns | Fraud Count | Fraud Rate (%) |\n",
        "|--------|------------|-------------|----------------|\n",
        "| F      | 709,863    | 3,735       | 0.53           |\n",
        "| M      | 586,812    | 3,771       | 0.64           |\n",
        "\n",
        "**Key patterns:**\n",
        "- **Highest risk:** M (0.64% fraud rate), above overall average (~0.58%)\n",
        "- **Lowest risk:** F (0.53% fraud rate), below average\n",
        "- **Risk ratio:** 1.22x higher for M vs. F\n",
        "- Fraud counts are similar; rate gap driven by lower male transaction volume. Use gender with care for bias and fairness.\n",
        "\n",
        "**Next:** Section 9.3 - Job/Occupation Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.3 Job/Occupation Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- **Expected:** Certain occupations may correlate with fraud risk\n",
        "  - High-income jobs (e.g., \"Engineer\", \"Manager\") may have different patterns\n",
        "  - Low-income jobs may show different risk profiles\n",
        "  - Job categories might indicate lifestyle factors affecting fraud risk\n",
        "\n",
        "### What We'll Analyze\n",
        "- Job distribution in dataset\n",
        "- Fraud rate by job/occupation\n",
        "- Top riskiest and safest occupations\n",
        "- Transaction volume by job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3.1 Data Validation - Job Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:24.510198Z",
          "iopub.status.busy": "2026-01-24T00:56:24.509914Z",
          "iopub.status.idle": "2026-01-24T00:56:25.458012Z",
          "shell.execute_reply": "2026-01-24T00:56:25.457013Z",
          "shell.execute_reply.started": "2026-01-24T00:56:24.510171Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATE JOB DATA COVERAGE\n",
        "# ============================================================\n",
        "\n",
        "total_rows = TOTAL_DATASET_ROWS\n",
        "job_valid = train_df.filter(col(\"job\").isNotNull() & (trim(col(\"job\")) != \"\")).count()\n",
        "job_coverage = (job_valid / total_rows) * 100\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA VALIDATION: Job/Occupation Analysis\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows:      {total_rows:,}\")\n",
        "print(f\"Valid job:       {job_valid:,}\")\n",
        "print(f\"Coverage:        {job_coverage:.2f}%\")\n",
        "print(f\"Excluded (NULL): {total_rows - job_valid:,}\")\n",
        "\n",
        "if job_coverage < 95:\n",
        "    print(f\"\\nâš ï¸  WARNING: Only {job_coverage:.2f}% coverage - results may be biased\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ Coverage acceptable ({job_coverage:.2f}%)\")\n",
        "\n",
        "# Show unique job count\n",
        "unique_jobs = train_df.filter(col(\"job\").isNotNull() & (trim(col(\"job\")) != \"\")) \\\n",
        "    .select(\"job\").distinct().count()\n",
        "print(f\"\\nUnique jobs/occupations: {unique_jobs}\")\n",
        "\n",
        "# Show top 10 most common jobs\n",
        "print(\"\\nTop 10 most common jobs:\")\n",
        "train_df.filter(col(\"job\").isNotNull() & (trim(col(\"job\")) != \"\")) \\\n",
        "    .groupBy(\"job\").agg(count(\"*\").alias(\"count\")) \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show(10, truncate=False)\n",
        "\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3.2 Job-Based Fraud Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:25.459485Z",
          "iopub.status.busy": "2026-01-24T00:56:25.458953Z",
          "iopub.status.idle": "2026-01-24T00:56:25.93287Z",
          "shell.execute_reply": "2026-01-24T00:56:25.932027Z",
          "shell.execute_reply.started": "2026-01-24T00:56:25.459145Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# AGGREGATE FRAUD BY JOB/OCCUPATION\n",
        "# ============================================================\n",
        "\n",
        "job_fraud_stats = results_manager.load_dataframe(\"9.3\", \"job_fraud_stats\") if \"results_manager\" in globals() else None\n",
        "if job_fraud_stats is not None:\n",
        "    print(\"âœ“ Loaded job_fraud_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    job_analysis_df = train_df.filter(\n",
        "        col(\"job\").isNotNull() & (trim(col(\"job\")) != \"\")\n",
        "    )\n",
        "    job_fraud_stats = aggregate_fraud_by_dimension(\n",
        "        df=job_analysis_df,\n",
        "        dimension_col=\"job\",\n",
        "        dimension_name=\"Job/Occupation\",\n",
        "        cache_name=\"cached_job_fraud_stats\",\n",
        "        save_result=True, section=\"9.3\", result_name=\"job_fraud_stats\"\n",
        "    )\n",
        "\n",
        "job_fraud_stats = job_fraud_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY JOB/OCCUPATION (Top 20 Riskiest)\")\n",
        "print(\"=\" * 100)\n",
        "print(job_fraud_stats.head(20)[['job', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "if len(job_fraud_stats) > 0:\n",
        "    riskiest_job = job_fraud_stats.iloc[0]\n",
        "    safest_job = job_fraud_stats.iloc[-1]\n",
        "    risk_ratio = riskiest_job['fraud_rate_pct'] / safest_job['fraud_rate_pct'] if safest_job['fraud_rate_pct'] > 0 else float('inf')\n",
        "    \n",
        "    print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "    print(f\"Riskiest job:  {riskiest_job['job']} ({riskiest_job['fraud_rate_pct']:.4f}% fraud rate, {riskiest_job['total_txns']:,} transactions)\")\n",
        "    print(f\"Safest job:    {safest_job['job']} ({safest_job['fraud_rate_pct']:.4f}% fraud rate, {safest_job['total_txns']:,} transactions)\")\n",
        "    print(f\"Risk ratio:    {risk_ratio:.2f}x higher at peak\")\n",
        "    \n",
        "    # Filter to jobs with minimum transaction count (e.g., >= 100) for more reliable stats\n",
        "    min_txns = 100\n",
        "    job_fraud_stats_filtered = job_fraud_stats[job_fraud_stats['total_txns'] >= min_txns]\n",
        "    \n",
        "    if len(job_fraud_stats_filtered) > 0:\n",
        "        riskiest_job_filtered = job_fraud_stats_filtered.iloc[0]\n",
        "        safest_job_filtered = job_fraud_stats_filtered.iloc[-1]\n",
        "        print(f\"\\nðŸ“Š KEY INSIGHTS (Jobs with â‰¥{min_txns} transactions):\")\n",
        "        print(f\"Riskiest job:  {riskiest_job_filtered['job']} ({riskiest_job_filtered['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "        print(f\"Safest job:    {safest_job_filtered['job']} ({safest_job_filtered['fraud_rate_pct']:.4f}% fraud rate)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3.3 Job-Based Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:25.934011Z",
          "iopub.status.busy": "2026-01-24T00:56:25.933753Z",
          "iopub.status.idle": "2026-01-24T00:56:26.858492Z",
          "shell.execute_reply": "2026-01-24T00:56:26.857507Z",
          "shell.execute_reply.started": "2026-01-24T00:56:25.933989Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# JOB-BASED FRAUD VISUALIZATIONS\n",
        "# ============================================================\n",
        "# Focus on top 15 riskiest jobs (by fraud rate) with minimum transaction threshold\n",
        "\n",
        "min_txns_viz = 50  # Minimum transactions for visualization\n",
        "job_fraud_viz = job_fraud_stats[job_fraud_stats['total_txns'] >= min_txns_viz].head(15)\n",
        "\n",
        "if len(job_fraud_viz) > 0:\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
        "    \n",
        "    # Chart 1: Fraud Rate by Job (Top 15 Riskiest)\n",
        "    ax1 = axes[0]\n",
        "    ax1.barh(range(len(job_fraud_viz)), job_fraud_viz['fraud_rate_pct'], \n",
        "            color='crimson', alpha=0.7)\n",
        "    ax1.set_yticks(range(len(job_fraud_viz)))\n",
        "    ax1.set_yticklabels(job_fraud_viz['job'])\n",
        "    ax1.set_xlabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "    ax1.set_title(f'Top 15 Riskiest Jobs by Fraud Rate (â‰¥{min_txns_viz} transactions)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax1.grid(axis='x', alpha=0.3)\n",
        "    ax1.invert_yaxis()\n",
        "    \n",
        "    # Chart 2: Transaction Volume vs Fraud Rate (Scatter)\n",
        "    ax2 = axes[1]\n",
        "    scatter = ax2.scatter(job_fraud_viz['total_txns'], job_fraud_viz['fraud_rate_pct'], \n",
        "                         s=100, alpha=0.6, c=job_fraud_viz['fraud_rate_pct'], \n",
        "                         cmap='Reds', edgecolors='black', linewidth=1)\n",
        "    ax2.set_xlabel('Total Transactions', fontsize=12, fontweight='bold')\n",
        "    ax2.set_ylabel('Fraud Rate (%)', fontsize=12, fontweight='bold')\n",
        "    ax2.set_title(f'Transaction Volume vs Fraud Rate (Top 15 Riskiest Jobs, â‰¥{min_txns_viz} transactions)', \n",
        "                  fontsize=14, fontweight='bold')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add job labels\n",
        "    for idx, row in job_fraud_viz.iterrows():\n",
        "        ax2.annotate(row['job'], (row['total_txns'], row['fraud_rate_pct']), \n",
        "                    fontsize=8, alpha=0.7)\n",
        "    \n",
        "    plt.colorbar(scatter, ax=ax2, label='Fraud Rate (%)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('job_fraud_patterns.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"âœ“ Visualizations saved as 'job_fraud_patterns.png'\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Not enough jobs with â‰¥{min_txns_viz} transactions for visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3.4 Key Findings Summary\n",
        "\n",
        "**Data validation:** Job coverage 100%. **494** unique jobs.\n",
        "\n",
        "**Riskiest / safest (â‰¥100 transactions):**\n",
        "- **Riskiest:** Lawyer (5.19% fraud rate, 540 txns)\n",
        "- **Safest:** Tour manager (0.00% fraud rate, 498 txns)\n",
        "\n",
        "**Top 15 riskiest jobs (â‰¥50 txns):** Lawyer (5.18%), TEFL teacher (~4.1%), Community development worker (~4.1%), Clinical cytogeneticist (~3.5%), Writer (~3.0%), plus Geneticist (molecular), Conservator (museum/gallery), Magazine journalist, Field trials officer, Civil Service administrator, Medical technical officer, Charity officer, Pharmacist (hospital), Minerals surveyor, Engineer (structural) (all ~2.2â€“2.9%). Medical technical officer and Pharmacist (hospital) have the highest transaction volume within this set but lower fraud rates.\n",
        "\n",
        "**Note:** Minimum transaction thresholds (â‰¥50 for viz, â‰¥100 for reliable stats) used to avoid small-sample noise. Raw â€œ100% fraudâ€ jobs with &lt;20 txns excluded from these summaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.4 Save Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 9 (DEMOGRAPHICS FEATURES COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 9 uses (demographics features)\n",
        "required_columns_section9 = [\"dob\", \"gender\", \"job\", \"is_fraud\"]\n",
        "\n",
        "missing_cols = [col for col in required_columns_section9 if col not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"WARNING: Missing demographics columns: {missing_cols}\")\n",
        "    print(\"  Some demographics features may not have been created.\")\n",
        "    print(\"  Checkpoint will be saved with available columns.\")\n",
        "else:\n",
        "    print(\"All Section 9 demographics columns present - saving section-level checkpoint...\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df,\n",
        "    CHECKPOINT_SECTION9,\n",
        "    \"Section 9 (Demographics Features Complete - Section-Level Checkpoint)\"\n",
        ")\n",
        "print(\"Section 9 section-level checkpoint saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.5 Customer Demographics Summary & Conclusions\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "Demographics show **actionable fraud signals**: **age** (1.64x risk ratio, 65+ vs. 36-50), **gender** (1.22x, M vs. F), and **job** (Lawyer 5.19% vs. Tour manager 0% among jobs with â‰¥100 txns). All three have 100% coverage. Older age groups and males have higher fraud rates; job risk is highly heterogeneous. Use age and job as high-priority features; keep gender with explicit bias monitoring.\n",
        "\n",
        "### Key Findings by Dimension\n",
        "\n",
        "#### 1. Age Analysis (Section 9.1)\n",
        "- **Age bins:** 18-25, 26-35, 36-50, 51-65, 65+\n",
        "- **Coverage:** DOB 100%; valid ages (18â€“100) 98.80%\n",
        "- **Key patterns:**\n",
        "  - Highest risk: **65+** (0.77% fraud rate)\n",
        "  - Lowest risk: **36-50** (0.47% fraud rate)\n",
        "  - Risk ratio: **1.64x** (peak vs. safest)\n",
        "- 51-65 and 65+ above overall average; 36-50 has highest volume but lowest rate.\n",
        "\n",
        "#### 2. Gender Analysis (Section 9.2)\n",
        "- **Coverage:** 100% (F: 709,863; M: 586,812 txns)\n",
        "- **Key patterns:**\n",
        "  - **M** highest risk (0.64%), **F** lowest (0.53%); **1.22x** ratio\n",
        "  - M above overall average (~0.58%), F below\n",
        "- Statistical patterns only; use with bias and fairness care.\n",
        "\n",
        "#### 3. Job/Occupation Analysis (Section 9.3)\n",
        "- **Unique jobs:** 494; **Coverage:** 100%\n",
        "- **Key patterns:**\n",
        "  - **Riskiest (â‰¥100 txns):** Lawyer (5.19%); **Safest:** Tour manager (0.00%)\n",
        "  - Top risk roles include TEFL teacher, Community development worker, Clinical cytogeneticist, Writer (~2.2â€“4.1%)\n",
        "- Minimum transaction threshold (â‰¥50 viz, â‰¥100 for stable stats) applied.\n",
        "\n",
        "### Feature Engineering Implications\n",
        "\n",
        "**High-value features to keep:**\n",
        "- **Age bins** (categorical) and **age** (continuous if needed)\n",
        "- **Job/Occupation** (frequency or target encoding; group rare jobs)\n",
        "- **Gender** (categorical, with bias monitoring)\n",
        "\n",
        "**Considerations:**\n",
        "- Age bins preserve ordinal structure; 65+ and 51-65 are strong risk signals.\n",
        "- Job encoding should use â‰¥100-txn filter and rare-job grouping.\n",
        "- Gender: monitor fairness and avoid causal interpretation.\n",
        "\n",
        "### Statistical Significance Assessment\n",
        "\n",
        "- **Age:** 1.64x variation across bins; 65+ and 51-65 clearly elevated.\n",
        "- **Gender:** 1.22x variation; moderate signal, use with caution.\n",
        "- **Job:** Large variation (e.g. Lawyer 5.19% vs. Tour manager 0%); high cardinality requires thresholding and grouping.\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "1. **Age:** Use age bins; consider continuous age for non-linear models.\n",
        "2. **Job:** Apply â‰¥100-txn threshold before encoding; group low-volume jobs.\n",
        "3. **Gender:** Track fairness metrics; avoid unwarranted causal claims.\n",
        "4. **Interactions:** Explore age Ã— job, age Ã— gender in later modeling.\n",
        "\n",
        "---\n",
        "\n",
        "**Section 9 Complete**\n",
        "\n",
        "**Next Section:** 10.0 - Credit Card Analysis (if needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Credit Card Analysis\n",
        "\n",
        "## 10.0 Load Checkpoint & Helper Functions\n",
        "\n",
        "### Purpose\n",
        "Load the required DataFrame from Section 9 checkpoint (or use existing `train_df` if available).\n",
        "Analyze fraud patterns based on credit card characteristics:\n",
        "- Card age (time since first transaction)\n",
        "- Transaction history (count per card)\n",
        "- Usage patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT: SECTION 10 (CREDIT CARD ANALYSIS)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns for Section 10\n",
        "required_columns_section10 = [\n",
        "    \"cc_num\",\n",
        "    \"is_fraud\",\n",
        "    \"merchant_local_time\"  # or customer_local_time for timestamp\n",
        "]\n",
        "\n",
        "# Try to load from Section 9 checkpoint (preferred)\n",
        "df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_SECTION9,\n",
        "    required_columns=required_columns_section10,\n",
        "    cell_name=\"Section 9 (preferred for Section 10)\"\n",
        ")\n",
        "\n",
        "if loaded_from_checkpoint:\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "else:\n",
        "    # Fallback: Try Section 8 checkpoint\n",
        "    df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "        checkpoint_path=CHECKPOINT_SECTION8,\n",
        "        required_columns=required_columns_section10,\n",
        "        cell_name=\"Section 8 (fallback for Section 10)\"\n",
        "    )\n",
        "    \n",
        "    if loaded_from_checkpoint:\n",
        "        train_df = df_loaded\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "    else:\n",
        "        # Final fallback: Use in-memory train_df\n",
        "        if 'train_df' not in globals() or train_df is None:\n",
        "            raise ValueError(\n",
        "                f\"train_df not found and checkpoint not available. \"\n",
        "                \"Run previous sections first or ensure checkpoint exists.\"\n",
        "            )\n",
        "        missing_cols = [c for c in required_columns_section10 if c not in train_df.columns]\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing required columns for Section 10: {missing_cols}\")\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(\"âœ“ train_df already in memory\")\n",
        "\n",
        "# Validate cc_num column exists\n",
        "cc_num_validation = validate_column_coverage(train_df, \"cc_num\", \"Credit Card Analysis\")\n",
        "cc_num_df, cc_num_coverage, cc_num_valid, cc_num_total = cc_num_validation\n",
        "\n",
        "# Validate timestamp column\n",
        "if \"merchant_local_time\" not in train_df.columns and \"customer_local_time\" not in train_df.columns:\n",
        "    raise ValueError(\"Neither merchant_local_time nor customer_local_time found. Run Section 6 first.\")\n",
        "\n",
        "print(f\"\\nâœ“ Ready for Credit Card Analysis\")\n",
        "print(f\"  Total rows: {TOTAL_DATASET_ROWS:,}\")\n",
        "print(f\"  cc_num coverage: {cc_num_coverage:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.1 Card Age Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- New cards (<30 days old) may have higher fraud rates\n",
        "- Established cards (180+ days) may have lower fraud rates\n",
        "\n",
        "### What We'll Analyze\n",
        "- First transaction date per card (minimum timestamp per cc_num)\n",
        "- Card age = current transaction date - first transaction date\n",
        "- Card age bins: <7 days, 7-30 days, 30-90 days, 90-180 days, 180+ days\n",
        "- Fraud rate by card age bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CALCULATE CARD AGE (TIME SINCE FIRST TRANSACTION)\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import min as spark_min, datediff, current_timestamp, col\n",
        "\n",
        "# Determine which timestamp column to use\n",
        "time_col = \"merchant_local_time\" if \"merchant_local_time\" in train_df.columns else \"customer_local_time\"\n",
        "\n",
        "# Calculate first transaction date per card\n",
        "first_txn_per_card = train_df.groupBy(\"cc_num\").agg(\n",
        "    spark_min(time_col).alias(\"first_transaction_date\")\n",
        ")\n",
        "\n",
        "# Join back to main dataframe and calculate card age\n",
        "train_df = train_df.join(first_txn_per_card, on=\"cc_num\", how=\"left\")\n",
        "\n",
        "# Calculate card age in days\n",
        "train_df = train_df.withColumn(\n",
        "    \"card_age_days\",\n",
        "    datediff(col(time_col), col(\"first_transaction_date\"))\n",
        ")\n",
        "\n",
        "# Create card age bins\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "train_df = train_df.withColumn(\n",
        "    \"card_age_bin\",\n",
        "    when(col(\"card_age_days\") < 7, \"<7 days\")\n",
        "    .when((col(\"card_age_days\") >= 7) & (col(\"card_age_days\") < 30), \"7-30 days\")\n",
        "    .when((col(\"card_age_days\") >= 30) & (col(\"card_age_days\") < 90), \"30-90 days\")\n",
        "    .when((col(\"card_age_days\") >= 90) & (col(\"card_age_days\") < 180), \"90-180 days\")\n",
        "    .otherwise(\"180+ days\")\n",
        ")\n",
        "\n",
        "print(\"âœ“ Calculated card age and age bins\")\n",
        "\n",
        "# ============================================================\n",
        "# CARD AGE FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "card_age_stats = results_manager.load_dataframe(\"10.1\", \"card_age_stats\") if \"results_manager\" in globals() else None\n",
        "if card_age_stats is not None:\n",
        "    print(\"âœ“ Loaded card_age_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    card_age_stats = aggregate_fraud_by_dimension(\n",
        "        df=train_df,\n",
        "        dimension_col=\"card_age_bin\",\n",
        "        dimension_name=\"Card Age\",\n",
        "        cache_name=\"cached_card_age_stats\",\n",
        "        save_result=True, section=\"10.1\", result_name=\"card_age_stats\"\n",
        "    )\n",
        "\n",
        "# Sort by fraud rate\n",
        "card_age_stats = card_age_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY CARD AGE\")\n",
        "print(\"=\" * 100)\n",
        "print(card_age_stats[['card_age_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "peak_age = card_age_stats.loc[card_age_stats['fraud_rate_pct'].idxmax()]\n",
        "safest_age = card_age_stats.loc[card_age_stats['fraud_rate_pct'].idxmin()]\n",
        "risk_ratio = peak_age['fraud_rate_pct'] / safest_age['fraud_rate_pct'] if safest_age['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Riskiest card age:  {peak_age['card_age_bin']} ({peak_age['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Safest card age:    {safest_age['card_age_bin']} ({safest_age['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:         {risk_ratio:.2f}x higher for newest vs. oldest\" if risk_ratio != float('inf') else \"Risk ratio:         N/A (safest has 0% fraud)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1.1 Key Findings Summary\n",
        "\n",
        "**Riskiest card age:** <7 days (6.55% fraud rate)\n",
        "\n",
        "**Safest card age:** 90-180 days (0.48% fraud rate)\n",
        "\n",
        "**Risk ratio:** 13.77x higher for newest vs. oldest\n",
        "\n",
        "**Key patterns:**\n",
        "- New cards (<7 days or 7-30 days) show much higher fraud rates\n",
        "- Established cards (90-180 days, 180+ days) show lowest fraud rates\n",
        "\n",
        "**Next:** Section 10.1.2 - Validation: Card Age Calculation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1.2 Validation: Card Age Calculation\n",
        "\n",
        "**Purpose:** Verify card age calculation logic to ensure correctness.\n",
        "\n",
        "**Validation Tasks:**\n",
        "1. Verify first_transaction_date calculation uses min() correctly\n",
        "2. Check datediff produces positive values\n",
        "3. Sample 5-10 cards and manually verify card_age_days\n",
        "4. Document 90-180 vs 180+ distribution findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Verify Card Age Calculation Logic\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, min as spark_min, max as spark_max, avg, percentile_approx\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Card Age Calculation\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Verify first_transaction_date calculation\n",
        "print(\"\\n1. Verifying first_transaction_date calculation:\")\n",
        "print(f\"   Using time column: {time_col}\")\n",
        "\n",
        "# Check that first_transaction_date is actually the minimum\n",
        "sample_cards = first_txn_per_card.limit(10)\n",
        "print(\"\\n   Sample first_transaction_date values:\")\n",
        "sample_cards.show(10, truncate=False)\n",
        "\n",
        "# For each sample card, verify first_transaction_date matches min(time_col)\n",
        "print(\"\\n2. Manually verifying first_transaction_date matches min(time_col) for sample cards:\")\n",
        "match_count = 0\n",
        "mismatch_count = 0\n",
        "\n",
        "for row in sample_cards.collect():\n",
        "    cc_num = row['cc_num']\n",
        "    expected_first_date = row['first_transaction_date']\n",
        "    \n",
        "    # Get actual min time for this card from train_df\n",
        "    actual_min_time = train_df.filter(col(\"cc_num\") == cc_num).agg(\n",
        "        spark_min(time_col).alias(\"actual_min\")\n",
        "    ).collect()[0]['actual_min']\n",
        "    \n",
        "    if expected_first_date == actual_min_time:\n",
        "        match_count += 1\n",
        "        status = \"âœ“ MATCH\"\n",
        "    else:\n",
        "        mismatch_count += 1\n",
        "        status = \"âœ— MISMATCH\"\n",
        "    \n",
        "    print(f\"   Card {str(cc_num)[:8]}...: Expected={expected_first_date}, Actual={actual_min_time} {status}\")\n",
        "\n",
        "print(f\"\\n   Summary: {match_count} matches, {mismatch_count} mismatches out of {sample_cards.count()} sample cards\")\n",
        "\n",
        "# 3. Verify datediff produces positive values\n",
        "print(\"\\n3. Verifying card_age_days calculation (datediff):\")\n",
        "card_age_stats = train_df.select(\n",
        "    spark_min(\"card_age_days\").alias(\"min_age\"),\n",
        "    spark_max(\"card_age_days\").alias(\"max_age\"),\n",
        "    avg(\"card_age_days\").alias(\"avg_age\"),\n",
        "    percentile_approx(\"card_age_days\", 0.5).alias(\"median_age\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"   Min card_age_days: {card_age_stats['min_age']}\")\n",
        "print(f\"   Max card_age_days: {card_age_stats['max_age']}\")\n",
        "print(f\"   Average card_age_days: {card_age_stats['avg_age']:.2f}\")\n",
        "print(f\"   Median card_age_days: {card_age_stats['median_age']}\")\n",
        "\n",
        "# Check for negative or zero values (should be >= 0, with 0 being the first transaction)\n",
        "negative_count = train_df.filter(col(\"card_age_days\") < 0).count()\n",
        "zero_count = train_df.filter(col(\"card_age_days\") == 0).count()\n",
        "print(f\"\\n   Negative card_age_days: {negative_count} (should be 0)\")\n",
        "print(f\"   Zero card_age_days: {zero_count} (first transaction per card)\")\n",
        "\n",
        "if negative_count > 0:\n",
        "    print(\"   âš  WARNING: Found negative card_age_days values!\")\n",
        "else:\n",
        "    print(\"   âœ“ All card_age_days values are non-negative\")\n",
        "\n",
        "# 4. Sample cards manually to verify card_age_days\n",
        "print(\"\\n4. Manual verification: Sample cards with card_age_days:\")\n",
        "sample_for_verification = train_df.select(\n",
        "    \"cc_num\", \"first_transaction_date\", time_col, \"card_age_days\", \"card_age_bin\"\n",
        ").distinct().limit(10).toPandas()\n",
        "\n",
        "print(\"\\n   Sample cards:\")\n",
        "for idx, row in sample_for_verification.iterrows():\n",
        "    cc_num_str = str(row['cc_num'])[:8] if row['cc_num'] is not None else \"N/A\"\n",
        "    print(f\"   Card {cc_num_str}...: first_txn={row['first_transaction_date']}, \"\n",
        "          f\"current_txn={row[time_col]}, age_days={row['card_age_days']}, bin={row['card_age_bin']}\")\n",
        "\n",
        "print(\"\\nâœ“ Card age calculation validation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Document Card Age Distribution (90-180 vs 180+)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Card Age Distribution Analysis (90-180 vs 180+ days)\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Compare 90-180 days and 180+ days bins\n",
        "age_bins_to_compare = [\"90-180 days\", \"180+ days\"]\n",
        "\n",
        "print(\"\\n1. Fraud rate comparison for 90-180 days vs 180+ days:\")\n",
        "comparison_stats = card_age_stats[card_age_stats['card_age_bin'].isin(age_bins_to_compare)].copy()\n",
        "comparison_stats = comparison_stats.sort_values('card_age_bin')\n",
        "\n",
        "print(comparison_stats[['card_age_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "\n",
        "# Calculate difference\n",
        "if len(comparison_stats) == 2:\n",
        "    bin_90_180 = comparison_stats[comparison_stats['card_age_bin'] == '90-180 days'].iloc[0]\n",
        "    bin_180_plus = comparison_stats[comparison_stats['card_age_bin'] == '180+ days'].iloc[0]\n",
        "    \n",
        "    fraud_rate_diff = abs(bin_90_180['fraud_rate_pct'] - bin_180_plus['fraud_rate_pct'])\n",
        "    \n",
        "    print(f\"\\n2. Fraud rate difference: {fraud_rate_diff:.4f} percentage points\")\n",
        "    \n",
        "    if fraud_rate_diff < 0.5:  # Less than 0.5% difference\n",
        "        print(\"   âœ“ Both bins show similar low fraud rates (expected behavior)\")\n",
        "        print(\"   âœ“ This confirms that established cards (90+ days) have consistently low fraud risk\")\n",
        "    else:\n",
        "        print(f\"   âš  Note: There is a {fraud_rate_diff:.4f}% difference between bins\")\n",
        "    \n",
        "    print(f\"\\n3. Key finding:\")\n",
        "    print(f\"   - 90-180 days: {bin_90_180['fraud_rate_pct']:.4f}% fraud rate ({bin_90_180['total_txns']} transactions)\")\n",
        "    print(f\"   - 180+ days: {bin_180_plus['fraud_rate_pct']:.4f}% fraud rate ({bin_180_plus['total_txns']} transactions)\")\n",
        "    print(f\"   - Both bins represent established cards with low fraud risk\")\n",
        "\n",
        "# Distribution of card ages in these bins\n",
        "print(\"\\n4. Card age distribution within 90-180 and 180+ bins:\")\n",
        "for bin_name in age_bins_to_compare:\n",
        "    bin_df = train_df.filter(col(\"card_age_bin\") == bin_name)\n",
        "    if bin_df.count() > 0:\n",
        "        bin_age_stats = bin_df.select(\n",
        "            spark_min(\"card_age_days\").alias(\"min_age\"),\n",
        "            spark_max(\"card_age_days\").alias(\"max_age\"),\n",
        "            avg(\"card_age_days\").alias(\"avg_age\"),\n",
        "            percentile_approx(\"card_age_days\", 0.5).alias(\"median_age\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        print(f\"\\n   {bin_name}:\")\n",
        "        print(f\"   - Min age: {bin_age_stats['min_age']} days\")\n",
        "        print(f\"   - Max age: {bin_age_stats['max_age']} days\")\n",
        "        print(f\"   - Average age: {bin_age_stats['avg_age']:.2f} days\")\n",
        "        print(f\"   - Median age: {bin_age_stats['median_age']} days\")\n",
        "\n",
        "print(\"\\nâœ“ Card age distribution validation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Transaction History Analysis\n",
        "\n",
        "### Hypothesis\n",
        "- Cards with very few transactions (1-5) may show higher fraud rates (fraudsters testing cards)\n",
        "- Cards with many transactions (100+) may show lower fraud rates (established usage patterns)\n",
        "\n",
        "### What We'll Analyze\n",
        "- Transaction count per card (groupBy cc_num, count transactions)\n",
        "- Transaction count bins: 1-5, 6-20, 21-50, 51-100, 100+\n",
        "- Fraud rate by transaction count bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CALCULATE TRANSACTION COUNT PER CARD\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import count as spark_count\n",
        "\n",
        "# Calculate transaction count per card\n",
        "txn_count_per_card = train_df.groupBy(\"cc_num\").agg(\n",
        "    spark_count(\"*\").alias(\"transaction_count\")\n",
        ")\n",
        "\n",
        "# Join back to main dataframe\n",
        "train_df = train_df.join(txn_count_per_card, on=\"cc_num\", how=\"left\")\n",
        "\n",
        "# Create transaction count bins\n",
        "train_df = train_df.withColumn(\n",
        "    \"transaction_count_bin\",\n",
        "    when(col(\"transaction_count\") <= 5, \"1-5\")\n",
        "    .when((col(\"transaction_count\") > 5) & (col(\"transaction_count\") <= 20), \"6-20\")\n",
        "    .when((col(\"transaction_count\") > 20) & (col(\"transaction_count\") <= 50), \"21-50\")\n",
        "    .when((col(\"transaction_count\") > 50) & (col(\"transaction_count\") <= 100), \"51-100\")\n",
        "    .otherwise(\"100+\")\n",
        ")\n",
        "\n",
        "print(\"âœ“ Calculated transaction count per card and count bins\")\n",
        "\n",
        "# ============================================================\n",
        "# TRANSACTION COUNT FRAUD AGGREGATION\n",
        "# ============================================================\n",
        "\n",
        "transaction_count_stats = results_manager.load_dataframe(\"10.2\", \"transaction_count_stats\") if \"results_manager\" in globals() else None\n",
        "if transaction_count_stats is not None:\n",
        "    print(\"âœ“ Loaded transaction_count_stats from saved results\")\n",
        "else:\n",
        "    print(\"No saved results found - will compute\")\n",
        "    transaction_count_stats = aggregate_fraud_by_dimension(\n",
        "        df=train_df,\n",
        "        dimension_col=\"transaction_count_bin\",\n",
        "        dimension_name=\"Transaction Count\",\n",
        "        cache_name=\"cached_transaction_count_stats\",\n",
        "        save_result=True, section=\"10.2\", result_name=\"transaction_count_stats\"\n",
        "    )\n",
        "\n",
        "# Sort by fraud rate\n",
        "transaction_count_stats = transaction_count_stats.sort_values(\"fraud_rate_pct\", ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FRAUD STATISTICS BY TRANSACTION COUNT\")\n",
        "print(\"=\" * 100)\n",
        "print(transaction_count_stats[['transaction_count_bin', 'total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Key insights\n",
        "peak_count = transaction_count_stats.loc[transaction_count_stats['fraud_rate_pct'].idxmax()]\n",
        "safest_count = transaction_count_stats.loc[transaction_count_stats['fraud_rate_pct'].idxmin()]\n",
        "risk_ratio = peak_count['fraud_rate_pct'] / safest_count['fraud_rate_pct'] if safest_count['fraud_rate_pct'] > 0 else float('inf')\n",
        "\n",
        "print(f\"\\nðŸ“Š KEY INSIGHTS:\")\n",
        "print(f\"Riskiest transaction count:  {peak_count['transaction_count_bin']} ({peak_count['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Safest transaction count:    {safest_count['transaction_count_bin']} ({safest_count['fraud_rate_pct']:.4f}% fraud rate)\")\n",
        "print(f\"Risk ratio:                  {risk_ratio:.2f}x higher for low-volume vs. high-volume\" if risk_ratio != float('inf') else \"Risk ratio:                  N/A (safest has 0% fraud)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2.1 Key Findings Summary\n",
        "\n",
        "**Riskiest transaction count:** 6-20 (100% fraud rate; 741 txns, all fraud)\n",
        "\n",
        "**Safest transaction count:** 100+ (0.52% fraud rate)\n",
        "\n",
        "**Risk ratio:** 191.57x higher for low-volume vs. high-volume\n",
        "\n",
        "**Key patterns:**\n",
        "- Low-volume cards (6-20 transactions) show extreme fraud concentration\n",
        "- High-volume cards (100+ transactions) show lowest fraud rates\n",
        "\n",
        "**Next:** Section 10.2.2 - Validation: Transaction Count Calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2.2 Validation: Transaction Count Calculation\n",
        "\n",
        "**Purpose:** Validate the transaction_count calculation logic to ensure correctness before trusting the results.\n",
        "\n",
        "**Validation Tasks:**\n",
        "1. Verify groupBy/agg logic for transaction count calculation\n",
        "2. Sample cards manually to verify counts match expected values\n",
        "3. Verify join correctness (no NULLs, counts match)\n",
        "4. Check transaction_count distribution and binning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION 1: Verify groupBy/agg Logic\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION 1: Transaction Count Calculation Logic\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check txn_count_per_card structure\n",
        "print(\"\\n1. Checking txn_count_per_card DataFrame structure:\")\n",
        "print(f\"   Columns: {txn_count_per_card.columns}\")\n",
        "print(f\"   Number of unique cards: {txn_count_per_card.count()}\")\n",
        "\n",
        "# Check for NULLs in transaction_count\n",
        "null_count = txn_count_per_card.filter(col(\"transaction_count\").isNull()).count()\n",
        "print(f\"\\n2. NULL transaction_count values in txn_count_per_card: {null_count}\")\n",
        "\n",
        "# Check transaction_count statistics\n",
        "print(\"\\n3. Transaction count statistics:\")\n",
        "txn_count_stats = txn_count_per_card.select(\n",
        "    spark_min(\"transaction_count\").alias(\"min_count\"),\n",
        "    spark_max(\"transaction_count\").alias(\"max_count\"),\n",
        "    avg(\"transaction_count\").alias(\"avg_count\"),\n",
        "    percentile_approx(\"transaction_count\", 0.5).alias(\"median_count\"),\n",
        "    percentile_approx(\"transaction_count\", 0.25).alias(\"p25_count\"),\n",
        "    percentile_approx(\"transaction_count\", 0.75).alias(\"p75_count\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"   Min: {txn_count_stats['min_count']}\")\n",
        "print(f\"   Max: {txn_count_stats['max_count']}\")\n",
        "print(f\"   Average: {txn_count_stats['avg_count']:.2f}\")\n",
        "print(f\"   Median: {txn_count_stats['median_count']}\")\n",
        "print(f\"   25th percentile: {txn_count_stats['p25_count']}\")\n",
        "print(f\"   75th percentile: {txn_count_stats['p75_count']}\")\n",
        "\n",
        "# Check distribution of transaction counts\n",
        "print(\"\\n4. Transaction count distribution:\")\n",
        "txn_count_dist = txn_count_per_card.groupBy(\"transaction_count\").agg(\n",
        "    spark_count(\"*\").alias(\"num_cards\")\n",
        ").orderBy(\"transaction_count\").limit(20).toPandas()\n",
        "print(txn_count_dist.to_string(index=False))\n",
        "\n",
        "print(\"\\nâœ“ Validation 1 complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION 2: Sample Cards Manually to Verify Counts\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION 2: Manual Card Sampling\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Sample a few cards from different transaction count ranges\n",
        "print(\"\\n1. Sampling cards from different transaction count ranges:\")\n",
        "\n",
        "# Get sample cards with low, medium, and high transaction counts\n",
        "sample_cards = txn_count_per_card.orderBy(\"transaction_count\").limit(5).union(\n",
        "    txn_count_per_card.filter(col(\"transaction_count\").between(10, 20)).limit(5)\n",
        ").union(\n",
        "    txn_count_per_card.orderBy(col(\"transaction_count\").desc()).limit(5)\n",
        ").select(\"cc_num\", \"transaction_count\").distinct().limit(10).toPandas()\n",
        "\n",
        "print(\"\\n   Sample cards and their transaction counts:\")\n",
        "print(sample_cards.to_string(index=False))\n",
        "\n",
        "# For each sample card, manually count transactions in train_df\n",
        "print(\"\\n2. Manually verifying transaction counts for sample cards:\")\n",
        "print(\"   (Comparing txn_count_per_card with actual count in train_df)\")\n",
        "\n",
        "match_count = 0\n",
        "mismatch_count = 0\n",
        "\n",
        "for idx, row in sample_cards.iterrows():\n",
        "    cc_num = row['cc_num']\n",
        "    expected_count = row['transaction_count']\n",
        "    \n",
        "    # Count transactions for this card in train_df (after join)\n",
        "    card_df = train_df.filter(col(\"cc_num\") == cc_num)\n",
        "    actual_count_in_joined = card_df.count()\n",
        "    \n",
        "    # Get transaction_count from joined dataframe\n",
        "    txn_count_row = card_df.select(\"transaction_count\").first()\n",
        "    txn_count_value = txn_count_row[\"transaction_count\"] if txn_count_row else None\n",
        "    \n",
        "    # All three should match: expected from txn_count_per_card, value from join, actual row count\n",
        "    all_match = (expected_count == txn_count_value == actual_count_in_joined)\n",
        "    match_status = \"âœ“ MATCH\" if all_match else \"âœ— MISMATCH\"\n",
        "    \n",
        "    if all_match:\n",
        "        match_count += 1\n",
        "    else:\n",
        "        mismatch_count += 1\n",
        "    \n",
        "    print(f\"   Card {cc_num[:8]}...: Expected={expected_count}, Joined={txn_count_value}, Actual rows={actual_count_in_joined} {match_status}\")\n",
        "\n",
        "print(f\"\\n   Summary: {match_count} matches, {mismatch_count} mismatches out of {len(sample_cards)} sample cards\")\n",
        "\n",
        "print(\"\\nâœ“ Validation 2 complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION 3: Verify Join Correctness\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION 3: Join Correctness Check\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check for NULL transaction_count values after join\n",
        "null_after_join = train_df.filter(col(\"transaction_count\").isNull()).count()\n",
        "total_rows = train_df.count()\n",
        "print(f\"\\n1. NULL transaction_count values after join: {null_after_join} / {total_rows}\")\n",
        "\n",
        "if null_after_join > 0:\n",
        "    print(\"   âš  WARNING: Found NULL transaction_count values after join!\")\n",
        "    print(\"   This suggests some cards in train_df don't have matching entries in txn_count_per_card\")\n",
        "    sample_nulls = train_df.filter(col(\"transaction_count\").isNull()).select(\"cc_num\").limit(5).toPandas()\n",
        "    print(f\"   Sample cards with NULL transaction_count: {sample_nulls['cc_num'].tolist()}\")\n",
        "else:\n",
        "    print(\"   âœ“ No NULL values found - join successful for all rows\")\n",
        "\n",
        "# Verify that transaction_count matches actual row count per card\n",
        "print(\"\\n2. Verifying transaction_count matches actual row count per card:\")\n",
        "\n",
        "# Sample a few cards and compare\n",
        "sample_verification = train_df.groupBy(\"cc_num\", \"transaction_count\").agg(\n",
        "    spark_count(\"*\").alias(\"actual_row_count\")\n",
        ").withColumn(\n",
        "    \"count_match\",\n",
        "    when(col(\"transaction_count\") == col(\"actual_row_count\"), \"MATCH\").otherwise(\"MISMATCH\")\n",
        ").filter(col(\"count_match\") == \"MISMATCH\").limit(10)\n",
        "\n",
        "mismatch_count = sample_verification.count()\n",
        "if mismatch_count > 0:\n",
        "    print(f\"   âš  WARNING: Found {mismatch_count} cards where transaction_count doesn't match actual row count!\")\n",
        "    print(\"   Sample mismatches:\")\n",
        "    sample_verification.show(10)\n",
        "else:\n",
        "    print(\"   âœ“ All sampled cards show matching transaction_count and actual row count\")\n",
        "\n",
        "# Check join type and cardinality\n",
        "print(\"\\n3. Join cardinality check:\")\n",
        "cards_in_txn_count = txn_count_per_card.select(\"cc_num\").distinct().count()\n",
        "cards_in_train_df = train_df.select(\"cc_num\").distinct().count()\n",
        "print(f\"   Unique cards in txn_count_per_card: {cards_in_txn_count}\")\n",
        "print(f\"   Unique cards in train_df: {cards_in_train_df}\")\n",
        "\n",
        "if cards_in_txn_count == cards_in_train_df:\n",
        "    print(\"   âœ“ All cards in train_df have transaction counts calculated\")\n",
        "else:\n",
        "    print(f\"   âš  Difference: {abs(cards_in_txn_count - cards_in_train_df)} cards\")\n",
        "    print(\"   This is expected if some cards were filtered out or if there are duplicates\")\n",
        "\n",
        "print(\"\\nâœ“ Validation 3 complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION 4: Check Transaction Count Distribution and Binning\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION 4: Transaction Count Distribution and Binning\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check distribution of transaction_count_bin\n",
        "print(\"\\n1. Distribution of transaction_count_bin:\")\n",
        "bin_dist = train_df.groupBy(\"transaction_count_bin\").agg(\n",
        "    spark_count(\"*\").alias(\"num_transactions\"),\n",
        "    spark_count(\"cc_num\").alias(\"num_unique_cards\")\n",
        ").orderBy(\"transaction_count_bin\").toPandas()\n",
        "\n",
        "print(bin_dist.to_string(index=False))\n",
        "\n",
        "# Check for cards in each bin range\n",
        "print(\"\\n2. Verifying bin boundaries are correct:\")\n",
        "print(\"   Checking sample transactions from each bin:\")\n",
        "\n",
        "for bin_name in [\"1-5\", \"6-20\", \"21-50\", \"51-100\", \"100+\"]:\n",
        "    sample = train_df.filter(col(\"transaction_count_bin\") == bin_name).select(\n",
        "        \"cc_num\", \"transaction_count\", \"transaction_count_bin\", \"is_fraud\"\n",
        "    ).limit(3).toPandas()\n",
        "    \n",
        "    if len(sample) > 0:\n",
        "        print(f\"\\n   Bin '{bin_name}':\")\n",
        "        print(f\"   {sample.to_string(index=False)}\")\n",
        "        \n",
        "        # Verify bin boundaries\n",
        "        if bin_name == \"1-5\":\n",
        "            valid = all((sample['transaction_count'] <= 5).all())\n",
        "        elif bin_name == \"6-20\":\n",
        "            valid = all((sample['transaction_count'] > 5) & (sample['transaction_count'] <= 20))\n",
        "        elif bin_name == \"21-50\":\n",
        "            valid = all((sample['transaction_count'] > 20) & (sample['transaction_count'] <= 50))\n",
        "        elif bin_name == \"51-100\":\n",
        "            valid = all((sample['transaction_count'] > 50) & (sample['transaction_count'] <= 100))\n",
        "        else:  # 100+\n",
        "            valid = all(sample['transaction_count'] > 100)\n",
        "        \n",
        "        status = \"âœ“ Valid\" if valid else \"âœ— Invalid\"\n",
        "        print(f\"   Boundary check: {status}\")\n",
        "    else:\n",
        "        print(f\"\\n   Bin '{bin_name}': EMPTY (no transactions in this bin)\")\n",
        "\n",
        "# Check the problematic 6-20 bin specifically\n",
        "print(\"\\n3. Detailed analysis of 6-20 bin (reported as 100% fraud):\")\n",
        "bin_6_20 = train_df.filter(col(\"transaction_count_bin\") == \"6-20\")\n",
        "bin_6_20_count = bin_6_20.count()\n",
        "bin_6_20_fraud = bin_6_20.filter(col(\"is_fraud\") == 1).count()\n",
        "bin_6_20_fraud_rate = (bin_6_20_fraud / bin_6_20_count * 100) if bin_6_20_count > 0 else 0\n",
        "\n",
        "print(f\"   Total transactions in 6-20 bin: {bin_6_20_count}\")\n",
        "print(f\"   Fraud transactions: {bin_6_20_fraud}\")\n",
        "print(f\"   Fraud rate: {bin_6_20_fraud_rate:.2f}%\")\n",
        "\n",
        "# Sample transactions from 6-20 bin\n",
        "print(\"\\n   Sample transactions from 6-20 bin:\")\n",
        "sample_6_20 = bin_6_20.select(\n",
        "    \"cc_num\", \"transaction_count\", \"transaction_count_bin\", \"is_fraud\", \"amt\", \"category\"\n",
        ").limit(10).toPandas()\n",
        "print(sample_6_20.to_string(index=False))\n",
        "\n",
        "# Check if all transactions in 6-20 bin are from the same card(s)\n",
        "print(\"\\n   Unique cards in 6-20 bin:\")\n",
        "unique_cards_6_20 = bin_6_20.select(\"cc_num\").distinct().count()\n",
        "print(f\"   Number of unique cards: {unique_cards_6_20}\")\n",
        "\n",
        "if unique_cards_6_20 <= 5:\n",
        "    print(\"   âš  WARNING: Very few unique cards in 6-20 bin - may indicate data quality issue\")\n",
        "    card_list = bin_6_20.select(\"cc_num\").distinct().limit(10).toPandas()\n",
        "    print(f\"   Sample cards: {card_list['cc_num'].tolist()}\")\n",
        "\n",
        "print(\"\\nâœ“ Validation 4 complete\")\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"VALIDATION SUMMARY\")\n",
        "print(\"=\" * 100)\n",
        "print(\"All validation checks completed. Review the results above to identify any issues.\")\n",
        "print(\"=\" * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2.3 Additional Validation: Transaction Count Analysis\n",
        "\n",
        "**Purpose:** Comprehensive validation of transaction count binning logic and investigation of the 6-20 bin anomaly.\n",
        "\n",
        "**Validation Tasks:**\n",
        "1. Validate binning boundaries and edge cases\n",
        "2. Investigate 6-20 bin (sample transactions, check fraud values, unique cards)\n",
        "3. Check for data leakage (time period analysis, sampling bias)\n",
        "4. Analyze transaction_count distribution (min/max/percentiles, missing bins)\n",
        "5. Verify binning coverage and document dataset structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Transaction Count Binning Logic\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, when, count as spark_count, min as spark_min, max as spark_max, avg, percentile_approx\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Transaction Count Binning Logic\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Validate bin boundaries\n",
        "print(\"\\n1. Validating bin boundaries:\")\n",
        "\n",
        "# Check each bin's boundaries\n",
        "bin_boundaries = {\n",
        "    \"1-5\": (1, 5),\n",
        "    \"6-20\": (6, 20),\n",
        "    \"21-50\": (21, 50),\n",
        "    \"51-100\": (51, 100),\n",
        "    \"100+\": (101, None)  # No upper limit\n",
        "}\n",
        "\n",
        "print(\"\\n   Expected bin boundaries:\")\n",
        "for bin_name, (min_val, max_val) in bin_boundaries.items():\n",
        "    if max_val:\n",
        "        print(f\"   {bin_name}: {min_val} <= transaction_count <= {max_val}\")\n",
        "    else:\n",
        "        print(f\"   {bin_name}: transaction_count > {min_val - 1}\")\n",
        "\n",
        "# Verify binning logic matches boundaries\n",
        "print(\"\\n2. Verifying binning logic conditions:\")\n",
        "\n",
        "# Sample transactions from each bin and verify they match boundaries\n",
        "for bin_name in [\"1-5\", \"6-20\", \"21-50\", \"51-100\", \"100+\"]:\n",
        "    bin_df = train_df.filter(col(\"transaction_count_bin\") == bin_name)\n",
        "    bin_count = bin_df.count()\n",
        "    \n",
        "    if bin_count > 0:\n",
        "        # Get min and max transaction_count for this bin\n",
        "        bin_stats = bin_df.select(\n",
        "            spark_min(\"transaction_count\").alias(\"min_txn_count\"),\n",
        "            spark_max(\"transaction_count\").alias(\"max_txn_count\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        min_txn = bin_stats['min_txn_count']\n",
        "        max_txn = bin_stats['max_txn_count']\n",
        "        \n",
        "        # Verify boundaries\n",
        "        expected_min, expected_max = bin_boundaries[bin_name]\n",
        "        if expected_max:\n",
        "            valid = (min_txn >= expected_min) and (max_txn <= expected_max)\n",
        "        else:\n",
        "            valid = (min_txn >= expected_min)\n",
        "        \n",
        "        status = \"âœ“ Valid\" if valid else \"âœ— Invalid\"\n",
        "        print(f\"   {bin_name}: min={min_txn}, max={max_txn} {status}\")\n",
        "    else:\n",
        "        print(f\"   {bin_name}: EMPTY (no transactions)\")\n",
        "\n",
        "# 3. Check for NULL transaction_count values\n",
        "print(\"\\n3. Checking for NULL transaction_count values:\")\n",
        "null_count = train_df.filter(col(\"transaction_count\").isNull()).count()\n",
        "total_count = train_df.count()\n",
        "\n",
        "print(f\"   NULL transaction_count values: {null_count} / {total_count}\")\n",
        "\n",
        "if null_count > 0:\n",
        "    print(\"   âš  WARNING: Found NULL transaction_count values!\")\n",
        "    print(\"   Sample rows with NULL transaction_count:\")\n",
        "    train_df.filter(col(\"transaction_count\").isNull()).select(\n",
        "        \"cc_num\", \"transaction_count\", \"transaction_count_bin\"\n",
        "    ).limit(5).show()\n",
        "else:\n",
        "    print(\"   âœ“ No NULL transaction_count values found\")\n",
        "\n",
        "# 4. Verify when().otherwise() logic covers all cases\n",
        "print(\"\\n4. Verifying binning logic coverage:\")\n",
        "\n",
        "# Check if all transaction_count values are assigned to a bin\n",
        "unbinned_count = train_df.filter(\n",
        "    col(\"transaction_count_bin\").isNull()\n",
        ").count()\n",
        "\n",
        "print(f\"   Unbinned transactions (NULL bin): {unbinned_count}\")\n",
        "\n",
        "if unbinned_count > 0:\n",
        "    print(\"   âš  WARNING: Some transactions are not assigned to any bin!\")\n",
        "else:\n",
        "    print(\"   âœ“ All transactions are assigned to a bin\")\n",
        "\n",
        "# Check transaction_count range\n",
        "txn_count_range = train_df.select(\n",
        "    spark_min(\"transaction_count\").alias(\"min_count\"),\n",
        "    spark_max(\"transaction_count\").alias(\"max_count\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\n   Transaction count range: {txn_count_range['min_count']} to {txn_count_range['max_count']}\")\n",
        "\n",
        "print(\"\\nâœ“ Binning logic validation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Investigate 6-20 Bin (Deep Dive)\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, count as spark_count, sum as spark_sum\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Deep Dive into 6-20 Bin\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Focus on the 6-20 bin which showed 100% fraud rate\n",
        "bin_6_20 = train_df.filter(col(\"transaction_count_bin\") == \"6-20\")\n",
        "\n",
        "print(\"\\n1. 6-20 Bin Overview:\")\n",
        "bin_6_20_count = bin_6_20.count()\n",
        "bin_6_20_fraud = bin_6_20.filter(col(\"is_fraud\") == 1).count()\n",
        "bin_6_20_fraud_rate = (bin_6_20_fraud / bin_6_20_count * 100) if bin_6_20_count > 0 else 0\n",
        "\n",
        "print(f\"   Total transactions: {bin_6_20_count}\")\n",
        "print(f\"   Fraud transactions: {bin_6_20_fraud}\")\n",
        "print(f\"   Non-fraud transactions: {bin_6_20_count - bin_6_20_fraud}\")\n",
        "print(f\"   Fraud rate: {bin_6_20_fraud_rate:.2f}%\")\n",
        "\n",
        "# 2. Sample 10-20 transactions from 6-20 bin\n",
        "print(\"\\n2. Sample transactions from 6-20 bin (10-20 samples):\")\n",
        "sample_6_20 = bin_6_20.select(\n",
        "    \"cc_num\", \"transaction_count\", \"transaction_count_bin\", \n",
        "    \"is_fraud\", \"amt\", \"category\", \"merchant\", \"zip\"\n",
        ").limit(20).toPandas()\n",
        "\n",
        "print(f\"\\n   Sample of {len(sample_6_20)} transactions:\")\n",
        "print(sample_6_20.to_string(index=False))\n",
        "\n",
        "# 3. Verify is_fraud values\n",
        "print(\"\\n3. Fraud value verification:\")\n",
        "fraud_dist = sample_6_20['is_fraud'].value_counts()\n",
        "print(f\"   Fraud distribution in sample:\")\n",
        "for fraud_val, count in fraud_dist.items():\n",
        "    print(f\"   is_fraud={fraud_val}: {count} transactions\")\n",
        "\n",
        "# Check if all are actually fraud\n",
        "all_fraud = (sample_6_20['is_fraud'] == 1).all()\n",
        "print(f\"\\n   All sample transactions are fraud: {all_fraud}\")\n",
        "\n",
        "if not all_fraud:\n",
        "    non_fraud_samples = sample_6_20[sample_6_20['is_fraud'] == 0]\n",
        "    print(f\"   âš  Found {len(non_fraud_samples)} non-fraud transactions in sample\")\n",
        "    print(f\"   Non-fraud sample transactions:\")\n",
        "    print(non_fraud_samples.to_string(index=False))\n",
        "\n",
        "# 4. Check unique cards in 6-20 bin\n",
        "print(\"\\n4. Unique cards analysis:\")\n",
        "unique_cards_6_20 = bin_6_20.select(\"cc_num\").distinct().count()\n",
        "print(f\"   Unique cards in 6-20 bin: {unique_cards_6_20}\")\n",
        "\n",
        "# Get transaction count distribution per card\n",
        "cards_in_6_20 = bin_6_20.groupBy(\"cc_num\").agg(\n",
        "    spark_count(\"*\").alias(\"txn_count\"),\n",
        "    spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
        ").orderBy(\"txn_count\").toPandas()\n",
        "\n",
        "print(f\"\\n   Cards in 6-20 bin (showing first 10):\")\n",
        "print(cards_in_6_20.head(10).to_string(index=False))\n",
        "\n",
        "if len(cards_in_6_20) <= 5:\n",
        "    print(f\"\\n   âš  WARNING: Only {len(cards_in_6_20)} unique cards in 6-20 bin\")\n",
        "    print(f\"   This suggests the bin may be dominated by a few specific cards\")\n",
        "\n",
        "# 5. Check transaction_count values in 6-20 bin\n",
        "print(\"\\n5. Transaction count distribution in 6-20 bin:\")\n",
        "txn_count_dist_6_20 = bin_6_20.groupBy(\"transaction_count\").agg(\n",
        "    spark_count(\"*\").alias(\"num_txns\"),\n",
        "    spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
        ").orderBy(\"transaction_count\").toPandas()\n",
        "\n",
        "print(txn_count_dist_6_20.to_string(index=False))\n",
        "\n",
        "print(\"\\nâœ“ 6-20 bin investigation complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Check for Data Leakage / Sampling Bias\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, min as spark_min, max as spark_max, count as spark_count\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Data Leakage / Sampling Bias Check\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Check if 6-20 bin cards are from specific time period\n",
        "print(\"\\n1. Time period analysis for 6-20 bin cards:\")\n",
        "\n",
        "# Determine which timestamp column to use\n",
        "time_col = \"merchant_local_time\" if \"merchant_local_time\" in train_df.columns else \"customer_local_time\"\n",
        "\n",
        "# Get time range for 6-20 bin transactions\n",
        "bin_6_20 = train_df.filter(col(\"transaction_count_bin\") == \"6-20\")\n",
        "time_range_6_20 = bin_6_20.select(\n",
        "    spark_min(time_col).alias(\"min_time\"),\n",
        "    spark_max(time_col).alias(\"max_time\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"   6-20 bin time range:\")\n",
        "print(f\"   Min time: {time_range_6_20['min_time']}\")\n",
        "print(f\"   Max time: {time_range_6_20['max_time']}\")\n",
        "\n",
        "# Compare with overall dataset time range\n",
        "overall_time_range = train_df.select(\n",
        "    spark_min(time_col).alias(\"min_time\"),\n",
        "    spark_max(time_col).alias(\"max_time\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\n   Overall dataset time range:\")\n",
        "print(f\"   Min time: {overall_time_range['min_time']}\")\n",
        "print(f\"   Max time: {overall_time_range['max_time']}\")\n",
        "\n",
        "# Check if 6-20 bin is concentrated in a specific time period\n",
        "if time_range_6_20['min_time'] == time_range_6_20['max_time']:\n",
        "    print(\"\\n   âš  WARNING: All 6-20 bin transactions are from the same timestamp!\")\n",
        "    print(\"   This suggests potential data leakage or sampling bias\")\n",
        "elif (time_range_6_20['max_time'] - time_range_6_20['min_time']).days < 7:\n",
        "    print(f\"\\n   âš  NOTE: 6-20 bin transactions span only {(time_range_6_20['max_time'] - time_range_6_20['min_time']).days} days\")\n",
        "    print(\"   This may indicate time-based sampling bias\")\n",
        "else:\n",
        "    print(\"\\n   âœ“ 6-20 bin transactions span a reasonable time period\")\n",
        "\n",
        "# 2. Check card creation dates for 6-20 bin cards\n",
        "print(\"\\n2. Card age analysis for 6-20 bin:\")\n",
        "if \"first_transaction_date\" in train_df.columns:\n",
        "    cards_6_20 = bin_6_20.select(\"cc_num\", \"first_transaction_date\").distinct()\n",
        "    card_age_stats_6_20 = cards_6_20.select(\n",
        "        spark_min(\"first_transaction_date\").alias(\"earliest_card\"),\n",
        "        spark_max(\"first_transaction_date\").alias(\"latest_card\"),\n",
        "        spark_count(\"*\").alias(\"num_cards\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"   Number of unique cards: {card_age_stats_6_20['num_cards']}\")\n",
        "    print(f\"   Earliest first transaction: {card_age_stats_6_20['earliest_card']}\")\n",
        "    print(f\"   Latest first transaction: {card_age_stats_6_20['latest_card']}\")\n",
        "    \n",
        "    # Check if all cards were created around the same time\n",
        "    if card_age_stats_6_20['earliest_card'] and card_age_stats_6_20['latest_card']:\n",
        "        time_diff = (card_age_stats_6_20['latest_card'] - card_age_stats_6_20['earliest_card']).days\n",
        "        if time_diff < 7:\n",
        "            print(f\"\\n   âš  NOTE: All 6-20 bin cards were created within {time_diff} days\")\n",
        "            print(\"   This may indicate sampling bias\")\n",
        "        else:\n",
        "            print(f\"\\n   âœ“ Cards span {time_diff} days (reasonable distribution)\")\n",
        "\n",
        "# 3. Check for sampling bias in other bins\n",
        "print(\"\\n3. Checking for sampling bias across all bins:\")\n",
        "for bin_name in [\"1-5\", \"6-20\", \"21-50\", \"51-100\", \"100+\"]:\n",
        "    bin_df = train_df.filter(col(\"transaction_count_bin\") == bin_name)\n",
        "    bin_count = bin_df.count()\n",
        "    \n",
        "    if bin_count > 0:\n",
        "        bin_time_range = bin_df.select(\n",
        "            spark_min(time_col).alias(\"min_time\"),\n",
        "            spark_max(time_col).alias(\"max_time\")\n",
        "        ).collect()[0]\n",
        "        \n",
        "        if bin_time_range['min_time'] and bin_time_range['max_time']:\n",
        "            time_span = (bin_time_range['max_time'] - bin_time_range['min_time']).days\n",
        "            print(f\"   {bin_name}: {bin_count} transactions, spans {time_span} days\")\n",
        "        else:\n",
        "            print(f\"   {bin_name}: {bin_count} transactions, time range unavailable\")\n",
        "\n",
        "print(\"\\nâœ“ Data leakage / sampling bias check complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Analyze Transaction Count Distribution\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, min as spark_min, max as spark_max, avg, percentile_approx, count as spark_count\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Transaction Count Distribution Analysis\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Full distribution statistics\n",
        "print(\"\\n1. Transaction count distribution statistics:\")\n",
        "txn_count_stats = train_df.select(\n",
        "    spark_min(\"transaction_count\").alias(\"min_count\"),\n",
        "    spark_max(\"transaction_count\").alias(\"max_count\"),\n",
        "    avg(\"transaction_count\").alias(\"avg_count\"),\n",
        "    percentile_approx(\"transaction_count\", 0.25).alias(\"p25\"),\n",
        "    percentile_approx(\"transaction_count\", 0.5).alias(\"median\"),\n",
        "    percentile_approx(\"transaction_count\", 0.75).alias(\"p75\"),\n",
        "    percentile_approx(\"transaction_count\", 0.90).alias(\"p90\"),\n",
        "    percentile_approx(\"transaction_count\", 0.95).alias(\"p95\"),\n",
        "    percentile_approx(\"transaction_count\", 0.99).alias(\"p99\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"   Min: {txn_count_stats['min_count']}\")\n",
        "print(f\"   25th percentile: {txn_count_stats['p25']}\")\n",
        "print(f\"   Median: {txn_count_stats['median']}\")\n",
        "print(f\"   75th percentile: {txn_count_stats['p75']}\")\n",
        "print(f\"   90th percentile: {txn_count_stats['p90']}\")\n",
        "print(f\"   95th percentile: {txn_count_stats['p95']}\")\n",
        "print(f\"   99th percentile: {txn_count_stats['p99']}\")\n",
        "print(f\"   Max: {txn_count_stats['max_count']}\")\n",
        "print(f\"   Average: {txn_count_stats['avg_count']:.2f}\")\n",
        "\n",
        "# 2. Check if 1-5, 21-50, 51-100 bins should exist based on distribution\n",
        "print(\"\\n2. Bin existence analysis:\")\n",
        "print(\"   Checking if bins should exist based on distribution:\")\n",
        "\n",
        "bin_existence = {}\n",
        "for bin_name in [\"1-5\", \"6-20\", \"21-50\", \"51-100\", \"100+\"]:\n",
        "    bin_df = train_df.filter(col(\"transaction_count_bin\") == bin_name)\n",
        "    bin_count = bin_df.count()\n",
        "    bin_existence[bin_name] = bin_count > 0\n",
        "    \n",
        "    if bin_count > 0:\n",
        "        bin_pct = (bin_count / train_df.count()) * 100\n",
        "        print(f\"   {bin_name}: {bin_count} transactions ({bin_pct:.2f}% of dataset) âœ“\")\n",
        "    else:\n",
        "        print(f\"   {bin_name}: EMPTY (0 transactions)\")\n",
        "\n",
        "# 3. Distribution by transaction_count value (first 30 values)\n",
        "print(\"\\n3. Distribution by exact transaction_count value (showing first 30):\")\n",
        "txn_count_dist = train_df.groupBy(\"transaction_count\").agg(\n",
        "    spark_count(\"*\").alias(\"num_cards\"),\n",
        "    spark_sum(\"is_fraud\").alias(\"fraud_count\")\n",
        ").orderBy(\"transaction_count\").limit(30).toPandas()\n",
        "\n",
        "print(txn_count_dist.to_string(index=False))\n",
        "\n",
        "# 4. Check for missing bins\n",
        "print(\"\\n4. Missing bin analysis:\")\n",
        "expected_bins = [\"1-5\", \"6-20\", \"21-50\", \"51-100\", \"100+\"]\n",
        "missing_bins = [bin_name for bin_name in expected_bins if not bin_existence[bin_name]]\n",
        "\n",
        "if missing_bins:\n",
        "    print(f\"   Missing bins: {', '.join(missing_bins)}\")\n",
        "    print(\"   Analysis:\")\n",
        "    \n",
        "    for bin_name in missing_bins:\n",
        "        min_val, max_val = bin_boundaries[bin_name]\n",
        "        if max_val:\n",
        "            count_in_range = train_df.filter(\n",
        "                (col(\"transaction_count\") >= min_val) & (col(\"transaction_count\") <= max_val)\n",
        "            ).count()\n",
        "        else:\n",
        "            count_in_range = train_df.filter(col(\"transaction_count\") >= min_val).count()\n",
        "        \n",
        "        print(f\"   - {bin_name}: {count_in_range} transactions in range, but bin is empty\")\n",
        "        print(f\"     This suggests a data characteristic (bimodal distribution?)\")\n",
        "else:\n",
        "    print(\"   âœ“ All expected bins have transactions\")\n",
        "\n",
        "# 5. Check for bimodal distribution\n",
        "print(\"\\n5. Distribution shape analysis:\")\n",
        "# Count transactions in low range (1-20) vs high range (100+)\n",
        "low_range_count = train_df.filter(col(\"transaction_count\") <= 20).count()\n",
        "high_range_count = train_df.filter(col(\"transaction_count\") > 100).count()\n",
        "mid_range_count = train_df.filter(\n",
        "    (col(\"transaction_count\") > 20) & (col(\"transaction_count\") <= 100)\n",
        ").count()\n",
        "\n",
        "total_count = train_df.count()\n",
        "print(f\"   Low range (1-20): {low_range_count} ({low_range_count/total_count*100:.2f}%)\")\n",
        "print(f\"   Mid range (21-100): {mid_range_count} ({mid_range_count/total_count*100:.2f}%)\")\n",
        "print(f\"   High range (100+): {high_range_count} ({high_range_count/total_count*100:.2f}%)\")\n",
        "\n",
        "if low_range_count > 0 and high_range_count > 0 and mid_range_count < (low_range_count + high_range_count) * 0.1:\n",
        "    print(\"\\n   âš  NOTE: Potential bimodal distribution detected\")\n",
        "    print(\"   Most transactions are in low (1-20) or high (100+) ranges\")\n",
        "    print(\"   This may explain why some mid-range bins (21-50, 51-100) are empty\")\n",
        "else:\n",
        "    print(\"\\n   âœ“ Distribution appears more uniform across ranges\")\n",
        "\n",
        "print(\"\\nâœ“ Transaction count distribution analysis complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Verify Binning Coverage & Dataset Structure\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Binning Coverage & Dataset Structure\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Verify when().otherwise() logic covers all cases\n",
        "print(\"\\n1. Verifying binning logic coverage:\")\n",
        "\n",
        "# Check all possible transaction_count values are covered\n",
        "txn_count_min = train_df.select(spark_min(\"transaction_count\")).collect()[0][0]\n",
        "txn_count_max = train_df.select(spark_max(\"transaction_count\")).collect()[0][0]\n",
        "\n",
        "print(f\"   Transaction count range: {txn_count_min} to {txn_count_max}\")\n",
        "\n",
        "# Verify each value in range is assigned to a bin\n",
        "print(\"\\n   Checking binning logic for edge cases:\")\n",
        "\n",
        "edge_cases = [1, 5, 6, 20, 21, 50, 51, 100, 101]\n",
        "if txn_count_max > 101:\n",
        "    edge_cases.append(txn_count_max)\n",
        "\n",
        "for test_count in edge_cases:\n",
        "    if test_count <= txn_count_max:\n",
        "        # Determine expected bin\n",
        "        if test_count <= 5:\n",
        "            expected_bin = \"1-5\"\n",
        "        elif test_count <= 20:\n",
        "            expected_bin = \"6-20\"\n",
        "        elif test_count <= 50:\n",
        "            expected_bin = \"21-50\"\n",
        "        elif test_count <= 100:\n",
        "            expected_bin = \"51-100\"\n",
        "        else:\n",
        "            expected_bin = \"100+\"\n",
        "        \n",
        "        # Check if any transactions with this count exist and what bin they're in\n",
        "        test_df = train_df.filter(col(\"transaction_count\") == test_count).limit(1)\n",
        "        if test_df.count() > 0:\n",
        "            actual_bin = test_df.select(\"transaction_count_bin\").first()[\"transaction_count_bin\"]\n",
        "            status = \"âœ“\" if actual_bin == expected_bin else \"âœ—\"\n",
        "            print(f\"   transaction_count={test_count}: expected={expected_bin}, actual={actual_bin} {status}\")\n",
        "\n",
        "# 2. Check for NULL transaction_count values after binning\n",
        "print(\"\\n2. NULL value check:\")\n",
        "null_txn_count = train_df.filter(col(\"transaction_count\").isNull()).count()\n",
        "null_bin = train_df.filter(col(\"transaction_count_bin\").isNull()).count()\n",
        "\n",
        "print(f\"   NULL transaction_count: {null_txn_count}\")\n",
        "print(f\"   NULL transaction_count_bin: {null_bin}\")\n",
        "\n",
        "if null_txn_count > 0 or null_bin > 0:\n",
        "    print(\"   âš  WARNING: Found NULL values!\")\n",
        "else:\n",
        "    print(\"   âœ“ No NULL values found\")\n",
        "\n",
        "# 3. Document dataset structure characteristics\n",
        "print(\"\\n3. Dataset structure characteristics:\")\n",
        "\n",
        "# Card-level statistics\n",
        "unique_cards = train_df.select(\"cc_num\").distinct().count()\n",
        "total_transactions = train_df.count()\n",
        "avg_txns_per_card = total_transactions / unique_cards if unique_cards > 0 else 0\n",
        "\n",
        "print(f\"   Unique cards: {unique_cards}\")\n",
        "print(f\"   Total transactions: {total_transactions}\")\n",
        "print(f\"   Average transactions per card: {avg_txns_per_card:.2f}\")\n",
        "\n",
        "# Transaction count distribution at card level\n",
        "card_txn_dist = train_df.select(\"cc_num\", \"transaction_count\").distinct()\n",
        "card_txn_stats = card_txn_dist.select(\n",
        "    spark_min(\"transaction_count\").alias(\"min_per_card\"),\n",
        "    spark_max(\"transaction_count\").alias(\"max_per_card\"),\n",
        "    avg(\"transaction_count\").alias(\"avg_per_card\"),\n",
        "    percentile_approx(\"transaction_count\", 0.5).alias(\"median_per_card\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\n   Per-card transaction count:\")\n",
        "print(f\"   - Min: {card_txn_stats['min_per_card']}\")\n",
        "print(f\"   - Max: {card_txn_stats['max_per_card']}\")\n",
        "print(f\"   - Average: {card_txn_stats['avg_per_card']:.2f}\")\n",
        "print(f\"   - Median: {card_txn_stats['median_per_card']}\")\n",
        "\n",
        "# 4. Document if bimodal distribution is expected\n",
        "print(\"\\n4. Distribution pattern documentation:\")\n",
        "\n",
        "# Count cards in each bin\n",
        "cards_per_bin = train_df.select(\"cc_num\", \"transaction_count_bin\").distinct().groupBy(\"transaction_count_bin\").agg(\n",
        "    spark_count(\"*\").alias(\"num_cards\")\n",
        ").orderBy(\"transaction_count_bin\").toPandas()\n",
        "\n",
        "print(\"\\n   Cards per bin:\")\n",
        "print(cards_per_bin.to_string(index=False))\n",
        "\n",
        "# Analyze distribution pattern\n",
        "low_volume_cards = train_df.filter(col(\"transaction_count\") <= 20).select(\"cc_num\").distinct().count()\n",
        "high_volume_cards = train_df.filter(col(\"transaction_count\") > 100).select(\"cc_num\").distinct().count()\n",
        "mid_volume_cards = train_df.filter(\n",
        "    (col(\"transaction_count\") > 20) & (col(\"transaction_count\") <= 100)\n",
        ").select(\"cc_num\").distinct().count()\n",
        "\n",
        "print(f\"\\n   Card distribution by volume:\")\n",
        "print(f\"   - Low volume (1-20 txns): {low_volume_cards} cards\")\n",
        "print(f\"   - Mid volume (21-100 txns): {mid_volume_cards} cards\")\n",
        "print(f\"   - High volume (100+ txns): {high_volume_cards} cards\")\n",
        "\n",
        "if mid_volume_cards < (low_volume_cards + high_volume_cards) * 0.2:\n",
        "    print(\"\\n   âš  NOTE: Bimodal distribution detected at card level\")\n",
        "    print(\"   Most cards have either very few (1-20) or many (100+) transactions\")\n",
        "    print(\"   This is a legitimate data characteristic and explains empty mid-range bins\")\n",
        "    print(\"   âœ“ This pattern is expected and does not indicate data quality issues\")\n",
        "else:\n",
        "    print(\"\\n   âœ“ Distribution appears more uniform across volume ranges\")\n",
        "\n",
        "print(\"\\nâœ“ Binning coverage and dataset structure validation complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Card Usage Patterns (Deferred)\n",
        "\n",
        "**Note:** Complex usage pattern analysis (sudden spikes, long inactivity) is deferred to Section 11 as interaction features.\n",
        "\n",
        "**Next:** Section 10.4 - Key Findings Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Key Findings Summary\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "Credit card characteristics show **actionable fraud signals**:\n",
        "- **Card age** patterns reveal risk differences between new and established cards\n",
        "- **Transaction count** patterns reveal risk differences between low-volume and high-volume cards\n",
        "\n",
        "### Key Findings by Dimension\n",
        "\n",
        "#### 1. Card Age Analysis (Section 10.1)\n",
        "\n",
        "**Key patterns:** Riskiest <7 days (6.55%), safest 90-180 days (0.48%), risk ratio 13.77x.\n",
        "\n",
        "#### 2. Transaction History Analysis (Section 10.2)\n",
        "\n",
        "**Key patterns:** Riskiest 6-20 (100% fraud), safest 100+ (0.52%), risk ratio 191.57x.\n",
        "\n",
        "### Feature Engineering Recommendations\n",
        "\n",
        "**High-value features to keep:**\n",
        "- `card_age_days` (continuous)\n",
        "- `card_age_bin` (categorical)\n",
        "- `transaction_count` (continuous)\n",
        "- `transaction_count_bin` (categorical)\n",
        "\n",
        "**Priority assessment:**\n",
        "- **Card age:** **Critical priority** (13.77x risk ratio - strongest signal from Section 10)\n",
        "- **Transaction count:** **Critical priority** (191.57x risk ratio - extreme signal, 6-20 bin shows 100% fraud)\n",
        "\n",
        "### Production Recommendations\n",
        "\n",
        "1. **Card age:** Use both continuous and binned features\n",
        "2. **Transaction count:** Use both continuous and binned features\n",
        "3. **Interactions:** Explore card age Ã— transaction count in Section 11\n",
        "\n",
        "---\n",
        "\n",
        "**Section 10 Complete**\n",
        "\n",
        "**Next Section:** 11.0 - Feature Enrichment & Interactions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE SECTION-LEVEL CHECKPOINT: SECTION 10 (CREDIT CARD FEATURES COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 10 adds\n",
        "required_columns_section10 = [\n",
        "    \"card_age_days\",\n",
        "    \"card_age_bin\",\n",
        "    \"transaction_count\",\n",
        "    \"transaction_count_bin\"\n",
        "]\n",
        "\n",
        "missing_cols = [c for c in required_columns_section10 if c not in train_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"WARNING: Missing credit card columns: {missing_cols}\")\n",
        "    print(\"  Some credit card features may not have been created.\")\n",
        "    print(\"  Checkpoint will be saved with available columns.\")\n",
        "else:\n",
        "    print(\"All Section 10 credit card columns present - saving section-level checkpoint...\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df,\n",
        "    CHECKPOINT_SECTION10,\n",
        "    \"Section 10 (Credit Card Features Complete - Section-Level Checkpoint)\"\n",
        ")\n",
        "print(\"Section 10 section-level checkpoint saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Feature Enrichment & Interactions\n",
        "\n",
        "## 11.0 Load Checkpoint & Overview\n",
        "\n",
        "### Purpose\n",
        "Synthesize findings from Sections 5-10, create interaction features, and prepare features for modeling.\n",
        "\n",
        "**What we'll do:**\n",
        "1. **Feature Inventory** - List all features created in Sections 5-10\n",
        "2. **Interaction Features** - Combine high-signal dimensions (temporal Ã— amount, geographic Ã— temporal, etc.)\n",
        "3. **Feature Enrichment** - Create risk scores, pattern flags, and composite features\n",
        "4. **Feature Selection** - Assess importance and categorize features for modeling\n",
        "5. **Final Summary** - Document all findings and recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD CHECKPOINT: SECTION 11 (FEATURE ENRICHMENT)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns for Section 11 (need all features from previous sections)\n",
        "required_columns_section11 = [\n",
        "    \"is_fraud\"  # Target variable - always required\n",
        "]\n",
        "\n",
        "# Try to load from Section 10 checkpoint (preferred - has all features)\n",
        "df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "    checkpoint_path=CHECKPOINT_SECTION10,\n",
        "    required_columns=required_columns_section11,\n",
        "    cell_name=\"Section 10 (preferred for Section 11)\"\n",
        ")\n",
        "\n",
        "if loaded_from_checkpoint:\n",
        "    train_df = df_loaded\n",
        "    TOTAL_DATASET_ROWS = train_df.count()\n",
        "    print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "else:\n",
        "    # Fallback: Try Section 9 checkpoint\n",
        "    df_loaded, loaded_from_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "        checkpoint_path=CHECKPOINT_SECTION9,\n",
        "        required_columns=required_columns_section11,\n",
        "        cell_name=\"Section 9 (fallback for Section 11)\"\n",
        "    )\n",
        "    \n",
        "    if loaded_from_checkpoint:\n",
        "        train_df = df_loaded\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(f\"âœ“ Loaded {TOTAL_DATASET_ROWS:,} rows from checkpoint\")\n",
        "    else:\n",
        "        # Final fallback: Use in-memory train_df\n",
        "        if 'train_df' not in globals() or train_df is None:\n",
        "            raise ValueError(\n",
        "                f\"train_df not found and checkpoint not available. \"\n",
        "                \"Run previous sections first or ensure checkpoint exists.\"\n",
        "            )\n",
        "        TOTAL_DATASET_ROWS = train_df.count()\n",
        "        print(\"âœ“ train_df already in memory\")\n",
        "\n",
        "print(f\"\\nâœ“ Ready for Feature Enrichment & Interactions\")\n",
        "print(f\"  Total rows: {TOTAL_DATASET_ROWS:,}\")\n",
        "print(f\"  Total columns: {len(train_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.1 Feature Inventory & Summary\n",
        "\n",
        "### Purpose\n",
        "List all features created in Sections 5-10 and create a summary for modeling readiness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FEATURE INVENTORY FROM SECTIONS 5-10\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Define feature inventory by section\n",
        "feature_inventory = {\n",
        "    \"Section 5 (Amount)\": [\"amount_bin\"],\n",
        "    \"Section 6 (Timezone)\": [\"merchant_local_time\", \"customer_local_time\"],\n",
        "    \"Section 7 (Temporal)\": [\"hour\", \"day_of_week\", \"month\", \"time_bin\", \"is_weekend\"],\n",
        "    \"Section 8 (Geographic)\": [\"customer_merchant_distance_km\", \"distance_category\", \"city_size\"],\n",
        "    \"Section 9 (Demographics)\": [\"age\", \"age_group\", \"gender\", \"job\"],\n",
        "    \"Section 10 (Credit Card)\": [\"card_age_days\", \"card_age_bin\", \"transaction_count\", \"transaction_count_bin\"]\n",
        "}\n",
        "\n",
        "# Check which features actually exist in train_df\n",
        "all_features = []\n",
        "for section, features in feature_inventory.items():\n",
        "    for feature in features:\n",
        "        exists = feature in train_df.columns\n",
        "        all_features.append({\n",
        "            \"Section\": section,\n",
        "            \"Feature\": feature,\n",
        "            \"Exists\": \"âœ“\" if exists else \"âœ—\"\n",
        "        })\n",
        "\n",
        "feature_df = pd.DataFrame(all_features)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"FEATURE INVENTORY (Sections 5-10)\")\n",
        "print(\"=\" * 100)\n",
        "print(feature_df.to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Count features\n",
        "total_features = len(feature_df)\n",
        "existing_features = len(feature_df[feature_df[\"Exists\"] == \"âœ“\"])\n",
        "missing_features = len(feature_df[feature_df[\"Exists\"] == \"âœ—\"])\n",
        "\n",
        "print(f\"\\nTotal features expected: {total_features}\")\n",
        "print(f\"Features present: {existing_features}\")\n",
        "print(f\"Features missing: {missing_features}\")\n",
        "\n",
        "if missing_features > 0:\n",
        "    print(f\"\\nâš ï¸  WARNING: {missing_features} features are missing. Some sections may not have been run.\")\n",
        "    missing_list = feature_df[feature_df[\"Exists\"] == \"âœ—\"][[\"Section\", \"Feature\"]].values.tolist()\n",
        "    for section, feature in missing_list:\n",
        "        print(f\"  - {section}: {feature}\")\n",
        "else:\n",
        "    print(\"\\nâœ“ All expected features are present\")\n",
        "\n",
        "# Save feature inventory\n",
        "if \"results_manager\" in globals():\n",
        "    results_manager.save_dataframe(feature_df, \"11.1\", \"feature_inventory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.2 High-Priority Interaction Features\n",
        "\n",
        "### Purpose\n",
        "Create interaction features that combine high-signal dimensions from Sections 5-10.\n",
        "\n",
        "**Interaction types:**\n",
        "1. **Temporal Ã— Amount** - Evening + High amount, Friday + Evening\n",
        "2. **Temporal Ã— Category** - Evening + Online shopping, Weekday + Grocery\n",
        "3. **Geographic Ã— Temporal** - Large city + Evening, Far distance + Weekend\n",
        "4. **Demographics Ã— Temporal** - Young + Evening, Friday + Young\n",
        "5. **Card Ã— Temporal** - New card + Evening, Low volume + Friday\n",
        "6. **Amount Ã— Category** - High amount + Online shopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREATE INTERACTION FEATURES\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# 1. Temporal Ã— Amount interactions\n",
        "if \"time_bin\" in train_df.columns and \"amount_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"evening_high_amount\",\n",
        "        when((col(\"time_bin\") == \"Evening\") & (col(\"amount_bin\").isin([\"High\", \"Very High\"])), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created evening_high_amount\")\n",
        "\n",
        "if \"day_of_week\" in train_df.columns and \"time_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"friday_evening\",\n",
        "        when((col(\"day_of_week\") == 6) & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created friday_evening\")\n",
        "\n",
        "# 2. Temporal Ã— Category interactions\n",
        "if \"time_bin\" in train_df.columns and \"category\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"evening_online_shopping\",\n",
        "        when((col(\"time_bin\") == \"Evening\") & (col(\"category\").isin([\"shopping_net\", \"misc_net\"])), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created evening_online_shopping\")\n",
        "\n",
        "if \"is_weekend\" in train_df.columns and \"category\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"weekday_grocery\",\n",
        "        when((col(\"is_weekend\") == 0) & (col(\"category\") == \"grocery_pos\"), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created weekday_grocery\")\n",
        "\n",
        "# 3. Geographic Ã— Temporal interactions\n",
        "if \"city_size\" in train_df.columns and \"time_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"large_city_evening\",\n",
        "        when((col(\"city_size\") == \"Large\") & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created large_city_evening\")\n",
        "\n",
        "if \"distance_category\" in train_df.columns and \"is_weekend\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"far_distance_weekend\",\n",
        "        when((col(\"distance_category\") == \"Far\") & (col(\"is_weekend\") == 1), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created far_distance_weekend\")\n",
        "\n",
        "# 4. Demographics Ã— Temporal interactions\n",
        "if \"age_group\" in train_df.columns and \"time_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"young_evening\",\n",
        "        when((col(\"age_group\").isin([\"18-25\", \"26-35\"])) & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created young_evening\")\n",
        "\n",
        "if \"day_of_week\" in train_df.columns and \"age_group\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"friday_young\",\n",
        "        when((col(\"day_of_week\") == 6) & (col(\"age_group\").isin([\"18-25\", \"26-35\"])), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created friday_young\")\n",
        "\n",
        "# 5. Card Ã— Temporal interactions\n",
        "if \"card_age_bin\" in train_df.columns and \"time_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"new_card_evening\",\n",
        "        when((col(\"card_age_bin\").isin([\"<7 days\", \"7-30 days\"])) & (col(\"time_bin\") == \"Evening\"), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created new_card_evening\")\n",
        "\n",
        "if \"transaction_count_bin\" in train_df.columns and \"day_of_week\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"low_volume_friday\",\n",
        "        when((col(\"transaction_count_bin\") == \"1-5\") & (col(\"day_of_week\") == 6), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created low_volume_friday\")\n",
        "\n",
        "# 6. Amount Ã— Category interactions\n",
        "if \"amount_bin\" in train_df.columns and \"category\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"high_amount_online\",\n",
        "        when((col(\"amount_bin\").isin([\"High\", \"Very High\"])) & (col(\"category\").isin([\"shopping_net\", \"misc_net\"])), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created high_amount_online\")\n",
        "\n",
        "print(\"\\nâœ“ All interaction features created\")\n",
        "\n",
        "# ============================================================\n",
        "# VALIDATE INTERACTION FEATURES (Sample validation)\n",
        "# ============================================================\n",
        "\n",
        "# Validate a few key interactions using aggregate_fraud_by_dimension\n",
        "interaction_features = [\n",
        "    \"evening_high_amount\", \"friday_evening\", \"evening_online_shopping\",\n",
        "    \"new_card_evening\", \"low_volume_friday\", \"high_amount_online\"\n",
        "]\n",
        "\n",
        "interaction_stats = []\n",
        "for feature in interaction_features:\n",
        "    if feature in train_df.columns:\n",
        "        stats = aggregate_fraud_by_dimension(\n",
        "            df=train_df,\n",
        "            dimension_col=feature,\n",
        "            dimension_name=feature,\n",
        "            cache_name=None,\n",
        "            save_result=False\n",
        "        )\n",
        "        interaction_stats.append(stats)\n",
        "\n",
        "if interaction_stats:\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"INTERACTION FEATURE VALIDATION (Sample)\")\n",
        "    print(\"=\" * 100)\n",
        "    for stats in interaction_stats[:3]:  # Show first 3\n",
        "        print(stats.to_string(index=False))\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "# Save interaction feature list\n",
        "if \"results_manager\" in globals():\n",
        "    interaction_feature_list = pd.DataFrame({\n",
        "        \"feature\": [f for f in interaction_features if f in train_df.columns]\n",
        "    })\n",
        "    results_manager.save_dataframe(interaction_feature_list, \"11.2\", \"interaction_features_list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2.1 Validation: Interaction Feature Values\n",
        "\n",
        "**Purpose:** Verify actual values for amount_bin, time_bin, and category before fixing interaction features.\n",
        "\n",
        "**Validation Tasks:**\n",
        "1. Verify amount_bin actual values (should be \"<$50\", \"$50-$100\", etc., not \"High\", \"Very High\")\n",
        "2. Verify time_bin actual values (confirm \"Evening\" exists and is spelled correctly)\n",
        "3. Verify category values (confirm \"shopping_net\" and \"misc_net\" exist)\n",
        "4. Fix evening_high_amount logic using correct amount_bin values\n",
        "5. Fix high_amount_online logic using correct amount_bin values\n",
        "6. Test interaction features after fixes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Verify Interaction Feature Base Values\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, count as spark_count\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Interaction Feature Base Values\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Verify amount_bin actual values\n",
        "print(\"\\n1. Verifying amount_bin actual values:\")\n",
        "if \"amount_bin\" in train_df.columns:\n",
        "    amount_bin_distinct = train_df.select(\"amount_bin\").distinct().orderBy(\"amount_bin\").toPandas()\n",
        "    print(\"\\n   Distinct amount_bin values:\")\n",
        "    print(amount_bin_distinct.to_string(index=False))\n",
        "    \n",
        "    # Check for incorrect values\n",
        "    incorrect_values = [\"High\", \"Very High\"]\n",
        "    found_incorrect = amount_bin_distinct[amount_bin_distinct['amount_bin'].isin(incorrect_values)]\n",
        "    \n",
        "    if len(found_incorrect) > 0:\n",
        "        print(f\"\\n   âš  WARNING: Found incorrect amount_bin values: {found_incorrect['amount_bin'].tolist()}\")\n",
        "        print(\"   Expected values: '<$50', '$50-$100', '$100-$200', '$200-$300', '$300-$500', '$500-$1000', '>$1000'\")\n",
        "    else:\n",
        "        print(\"\\n   âœ“ No incorrect values found (no 'High' or 'Very High')\")\n",
        "        print(\"   Expected values confirmed\")\n",
        "    \n",
        "    # Show distribution\n",
        "    amount_bin_dist = train_df.groupBy(\"amount_bin\").agg(\n",
        "        spark_count(\"*\").alias(\"count\")\n",
        "    ).orderBy(\"amount_bin\").toPandas()\n",
        "    print(\"\\n   amount_bin distribution:\")\n",
        "    print(amount_bin_dist.to_string(index=False))\n",
        "else:\n",
        "    print(\"   âš  amount_bin column not found in train_df\")\n",
        "\n",
        "# 2. Verify time_bin actual values\n",
        "print(\"\\n2. Verifying time_bin actual values:\")\n",
        "if \"time_bin\" in train_df.columns:\n",
        "    time_bin_distinct = train_df.select(\"time_bin\").distinct().orderBy(\"time_bin\").toPandas()\n",
        "    print(\"\\n   Distinct time_bin values:\")\n",
        "    print(time_bin_distinct.to_string(index=False))\n",
        "    \n",
        "    # Check if \"Evening\" exists\n",
        "    if \"Evening\" in time_bin_distinct['time_bin'].values:\n",
        "        print(\"\\n   âœ“ 'Evening' value found\")\n",
        "    else:\n",
        "        print(\"\\n   âš  WARNING: 'Evening' value not found!\")\n",
        "        print(\"   Available values:\", time_bin_distinct['time_bin'].tolist())\n",
        "    \n",
        "    # Check spelling variations\n",
        "    evening_variations = [\"Evening\", \"evening\", \"EVENING\"]\n",
        "    found_variations = [val for val in evening_variations if val in time_bin_distinct['time_bin'].values]\n",
        "    if len(found_variations) > 0:\n",
        "        print(f\"   Found evening variations: {found_variations}\")\n",
        "    \n",
        "    # Show distribution\n",
        "    time_bin_dist = train_df.groupBy(\"time_bin\").agg(\n",
        "        spark_count(\"*\").alias(\"count\")\n",
        "    ).orderBy(\"time_bin\").toPandas()\n",
        "    print(\"\\n   time_bin distribution:\")\n",
        "    print(time_bin_dist.to_string(index=False))\n",
        "else:\n",
        "    print(\"   âš  time_bin column not found in train_df\")\n",
        "\n",
        "# 3. Verify category values\n",
        "print(\"\\n3. Verifying category values:\")\n",
        "if \"category\" in train_df.columns:\n",
        "    category_distinct = train_df.select(\"category\").distinct().orderBy(\"category\").toPandas()\n",
        "    print(\"\\n   Distinct category values (showing all):\")\n",
        "    print(category_distinct.to_string(index=False))\n",
        "    \n",
        "    # Check for required values\n",
        "    required_categories = [\"shopping_net\", \"misc_net\"]\n",
        "    found_categories = category_distinct[category_distinct['category'].isin(required_categories)]\n",
        "    \n",
        "    print(f\"\\n   Required categories check:\")\n",
        "    for req_cat in required_categories:\n",
        "        if req_cat in category_distinct['category'].values:\n",
        "            print(f\"   âœ“ '{req_cat}' found\")\n",
        "        else:\n",
        "            print(f\"   âš  WARNING: '{req_cat}' not found!\")\n",
        "    \n",
        "    # Show distribution for online categories\n",
        "    online_categories = category_distinct[category_distinct['category'].isin(required_categories)]\n",
        "    if len(online_categories) > 0:\n",
        "        online_cat_dist = train_df.filter(col(\"category\").isin(required_categories)).groupBy(\"category\").agg(\n",
        "            spark_count(\"*\").alias(\"count\")\n",
        "        ).orderBy(\"category\").toPandas()\n",
        "        print(\"\\n   Online category distribution:\")\n",
        "        print(online_cat_dist.to_string(index=False))\n",
        "else:\n",
        "    print(\"   âš  category column not found in train_df\")\n",
        "\n",
        "print(\"\\nâœ“ Base value verification complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FIX: Update Interaction Features with Correct Values\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"FIX: Updating Interaction Features with Correct amount_bin Values\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Determine correct high amount bins based on actual values\n",
        "if \"amount_bin\" in train_df.columns:\n",
        "    amount_bin_values = train_df.select(\"amount_bin\").distinct().orderBy(\"amount_bin\").toPandas()['amount_bin'].tolist()\n",
        "    \n",
        "    # Identify high amount bins (typically \"$500-$1000\" and \">$1000\")\n",
        "    high_amount_bins = [val for val in amount_bin_values if \"$500\" in str(val) or \">$1000\" in str(val) or val == \">$1000\"]\n",
        "    \n",
        "    print(f\"\\n1. Identified high amount bins: {high_amount_bins}\")\n",
        "    \n",
        "    if not high_amount_bins:\n",
        "        print(\"   âš  WARNING: No high amount bins found!\")\n",
        "        print(\"   Available amount_bin values:\", amount_bin_values)\n",
        "        print(\"   Using fallback: ['$500-$1000', '>$1000']\")\n",
        "        high_amount_bins = [\"$500-$1000\", \">$1000\"]\n",
        "    else:\n",
        "        print(f\"   âœ“ Using high amount bins: {high_amount_bins}\")\n",
        "\n",
        "# 1. Fix evening_high_amount\n",
        "print(\"\\n2. Fixing evening_high_amount:\")\n",
        "if \"time_bin\" in train_df.columns and \"amount_bin\" in train_df.columns:\n",
        "    # Remove old column if it exists\n",
        "    if \"evening_high_amount\" in train_df.columns:\n",
        "        train_df = train_df.drop(\"evening_high_amount\")\n",
        "        print(\"   Removed old evening_high_amount column\")\n",
        "    \n",
        "    # Create with correct values\n",
        "    train_df = train_df.withColumn(\n",
        "        \"evening_high_amount\",\n",
        "        when((col(\"time_bin\") == \"Evening\") & (col(\"amount_bin\").isin(high_amount_bins)), 1).otherwise(0)\n",
        "    )\n",
        "    print(f\"   âœ“ Created evening_high_amount with correct amount_bin values: {high_amount_bins}\")\n",
        "    \n",
        "    # Verify the fix\n",
        "    evening_high_count = train_df.filter(col(\"evening_high_amount\") == 1).count()\n",
        "    print(f\"   Transactions with evening_high_amount=1: {evening_high_count}\")\n",
        "else:\n",
        "    print(\"   âš  Cannot fix: time_bin or amount_bin column missing\")\n",
        "\n",
        "# 2. Fix high_amount_online\n",
        "print(\"\\n3. Fixing high_amount_online:\")\n",
        "if \"amount_bin\" in train_df.columns and \"category\" in train_df.columns:\n",
        "    # Remove old column if it exists\n",
        "    if \"high_amount_online\" in train_df.columns:\n",
        "        train_df = train_df.drop(\"high_amount_online\")\n",
        "        print(\"   Removed old high_amount_online column\")\n",
        "    \n",
        "    # Create with correct values\n",
        "    train_df = train_df.withColumn(\n",
        "        \"high_amount_online\",\n",
        "        when((col(\"amount_bin\").isin(high_amount_bins)) & (col(\"category\").isin([\"shopping_net\", \"misc_net\"])), 1).otherwise(0)\n",
        "    )\n",
        "    print(f\"   âœ“ Created high_amount_online with correct amount_bin values: {high_amount_bins}\")\n",
        "    \n",
        "    # Verify the fix\n",
        "    high_amount_online_count = train_df.filter(col(\"high_amount_online\") == 1).count()\n",
        "    print(f\"   Transactions with high_amount_online=1: {high_amount_online_count}\")\n",
        "else:\n",
        "    print(\"   âš  Cannot fix: amount_bin or category column missing\")\n",
        "\n",
        "print(\"\\nâœ“ Interaction feature fixes complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# VALIDATION: Test Interaction Features After Fix\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, count as spark_count, sum as spark_sum, avg\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"VALIDATION: Testing Interaction Features After Fix\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# 1. Verify non-zero values\n",
        "print(\"\\n1. Verifying interaction features have non-zero values:\")\n",
        "\n",
        "interaction_features_to_test = [\"evening_high_amount\", \"high_amount_online\"]\n",
        "\n",
        "for feature in interaction_features_to_test:\n",
        "    if feature in train_df.columns:\n",
        "        feature_count = train_df.filter(col(feature) == 1).count()\n",
        "        total_count = train_df.count()\n",
        "        feature_pct = (feature_count / total_count * 100) if total_count > 0 else 0\n",
        "        \n",
        "        print(f\"\\n   {feature}:\")\n",
        "        print(f\"   - Transactions with {feature}=1: {feature_count} ({feature_pct:.2f}%)\")\n",
        "        \n",
        "        if feature_count > 0:\n",
        "            print(f\"   âœ“ Feature has non-zero values\")\n",
        "        else:\n",
        "            print(f\"   âš  WARNING: Feature has no non-zero values!\")\n",
        "    else:\n",
        "        print(f\"\\n   âš  {feature} column not found\")\n",
        "\n",
        "# 2. Check fraud rates for fixed interaction features\n",
        "print(\"\\n2. Checking fraud rates for fixed interaction features:\")\n",
        "\n",
        "for feature in interaction_features_to_test:\n",
        "    if feature in train_df.columns:\n",
        "        feature_stats = train_df.groupBy(feature).agg(\n",
        "            spark_count(\"*\").alias(\"total_txns\"),\n",
        "            spark_sum(\"is_fraud\").alias(\"fraud_count\"),\n",
        "            avg(\"is_fraud\").alias(\"fraud_rate\")\n",
        "        ).orderBy(feature).toPandas()\n",
        "        \n",
        "        print(f\"\\n   {feature} fraud statistics:\")\n",
        "        feature_stats['fraud_rate_pct'] = feature_stats['fraud_rate'] * 100\n",
        "        print(feature_stats[['total_txns', 'fraud_count', 'fraud_rate_pct']].to_string(index=False))\n",
        "        \n",
        "        # Compare fraud rates\n",
        "        if len(feature_stats) == 2:\n",
        "            fraud_rate_when_1 = feature_stats[feature_stats[feature] == 1]['fraud_rate_pct'].values[0]\n",
        "            fraud_rate_when_0 = feature_stats[feature_stats[feature] == 0]['fraud_rate_pct'].values[0]\n",
        "            \n",
        "            if fraud_rate_when_1 > fraud_rate_when_0:\n",
        "                risk_ratio = fraud_rate_when_1 / fraud_rate_when_0 if fraud_rate_when_0 > 0 else float('inf')\n",
        "                print(f\"\\n   Risk ratio: {risk_ratio:.2f}x higher when {feature}=1\")\n",
        "                print(f\"   âœ“ Feature shows expected fraud signal\")\n",
        "            else:\n",
        "                print(f\"\\n   âš  NOTE: Feature does not show expected fraud signal\")\n",
        "                print(f\"   Fraud rate when {feature}=1 ({fraud_rate_when_1:.2f}%) <= when {feature}=0 ({fraud_rate_when_0:.2f}%)\")\n",
        "\n",
        "# 3. Sample transactions with fixed features\n",
        "print(\"\\n3. Sampling transactions with fixed features:\")\n",
        "\n",
        "for feature in interaction_features_to_test:\n",
        "    if feature in train_df.columns:\n",
        "        sample = train_df.filter(col(feature) == 1).select(\n",
        "            \"cc_num\", feature, \"time_bin\", \"amount_bin\", \"category\", \"is_fraud\"\n",
        "        ).limit(5).toPandas()\n",
        "        \n",
        "        if len(sample) > 0:\n",
        "            print(f\"\\n   Sample transactions with {feature}=1:\")\n",
        "            print(sample.to_string(index=False))\n",
        "        else:\n",
        "            print(f\"\\n   âš  No transactions found with {feature}=1\")\n",
        "\n",
        "print(\"\\nâœ“ Interaction feature testing complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.3 Feature Enrichment (Derived Features)\n",
        "\n",
        "### Purpose\n",
        "Create enriched features based on insights from Sections 5-10:\n",
        "- Risk score components (temporal, geographic, card)\n",
        "- Pattern flags (peak fraud hour, day, season, category, card characteristics)\n",
        "- Composite features (risk tier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREATE ENRICHED FEATURES\n",
        "# ============================================================\n",
        "\n",
        "from pyspark.sql.functions import col, when\n",
        "from functools import reduce\n",
        "import operator\n",
        "\n",
        "# 1. Pattern Flags (based on findings from Sections 5-10)\n",
        "\n",
        "# Peak fraud hour (16-19 PM from Section 7.1)\n",
        "if \"hour\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_peak_fraud_hour\",\n",
        "        when((col(\"hour\") >= 16) & (col(\"hour\") <= 19), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created is_peak_fraud_hour (16-19 PM)\")\n",
        "\n",
        "# Peak fraud day (Wednesday-Friday from Section 7.2)\n",
        "if \"day_of_week\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_peak_fraud_day\",\n",
        "        when(col(\"day_of_week\").isin([4, 5, 6]), 1).otherwise(0)  # Wed=4, Thu=5, Fri=6\n",
        "    )\n",
        "    print(\"âœ“ Created is_peak_fraud_day (Wednesday-Friday)\")\n",
        "\n",
        "# Peak fraud season (January-February from Section 7.3)\n",
        "if \"month\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_peak_fraud_season\",\n",
        "        when(col(\"month\").isin([1, 2]), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created is_peak_fraud_season (January-February)\")\n",
        "\n",
        "# High-risk category (shopping_net, misc_net, grocery_pos from Section 7.7)\n",
        "if \"category\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_high_risk_category\",\n",
        "        when(col(\"category\").isin([\"shopping_net\", \"misc_net\", \"grocery_pos\"]), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created is_high_risk_category\")\n",
        "\n",
        "# New card (<30 days from Section 10.1)\n",
        "if \"card_age_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_new_card\",\n",
        "        when(col(\"card_age_bin\").isin([\"<7 days\", \"7-30 days\"]), 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created is_new_card (<30 days)\")\n",
        "\n",
        "# Low volume card (1-5 transactions from Section 10.2)\n",
        "if \"transaction_count_bin\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"is_low_volume_card\",\n",
        "        when(col(\"transaction_count_bin\") == \"1-5\", 1).otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created is_low_volume_card (1-5 transactions)\")\n",
        "\n",
        "# 2. Risk Score Components (simplified - count risk flags)\n",
        "\n",
        "# Temporal risk score (count of temporal risk flags)\n",
        "temporal_flags = [\"is_peak_fraud_hour\", \"is_peak_fraud_day\", \"is_peak_fraud_season\"]\n",
        "temporal_flags_exist = [f for f in temporal_flags if f in train_df.columns]\n",
        "if temporal_flags_exist:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"temporal_risk_score\",\n",
        "        reduce(operator.add, [col(f) for f in temporal_flags_exist])\n",
        "    )\n",
        "    print(f\"âœ“ Created temporal_risk_score (from {len(temporal_flags_exist)} flags)\")\n",
        "\n",
        "# Geographic risk score (simplified - based on distance and city size)\n",
        "if \"distance_category\" in train_df.columns and \"city_size\" in train_df.columns:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"geographic_risk_score\",\n",
        "        when((col(\"distance_category\") == \"Far\") & (col(\"city_size\") == \"Large\"), 2)\n",
        "        .when((col(\"distance_category\") == \"Far\") | (col(\"city_size\") == \"Large\"), 1)\n",
        "        .otherwise(0)\n",
        "    )\n",
        "    print(\"âœ“ Created geographic_risk_score\")\n",
        "\n",
        "# Card risk score (based on card age and transaction count)\n",
        "card_flags = [\"is_new_card\", \"is_low_volume_card\"]\n",
        "card_flags_exist = [f for f in card_flags if f in train_df.columns]\n",
        "if card_flags_exist:\n",
        "    train_df = train_df.withColumn(\n",
        "        \"card_risk_score\",\n",
        "        reduce(operator.add, [col(f) for f in card_flags_exist])\n",
        "    )\n",
        "    print(f\"âœ“ Created card_risk_score (from {len(card_flags_exist)} flags)\")\n",
        "\n",
        "# 3. Composite Risk Tier\n",
        "risk_score_cols = [\"temporal_risk_score\", \"geographic_risk_score\", \"card_risk_score\"]\n",
        "risk_score_cols_exist = [f for f in risk_score_cols if f in train_df.columns]\n",
        "if len(risk_score_cols_exist) >= 2:\n",
        "    total_risk = reduce(operator.add, [col(f) for f in risk_score_cols_exist])\n",
        "    train_df = train_df.withColumn(\n",
        "        \"risk_tier\",\n",
        "        when(total_risk >= 4, \"Very High\")\n",
        "        .when(total_risk >= 3, \"High\")\n",
        "        .when(total_risk >= 2, \"Medium\")\n",
        "        .when(total_risk >= 1, \"Low\")\n",
        "        .otherwise(\"Very Low\")\n",
        "    )\n",
        "    print(\"âœ“ Created risk_tier (composite feature)\")\n",
        "\n",
        "print(\"\\nâœ“ All enriched features created\")\n",
        "\n",
        "# Save enriched feature list\n",
        "if \"results_manager\" in globals():\n",
        "    enriched_features = [\n",
        "        \"is_peak_fraud_hour\", \"is_peak_fraud_day\", \"is_peak_fraud_season\",\n",
        "        \"is_high_risk_category\", \"is_new_card\", \"is_low_volume_card\",\n",
        "        \"temporal_risk_score\", \"geographic_risk_score\", \"card_risk_score\", \"risk_tier\"\n",
        "    ]\n",
        "    enriched_feature_list = pd.DataFrame({\n",
        "        \"feature\": [f for f in enriched_features if f in train_df.columns]\n",
        "    })\n",
        "    results_manager.save_dataframe(enriched_feature_list, \"11.3\", \"enriched_features_list\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.4 Feature Selection & Modeling Readiness\n",
        "\n",
        "### Purpose\n",
        "Assess feature importance and categorize features for modeling pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FEATURE CATEGORIZATION FOR MODELING\n",
        "# ============================================================\n",
        "\n",
        "# Categorize features by priority (based on findings from Sections 5-10)\n",
        "feature_categories = {\n",
        "    \"Critical\": [\n",
        "        \"hour\", \"time_bin\", \"is_peak_fraud_hour\",  # Section 7.1: 36.85x variation\n",
        "        \"card_age_bin\", \"is_new_card\",  # Section 10.1: Expected strong signal\n",
        "        \"transaction_count_bin\", \"is_low_volume_card\"  # Section 10.2: Expected strong signal\n",
        "    ],\n",
        "    \"High Priority\": [\n",
        "        \"day_of_week\", \"is_peak_fraud_day\", \"month\", \"is_peak_fraud_season\",  # Section 7.2, 7.3\n",
        "        \"category\", \"is_high_risk_category\",  # Section 7.7: 11.34x variation\n",
        "        \"age_group\", \"card_age_days\", \"transaction_count\"  # Sections 9.1, 10.1, 10.2\n",
        "    ],\n",
        "    \"Moderate Priority\": [\n",
        "        \"amount_bin\", \"is_weekend\",  # Sections 5.3, 7.2\n",
        "        \"customer_merchant_distance_km\", \"distance_category\", \"city_size\",  # Section 8\n",
        "        \"gender\", \"job\"  # Section 9.2, 9.3\n",
        "    ],\n",
        "    \"Low Priority\": [\n",
        "        \"age\"  # Continuous version of age_group\n",
        "    ],\n",
        "    \"Interaction Features\": [\n",
        "        \"evening_high_amount\", \"friday_evening\", \"evening_online_shopping\",\n",
        "        \"weekday_grocery\", \"large_city_evening\", \"far_distance_weekend\",\n",
        "        \"young_evening\", \"friday_young\", \"new_card_evening\",\n",
        "        \"low_volume_friday\", \"high_amount_online\"\n",
        "    ],\n",
        "    \"Enriched Features\": [\n",
        "        \"temporal_risk_score\", \"geographic_risk_score\", \"card_risk_score\", \"risk_tier\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Check which features exist and categorize\n",
        "feature_list = []\n",
        "for category, features in feature_categories.items():\n",
        "    for feature in features:\n",
        "        exists = feature in train_df.columns\n",
        "        feature_list.append({\n",
        "            \"Category\": category,\n",
        "            \"Feature\": feature,\n",
        "            \"Exists\": \"âœ“\" if exists else \"âœ—\"\n",
        "        })\n",
        "\n",
        "feature_categorization_df = pd.DataFrame(feature_list)\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"FEATURE CATEGORIZATION FOR MODELING\")\n",
        "print(\"=\" * 100)\n",
        "print(feature_categorization_df.to_string(index=False))\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Summary by category\n",
        "print(\"\\nFeature Count by Category:\")\n",
        "for category in feature_categories.keys():\n",
        "    category_features = feature_categorization_df[\n",
        "        (feature_categorization_df[\"Category\"] == category) &\n",
        "        (feature_categorization_df[\"Exists\"] == \"âœ“\")\n",
        "    ]\n",
        "    print(f\"  {category}: {len(category_features)} features\")\n",
        "\n",
        "# Save feature categorization\n",
        "if \"results_manager\" in globals():\n",
        "    results_manager.save_dataframe(feature_categorization_df, \"11.4\", \"feature_categorization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5 Final Summary & Recommendations\n",
        "\n",
        "### Executive Summary\n",
        "\n",
        "This notebook has completed comprehensive EDA across **11 sections**, analyzing fraud patterns across:\n",
        "- **Amount** (Section 5)\n",
        "- **Timezone & Temporal Features** (Section 6)\n",
        "- **Temporal Patterns** (Section 7) - Hour, day, month, time bins, category\n",
        "- **Geographical Patterns** (Section 8) - State, city, distance, merchant\n",
        "- **Demographics** (Section 9) - Age, gender, job\n",
        "- **Credit Card Characteristics** (Section 10) - Card age, transaction count\n",
        "- **Feature Enrichment & Interactions** (Section 11) - Interaction features, risk scores, pattern flags\n",
        "\n",
        "### Key Findings Summary\n",
        "\n",
        "**Strongest Signals (191.57x - 11.34x variation):**\n",
        "- Transaction count (peak: 6-20, 100% vs. 100+, 0.52%) - **191.57x ratio**\n",
        "- Card age (peak: <7 days, 6.55% vs. 90-180 days, 0.48%) - **13.77x ratio**\n",
        "- Hour of day (peak: 18:00, 2.72% vs. 6:00, 0.07%) - **36.85x ratio**\n",
        "- Category (peak: shopping_net, 1.76% vs. health_fitness, 0.15%) - **11.34x ratio**\n",
        "\n",
        "**Moderate Signals (2.26x - 1.64x variation):**\n",
        "- Month/Season (peak: February, 0.87% vs. July, 0.38%) - **2.26x ratio**\n",
        "- Age group (peak: 65+, 0.77% vs. 36-50, 0.47%) - **1.64x ratio**\n",
        "\n",
        "**Feature Engineering Complete:**\n",
        "**Base features:** 16 features from Sections 5-10 (3 missing: day_of_week, is_weekend, age_group)\n",
        "**Interaction features:** 5 interaction features created (evening_high_amount, evening_online_shopping, large_city_evening, new_card_evening, high_amount_online)\n",
        "**Enriched features:** 9 pattern flags, risk scores, and composite features (temporal_risk_score, geographic_risk_score, card_risk_score, risk_tier, plus pattern flags)\n",
        "**Total features ready for modeling:** 55 columns (including base dataset columns)\n",
        "\n",
        "### Modeling Recommendations\n",
        "\n",
        "1. **Feature Selection:**\n",
        "   - Start with Critical and High Priority features\n",
        "   - Add Interaction and Enriched features based on model performance\n",
        "   - Use feature importance from tree-based models to validate\n",
        "\n",
        "2. **Model Types:**\n",
        "   - **Tree-based models** (XGBoost, LightGBM) - Handle interactions well\n",
        "   - **Neural networks** - Can learn complex patterns\n",
        "   - **Ensemble methods** - Combine multiple models\n",
        "\n",
        "3. **Production Deployment:**\n",
        "   - Use feature pipeline from Sections 5-11\n",
        "   - Monitor feature drift (especially temporal patterns)\n",
        "   - Implement real-time risk scoring using `risk_tier`\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Model Development** - Use features from this notebook\n",
        "2. **Feature Engineering Refinement** - Based on model performance\n",
        "3. **Production Pipeline** - Deploy feature engineering code\n",
        "\n",
        "---\n",
        "\n",
        "**EDA Complete - Ready for Modeling**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE FINAL CHECKPOINT: SECTION 11 (ALL ENRICHED FEATURES COMPLETE)\n",
        "# ============================================================\n",
        "\n",
        "# Required columns that Section 11 adds (interaction and enriched features)\n",
        "required_columns_section11 = [\n",
        "    \"evening_high_amount\", \"friday_evening\", \"evening_online_shopping\",\n",
        "    \"is_peak_fraud_hour\", \"is_peak_fraud_day\", \"is_peak_fraud_season\",\n",
        "    \"is_high_risk_category\", \"is_new_card\", \"is_low_volume_card\",\n",
        "    \"temporal_risk_score\", \"geographic_risk_score\", \"card_risk_score\", \"risk_tier\"\n",
        "]\n",
        "\n",
        "# Check which features exist (some may be missing if previous sections weren't run)\n",
        "missing_cols = [c for c in required_columns_section11 if c not in train_df.columns]\n",
        "existing_cols = [c for c in required_columns_section11 if c in train_df.columns]\n",
        "\n",
        "if missing_cols:\n",
        "    print(f\"WARNING: Missing enriched features: {missing_cols}\")\n",
        "    print(f\"  Created {len(existing_cols)} out of {len(required_columns_section11)} enriched features\")\n",
        "    print(\"  Checkpoint will be saved with available features.\")\n",
        "else:\n",
        "    print(f\"All Section 11 enriched features present ({len(existing_cols)} features) - saving final checkpoint...\")\n",
        "\n",
        "checkpoint_manager.save_checkpoint(\n",
        "    train_df,\n",
        "    CHECKPOINT_SECTION11,\n",
        "    \"Section 11 (All Enriched Features Complete - Final Checkpoint)\"\n",
        ")\n",
        "print(\"Section 11 final checkpoint saved.\")\n",
        "print(f\"\\nâœ“ EDA Complete - {len(train_df.columns)} total columns ready for modeling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-01-24T00:56:26.859492Z",
          "iopub.status.busy": "2026-01-24T00:56:26.859234Z",
          "iopub.status.idle": "2026-01-24T00:56:26.880492Z",
          "shell.execute_reply": "2026-01-24T00:56:26.879507Z",
          "shell.execute_reply.started": "2026-01-24T00:56:26.859470Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 817870,
          "sourceId": 1399887,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 9310098,
          "sourceId": 14574787,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31259,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
